{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0f84710-fa07-4ae7-a37f-f933992a5183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fea5d599-883a-41b7-9084-a2c54697cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from dask import delayed\n",
    "from runpy import run_path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import umap.umap_ as umap\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import local2global as l2g # ADDED\n",
    "\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.transforms import LargestConnectedComponents\n",
    "from torch_geometric.utils import to_networkx, from_networkx, one_hot\n",
    "#from torch_geometric.nn import Node2Vec, GCNConv, VGAE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29785136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "import random\n",
    "#import manopt_optimization as moptim\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric as tg\n",
    "from local2global_embedding.network import tgraph\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "\n",
    "\n",
    "import torch_geometric as tg\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import torch_scatter as ts\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.sparse import diags\n",
    "\n",
    "\n",
    "import scipy.sparse as ss\n",
    "import scipy.sparse.linalg as sl\n",
    "\n",
    "from local2global import Patch\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "\n",
    "import autograd.numpy as anp\n",
    "import pymanopt\n",
    "import pymanopt.manifolds\n",
    "import pymanopt.optimizers\n",
    "import random\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import numpy as np\n",
    "from pymanopt.manifolds import Stiefel, Euclidean,SpecialOrthogonalGroup,  Product\n",
    "from pymanopt.optimizers import SteepestDescent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4958605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import l2gl\n",
    "from l2gl.embedding import VGAE, GAE, VGAE_loss, GAE_loss\n",
    "from l2gl.utils import DataLoader\n",
    "from l2gl.patch import Patch, AlignmentProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44993e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import embedding as emb\n",
    "import torch_geometric as tg\n",
    "import local2global as l2g\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "#ADDED\n",
    "#import patches as pt\n",
    "#import network as ntw \n",
    "import autograd.numpy as anp\n",
    "import pymanopt\n",
    "import pymanopt.manifolds\n",
    "import pymanopt.optimizers\n",
    "import random\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import numpy as np\n",
    "\n",
    "def double_intersections_nodes(patches):\n",
    "    double_intersections=dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "    \n",
    "import itertools\n",
    "\n",
    "\n",
    "#import patches as pt\n",
    "#import network as ntw \n",
    "import numpy as np\n",
    "import geotorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "\n",
    "\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "import random\n",
    "#import manopt_optimization as moptim\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric as tg\n",
    "from local2global_embedding.network import tgraph\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.sparse as ss\n",
    "import scipy.sparse.linalg as sl\n",
    "\n",
    "import raphtory as rp\n",
    "from raphtory import Graph as rgraph\n",
    "\n",
    "\n",
    "\n",
    "import local2global as l2g\n",
    "\n",
    "import local2global_embedding\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "\n",
    "#from Local2Global_embedding.local2global_embedding import  clustering\n",
    "import community\n",
    "#from Local2Global_embedding.local2global_embedding.network import graph\n",
    "from local2global_embedding.network import TGraph\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "from local2global import Patch\n",
    "#import Local2Global_embedding.local2global_embedding.embedding.svd as svd\n",
    "#import Local2Global_embedding.local2global_embedding.embedding.gae as gae\n",
    "#import Local2Global_embedding.local2global_embedding.patches as patches\n",
    "\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torch_scatter as ts\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import optuna\n",
    "#from optuna.trial import TrialState\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from google.colab import drive, files\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.transforms import LargestConnectedComponents\n",
    "\n",
    "def connected_components(data: tg.data.Data):\n",
    "    \"\"\"Find the (weakly)-connected components of graph data. Components are sorted by size, such that id=0 corresponds\n",
    "     to the largest connected component\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "    is_undir = tg.utils.is_undirected(edge_index)\n",
    "    last_components = torch.full((data.num_nodes,), data.num_nodes, dtype=torch.long)\n",
    "    components = torch.arange(data.num_nodes, dtype=torch.long)\n",
    "    while not torch.equal(last_components, components):\n",
    "        last_components[:] = components\n",
    "        components = ts.scatter(last_components[edge_index[0]], edge_index[1], out=components, reduce='min')\n",
    "        if not is_undir:\n",
    "            components = ts.scatter(last_components[edge_index[1]], edge_index[0], out=components, reduce='min')\n",
    "    component_id, inverse, component_size = torch.unique(components, return_counts=True, return_inverse=True)\n",
    "    new_id = torch.argsort(component_size, descending=True)\n",
    "    return new_id[inverse]\n",
    "\n",
    "\n",
    "def largest_connected_component(data: tg.data.Data):\n",
    "    \"\"\"find largest connected component of data\"\"\"\n",
    "    components = connected_components(data)\n",
    "    nodes = torch.nonzero(components == 0).flatten()\n",
    "    return induced_subgraph(data, nodes)\n",
    "\n",
    "\n",
    "def induced_subgraph(data: tg.data.Data, nodes, extend_hops=0):\n",
    "    nodes = torch.as_tensor(nodes, dtype=torch.long)\n",
    "    if extend_hops > 0:\n",
    "        nodes, edge_index, node_map, edge_mask = tg.utils.k_hop_subgraph(nodes, num_hops=extend_hops,\n",
    "                                                                         edge_index=data.edge_index,\n",
    "                                                                         relabel_nodes=True)\n",
    "        edge_attr = data.edge_attr[edge_mask, :] if data.edge_attr is not None else None\n",
    "    else:\n",
    "        edge_index, edge_attr = tg.utils.subgraph(nodes, data.edge_index, data.edge_attr, relabel_nodes=True)\n",
    "\n",
    "    subgraph = tg.data.Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "    for key, value in data.__dict__.items():\n",
    "        if not key.startswith('edge'):\n",
    "            if hasattr(value, 'shape') and value.shape[0] == data.num_nodes:\n",
    "                setattr(subgraph, key, value[nodes])\n",
    "            else:\n",
    "                setattr(subgraph, key, value)\n",
    "    subgraph.nodes = nodes\n",
    "    subgraph.num_nodes = len(nodes)\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def conductance(graph: TGraph, source, target=None):\n",
    "    if target is None:\n",
    "        target_mask = torch.ones(graph.num_nodes, dtype=torch.bool, device=graph.device)\n",
    "        target_mask[source] = False\n",
    "    else:\n",
    "        target_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        target_mask[target] = True\n",
    "    out = torch.cat([graph.adj(node) for node in source])\n",
    "    cond = torch.sum(target_mask[out]).float()\n",
    "    s_deg = graph.degree[source].sum()\n",
    "    t_deg = graph.num_edges-s_deg if target is None else graph.degree[target].sum()\n",
    "    cond /= torch.minimum(s_deg, t_deg)\n",
    "    return cond\n",
    "\n",
    "\n",
    "def speye(n, dtype=torch.float):\n",
    "    \"\"\"identity matrix of dimension n as sparse_coo_tensor.\"\"\"\n",
    "    return torch.sparse_coo_tensor(torch.tile(torch.arange(n, dtype=torch.long), (2, 1)),\n",
    "                                   torch.ones(n, dtype=dtype),\n",
    "                                   (n, n))\n",
    "\n",
    "\n",
    "class DistanceDecoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistanceDecoder, self).__init__()\n",
    "        self.dist = torch.nn.PairwiseDistance()\n",
    "\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        value = -self.dist(z[edge_index[0]], z[edge_index[1]])\n",
    "        return torch.sigmoid(value) if sigmoid else value\n",
    "\n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        adj = torch.cdist(z, z)\n",
    "        return torch.sigmoid(adj) if sigmoid else adj\n",
    "\n",
    "\n",
    "class GAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_index\n",
    "        x = F.relu(self.conv1(data.x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class VGAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.mean_conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                        normalize=normalize)\n",
    "        self.var_conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                       normalize=normalize)\n",
    "\n",
    "    def forward(self, data: tg.data.Data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        mu = self.mean_conv2(x, edge_index)\n",
    "        sigma = self.var_conv2(x, edge_index)\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "def VGAE_loss(model, data):\n",
    "    return model.recon_loss(model.encode(data), data.edge_index) + model.kl_loss() / data.num_nodes\n",
    "\n",
    "\n",
    "def VGAE_model(dim, hidden_dim, num_features, dist=False):\n",
    "    if dist:\n",
    "        return tg.nn.VGAE(encoder=VGAEconv(dim, num_node_features=num_features, hidden_dim=hidden_dim),\n",
    "                          decoder=DistanceDecoder())\n",
    "    else:\n",
    "        return tg.nn.VGAE(encoder=VGAEconv(dim, num_node_features=num_features, hidden_dim=hidden_dim))\n",
    "\n",
    "\n",
    "def lr_grid_search(data, model, loss_fun, validation_loss_fun, lr_grid=(0.1, 0.01, 0.005, 0.001),\n",
    "                   num_epochs=10, runs=1, verbose=True):\n",
    "    val_loss = torch.zeros((len(lr_grid), runs))\n",
    "    val_start = torch.zeros((len(lr_grid), runs))\n",
    "    for i, lr in enumerate(lr_grid):\n",
    "        for r in range(runs):\n",
    "            model.reset_parameters()\n",
    "            val_start[i, r] = validation_loss_fun(model, data)\n",
    "            model = train(data, model, loss_fun, num_epochs=num_epochs, lr=lr, verbose=verbose)\n",
    "            val_loss[i, r] = validation_loss_fun(model, data)\n",
    "    model.reset_parameters()\n",
    "    return lr_grid[torch.argmax(torch.mean(val_loss, 1))], val_loss, val_start\n",
    "\n",
    "\n",
    "def train(data, model, loss_fun, num_epochs=100, verbose=True, lr=0.01, logger=lambda loss: None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    for e in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger(float(loss))\n",
    "        if verbose:\n",
    "            print(f'epoch {e}: loss={loss.item()}')\n",
    "        # schedule.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGAE_patch_embeddings(patch_data, dim=2, hidden_dim=32, num_epochs=100, decoder=None, device='cpu', lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        #for i in range(len(patch)):#added this for loop and replace the commented part with this\n",
    "            #if patch[i].x is None:\n",
    "                #patch[i].x=speye(patch[i].num_nodes)\n",
    "        \n",
    "        \n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        print(f\"training patch with {patch.edge_index.shape[1]} edges\")   #added [i] to every patch\n",
    "        model = tg.nn.VGAE(encoder=VGAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "\n",
    "        def loss_fun(model, data):\n",
    "            return model.recon_loss(model.encode(data), data.edge_index) + model.kl_loss() / data.num_nodes\n",
    "\n",
    "        model = train(patch, model, loss_fun, num_epochs=num_epochs, lr=lr)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            coordinates = model.encode(patch).to('cpu').numpy()\n",
    "            models.append(model)\n",
    "            patch_list.append(l2g.Patch(patch.nodes.to('cpu').numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "\n",
    "def GAE_patch_embeddings(patch_data, dim=2, hidden_dim=32, num_epochs=100, device='cpu', decoder=None, lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        print(f\"training patch with {patch.edge_index.shape[1]} edges\")\n",
    "        model = tg.nn.GAE(encoder=GAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "\n",
    "        def loss_fun(model, data):\n",
    "            return model.recon_loss(model.encode(data), data.edge_index)\n",
    "        model.train()\n",
    "        model = train(patch, model, loss_fun, num_epochs=num_epochs, lr=lr)\n",
    "        model.eval()\n",
    "        coordinates = model.encode(patch).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "        models.append(model)\n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadec485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cada8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import manopt_optimization as mopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f7e21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/nas/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e6778-b3bc-437a-b023-db0d538c05ff",
   "metadata": {},
   "source": [
    "# <font color=\"grey\"> Autonomous Systems</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc356df-39f0-4f1d-b98d-d4dcfe4665db",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to walk through the graph embedding and alignment process in a self-contained way. The full existing Local2Global package is available [here](https://github.com/LJeub/Local2Global_embedding) and the expectation is to pick parts from it as a starting point. It is also available in on this repository in the Local2Global_embedding folder for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4208fd-da21-4181-ae65-1fa8c152ea2c",
   "metadata": {},
   "source": [
    "### <font color=\"grey\">  Table of Contents</font>\n",
    "\n",
    "1. #### <a href='#chapter1'>Data</a>\n",
    "2. #### <a href='#chapter2'>Embedding</a>\n",
    "3. #### <a href='#chapter3'>Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d903eb7-22ad-4900-8720-f79d3eb44790",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> <font color=\"grey\">1. Data </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25865e-21a9-4e34-bf80-125bb3d22ca5",
   "metadata": {},
   "source": [
    "The data can be accessed via the dataloader. It is saved in the datasets/data/nas directory in two parquet files. There are many alternative ways of doing this. One option to explore is to have the datasets available as in [torch_geometric datasets](https://pytorch-geometric.readthedocs.io/en/2.6.0/modules/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38fcc940-390a-4039-aa4b-c136185c16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dset='nas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfd3ee-0e5d-4a5e-aa09-30e0dc958132",
   "metadata": {},
   "source": [
    "The data is stored in one dataframe for the nodes (including all the features) and one for the edges (including edge weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9273655-0c7c-4f2f-b40a-574d38773307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>nodes</th><th>nodetype</th><th>country</th><th>asname</th><th>nodename</th><th>cc</th></tr><tr><td>datetime[μs]</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>2024-09-14 00:00:00</td><td>0</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;WINDSTREAM&quot;</td><td>&quot;AS7029&quot;</td><td>235</td></tr><tr><td>2024-09-14 00:00:00</td><td>1</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;RUELALA-INC&quot;</td><td>&quot;AS32984&quot;</td><td>235</td></tr><tr><td>2024-09-14 00:00:00</td><td>2</td><td>&quot;asn&quot;</td><td>&quot;ID&quot;</td><td>&quot;FIBERSTAR-AS-I&quot;</td><td>&quot;AS136106&quot;</td><td>104</td></tr><tr><td>2024-09-14 00:00:00</td><td>3</td><td>&quot;asn&quot;</td><td>&quot;ID&quot;</td><td>&quot;HSPNET-AS-I&quot;</td><td>&quot;AS58495&quot;</td><td>104</td></tr><tr><td>2024-09-14 00:00:00</td><td>4</td><td>&quot;asn&quot;</td><td>&quot;US&quot;</td><td>&quot;BTN-ASN&quot;</td><td>&quot;AS3491&quot;</td><td>235</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────────────────┬───────┬──────────┬─────────┬────────────────┬──────────┬─────┐\n",
       "│ timestamp           ┆ nodes ┆ nodetype ┆ country ┆ asname         ┆ nodename ┆ cc  │\n",
       "│ ---                 ┆ ---   ┆ ---      ┆ ---     ┆ ---            ┆ ---      ┆ --- │\n",
       "│ datetime[μs]        ┆ i64   ┆ str      ┆ str     ┆ str            ┆ str      ┆ i64 │\n",
       "╞═════════════════════╪═══════╪══════════╪═════════╪════════════════╪══════════╪═════╡\n",
       "│ 2024-09-14 00:00:00 ┆ 0     ┆ asn      ┆ US      ┆ WINDSTREAM     ┆ AS7029   ┆ 235 │\n",
       "│ 2024-09-14 00:00:00 ┆ 1     ┆ asn      ┆ US      ┆ RUELALA-INC    ┆ AS32984  ┆ 235 │\n",
       "│ 2024-09-14 00:00:00 ┆ 2     ┆ asn      ┆ ID      ┆ FIBERSTAR-AS-I ┆ AS136106 ┆ 104 │\n",
       "│ 2024-09-14 00:00:00 ┆ 3     ┆ asn      ┆ ID      ┆ HSPNET-AS-I    ┆ AS58495  ┆ 104 │\n",
       "│ 2024-09-14 00:00:00 ┆ 4     ┆ asn      ┆ US      ┆ BTN-ASN        ┆ AS3491   ┆ 235 │\n",
       "└─────────────────────┴───────┴──────────┴─────────┴────────────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the nodes\n",
    "node_df = dl.get_nodes()\n",
    "node_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "149a77d9-d83c-45ff-a933-a774c50da54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>source</th><th>dest</th><th>weight</th></tr><tr><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>465</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>596</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1234</td><td>1</td></tr><tr><td>2024-09-14 00:00:00</td><td>0</td><td>1272</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────────────┬────────┬──────┬────────┐\n",
       "│ timestamp           ┆ source ┆ dest ┆ weight │\n",
       "│ ---                 ┆ ---    ┆ ---  ┆ ---    │\n",
       "│ datetime[μs]        ┆ i64    ┆ i64  ┆ i64    │\n",
       "╞═════════════════════╪════════╪══════╪════════╡\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1    ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 465  ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 596  ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1234 ┆ 1      │\n",
       "│ 2024-09-14 00:00:00 ┆ 0      ┆ 1272 ┆ 1      │\n",
       "└─────────────────────┴────────┴──────┴────────┘"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df = dl.get_edges()\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ee1fa-6bd2-41ae-bd74-36d3af72bdbe",
   "metadata": {},
   "source": [
    "Ultimately, working with the people at Pometry, we want to use the [Raphtory](https://www.raphtory.com/) graph format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c87a8252-ff89-4fe2-8a37-fd4e9cb0154f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8813f8f8941b417fb43246a6b6e2eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=16717484), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2ff01837624d18a1ee0824458c8652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=2538974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raphtory format\n",
    "g = dl.get_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca43c1c-c500-4597-b07e-61e751c6aa66",
   "metadata": {},
   "source": [
    "The Raphtory formal is still work in progress but one can contribute to their code (based in Rust), contribute to the discussion on their Slack (linked on their page) or directly get in touch with [Lucas Jeub](https://github.com/LJeub) and/or Ben Steer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d399ac6b-0eb2-40ea-bb32-37d6b0f0892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on the graph structure:\n",
      "Number of nodes (AS nodes): 85428\n",
      "Number of unique edges (src,dst): 914346\n",
      "Total interactions (edge updates): 16717484\n",
      "Stats on the graphs time range:\n",
      "Earliest datetime: 2024-09-14 00:00:00+00:00\n",
      "Latest datetime: 2024-10-13 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats on the graph structure:\")\n",
    "\n",
    "number_of_nodes = g.count_nodes()\n",
    "number_of_edges = g.count_edges()\n",
    "total_interactions = g.count_temporal_edges()\n",
    "\n",
    "print(\"Number of nodes (AS nodes):\", number_of_nodes)\n",
    "print(\"Number of unique edges (src,dst):\", number_of_edges)\n",
    "print(\"Total interactions (edge updates):\", total_interactions)\n",
    "\n",
    "print(\"Stats on the graphs time range:\")\n",
    "\n",
    "earliest_datetime = g.earliest_date_time\n",
    "latest_datetime = g.latest_date_time\n",
    "\n",
    "print(\"Earliest datetime:\", earliest_datetime)\n",
    "print(\"Latest datetime:\", latest_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "15a91cb9-ff6a-4fcd-8fbc-aecde67fee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The node features are:  ['asname', 'cc', 'country', 'nodename', 'nodetype']\n"
     ]
    }
   ],
   "source": [
    "print(\"The node features are: \", g.nodes.properties.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8afc9-aaa4-4bfc-a99a-4dcda9795f0b",
   "metadata": {},
   "source": [
    "The graphs we are dealing are **temporal**, meaning that nodes and edges have timestamps. One can interpret this as having one graph for each point in time, with a possible overlap of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "994cb72f-23c6-4151-b0fd-bf4b30340a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dl.get_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cff93-80cf-42f4-bfe4-ed4234e88a4d",
   "metadata": {},
   "source": [
    "For this particular dataset, the graph for each day represents a patch. In order to apply graph neural networks to each patch, we need to process these into the Data format used by pytorch-geometric, described [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html). In particular, for each patch we need to enumerate the nodes and use these indices to designate the nodes. We need a dictionary that maps the nodes in each patch to their names and we need to encode the node and edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ad726d0-98c2-4e50-b173-b4a81b4e547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode nodes present at each date\n",
    "nodes = {}\n",
    "node_dict = {}\n",
    "for d in dates:\n",
    "    nodes[d] = dl.get_node_list(ts=d)\n",
    "    node_dict[d] = dict(zip(nodes[d],range(len(nodes[d]))))\n",
    "all_nodes = dl.get_node_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e51fabc9-a781-4512-adf4-dcb15276a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbering_nodes= {x : i for i, x in enumerate(all_nodes)}\n",
    "list_nodes=list(nodes.values())\n",
    "\n",
    "list_nodes_renumbered=[]\n",
    "for l in list_nodes:\n",
    "    list_nodes_renumbered.append( [numbering_nodes[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b82e628b-08e8-4aa0-aa62-6b25d2c1e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode country codes\n",
    "cc = pl.read_csv('data/nas/country_codes.csv')\n",
    "countrycode_dict = dict(zip(cc[\"alpha-2\"].to_list(), range(cc.shape[0])))\n",
    "#cc_one_hot = one_hot(torch.tensor(list(countrycode_dict.values()), dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9d614f4-7a6f-4b24-b743-e4591a1ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign country code index to each node. The way this is done is a bit convoluted, as some nodes are assigned to both a country and to 'ZZ'\n",
    "# in the database, so we need to fix that. This should be done in pre-processing\n",
    "df = dl.get_nodes().with_columns(\n",
    "    pl.col(\"country\").replace(old=pl.Series(countrycode_dict.keys()), new=pl.Series(countrycode_dict.values())).cast(pl.Int64).alias('cc')\n",
    ").select([\"nodes\", \"cc\"]).group_by(\"nodes\").agg(pl.col(\"cc\").min().cast(pl.Int64).alias(\"cc\")).sort([\"cc\",\"nodes\"])\n",
    "node_cc_dict = dict(zip(df[\"nodes\"].to_list(), df[\"cc\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f87d7aa-70de-4bdc-b54e-4ba41b655c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every day, create a list of node features\n",
    "features = {}\n",
    "for d in dates:\n",
    "    features[d] = one_hot(torch.tensor(dl.get_nodes(ts=d).select(\n",
    "        pl.col(\"country\").replace(old=pl.Series(countrycode_dict.keys()), new=pl.Series(countrycode_dict.values())).cast(pl.Int64)\n",
    "    ).to_numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1470fe0a-a8b6-4151-8e1e-2fcd210b13d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84575, 250])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[dates[3]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dccedc3-6521-4158-90bf-aba33f2d902e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54ba4d030414ddbb744650521d842a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create pytorch-geometric Data object\n",
    "tg_graphs = {}\n",
    "for d in tqdm(dates):\n",
    "    edges = dl.edges.filter(pl.col('timestamp')==d).select(\n",
    "        pl.col('source').replace(old=pl.Series(node_dict[d].keys()), new=pl.Series(node_dict[d].values())).cast(pl.Int64),\n",
    "        pl.col('dest').replace(old=pl.Series(node_dict[d].keys()), new=pl.Series(node_dict[d].values())).cast(pl.Int64)\n",
    "    ).to_numpy()\n",
    "    edge_index = torch.tensor([tuple(x) for x in edges], dtype=torch.long).t().contiguous()\n",
    "    tgraph = Data(edge_index=edge_index)\n",
    "    # Add features - problem is that for the embedding we only want those present at a given time\n",
    "    tgraph.x = features[d]\n",
    "    tg_graphs[d] = tgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "692f927b-87d3-4ca3-885b-fab11ee7e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(dates):\n",
    "    tg_graphs[d].nodes=torch.tensor(list_nodes_renumbered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c7a02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "364827e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=tg_graphs[dates[0]]\n",
    "cc=test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fd6bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patches: 11\n",
      "average patch degree: 3.6363636363636362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c8c5e473484a0bb605c351aa5462f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "enlarging patch overlaps:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TG=TGraph(edge_index=cc.edge_index, edge_attr=cc.edge_attr,  num_nodes=cc.num_nodes, ensure_sorted=True, undir=False)\n",
    "pt, pgraph= create_patch_data(TG, partition_tensor= louvain_clustering(TG),\n",
    "                                           min_overlap=2000, target_overlap=4000, verbose=True)\n",
    "patch_data = [induced_subgraph(cc, p) for p in pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1242082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgraph.connected_component_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7bdc3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_edges = tg.utils.negative_sampling(test_data.edge_index, num_nodes=test_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e65bf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_intersections_nodes(patches):\n",
    "    double_intersections = dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list = []\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list = list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "def get_embedding(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    #prob=l2g.AlignmentProblem(patches)\n",
    "    #old_embedding=prob.get_aligned_embedding()\n",
    "    #embedding=embedding[nodes]\n",
    "    #old_embedding=old_embedding[nodes]\n",
    "    #error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import geotorch\n",
    "\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches, num_epochs=100, learning_rate=0.05):\n",
    "    #device = get_device()\n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    \n",
    "    model = Model(dim, n_patches, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model, loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82170d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training patch with 267352 edges\n",
      "epoch 0: loss=1.7359555959701538\n",
      "epoch 1: loss=1.6621803045272827\n",
      "epoch 2: loss=1.606276273727417\n",
      "epoch 3: loss=1.5505645275115967\n",
      "epoch 4: loss=1.477251648902893\n",
      "epoch 5: loss=1.3987598419189453\n",
      "epoch 6: loss=1.3191851377487183\n",
      "epoch 7: loss=1.245455026626587\n",
      "epoch 8: loss=1.187490463256836\n",
      "epoch 9: loss=1.148565649986267\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=1.7511072158813477\n",
      "epoch 1: loss=1.673311471939087\n",
      "epoch 2: loss=1.6395832300186157\n",
      "epoch 3: loss=1.5861210823059082\n",
      "epoch 4: loss=1.5454611778259277\n",
      "epoch 5: loss=1.501560926437378\n",
      "epoch 6: loss=1.4711217880249023\n",
      "epoch 7: loss=1.4074918031692505\n",
      "epoch 8: loss=1.3496296405792236\n",
      "epoch 9: loss=1.2981318235397339\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=1.7866861820220947\n",
      "epoch 1: loss=1.6616477966308594\n",
      "epoch 2: loss=1.6082724332809448\n",
      "epoch 3: loss=1.553657054901123\n",
      "epoch 4: loss=1.5102276802062988\n",
      "epoch 5: loss=1.4861811399459839\n",
      "epoch 6: loss=1.4531774520874023\n",
      "epoch 7: loss=1.4341284036636353\n",
      "epoch 8: loss=1.421158790588379\n",
      "epoch 9: loss=1.3980309963226318\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=1.8094793558120728\n",
      "epoch 1: loss=1.6922215223312378\n",
      "epoch 2: loss=1.57823646068573\n",
      "epoch 3: loss=1.640207290649414\n",
      "epoch 4: loss=1.4476763010025024\n",
      "epoch 5: loss=1.5194802284240723\n",
      "epoch 6: loss=1.5200477838516235\n",
      "epoch 7: loss=1.4097479581832886\n",
      "epoch 8: loss=1.388211727142334\n",
      "epoch 9: loss=1.3971344232559204\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=1.7473325729370117\n",
      "epoch 1: loss=1.6588176488876343\n",
      "epoch 2: loss=1.5868474245071411\n",
      "epoch 3: loss=1.5227957963943481\n",
      "epoch 4: loss=1.4889252185821533\n",
      "epoch 5: loss=1.4542542695999146\n",
      "epoch 6: loss=1.4163081645965576\n",
      "epoch 7: loss=1.3748807907104492\n",
      "epoch 8: loss=1.3261536359786987\n",
      "epoch 9: loss=1.2678884267807007\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=1.7144073247909546\n",
      "epoch 1: loss=1.6947050094604492\n",
      "epoch 2: loss=1.622812271118164\n",
      "epoch 3: loss=1.542664885520935\n",
      "epoch 4: loss=1.4955366849899292\n",
      "epoch 5: loss=1.439034342765808\n",
      "epoch 6: loss=1.3679778575897217\n",
      "epoch 7: loss=1.3061119318008423\n",
      "epoch 8: loss=1.232844591140747\n",
      "epoch 9: loss=1.1645835638046265\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=1.767409324645996\n",
      "epoch 1: loss=1.6879923343658447\n",
      "epoch 2: loss=1.6202644109725952\n",
      "epoch 3: loss=1.5622692108154297\n",
      "epoch 4: loss=1.4919236898422241\n",
      "epoch 5: loss=1.448940396308899\n",
      "epoch 6: loss=1.390580415725708\n",
      "epoch 7: loss=1.3354545831680298\n",
      "epoch 8: loss=1.2730623483657837\n",
      "epoch 9: loss=1.2142505645751953\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=1.7583407163619995\n",
      "epoch 1: loss=1.6809296607971191\n",
      "epoch 2: loss=1.620043396949768\n",
      "epoch 3: loss=1.5633213520050049\n",
      "epoch 4: loss=1.4932101964950562\n",
      "epoch 5: loss=1.4610446691513062\n",
      "epoch 6: loss=1.3973091840744019\n",
      "epoch 7: loss=1.347092866897583\n",
      "epoch 8: loss=1.2798885107040405\n",
      "epoch 9: loss=1.2173359394073486\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=1.7441956996917725\n",
      "epoch 1: loss=1.6426695585250854\n",
      "epoch 2: loss=1.5680193901062012\n",
      "epoch 3: loss=1.5183732509613037\n",
      "epoch 4: loss=1.4594144821166992\n",
      "epoch 5: loss=1.4178366661071777\n",
      "epoch 6: loss=1.3481801748275757\n",
      "epoch 7: loss=1.2808316946029663\n",
      "epoch 8: loss=1.2064684629440308\n",
      "epoch 9: loss=1.1405919790267944\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=1.7672159671783447\n",
      "epoch 1: loss=1.6826717853546143\n",
      "epoch 2: loss=1.6302763223648071\n",
      "epoch 3: loss=1.5660748481750488\n",
      "epoch 4: loss=1.5199596881866455\n",
      "epoch 5: loss=1.4746454954147339\n",
      "epoch 6: loss=1.4125244617462158\n",
      "epoch 7: loss=1.3816649913787842\n",
      "epoch 8: loss=1.3244311809539795\n",
      "epoch 9: loss=1.2571102380752563\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=1.740515947341919\n",
      "epoch 1: loss=1.6538504362106323\n",
      "epoch 2: loss=1.5779374837875366\n",
      "epoch 3: loss=1.5223989486694336\n",
      "epoch 4: loss=1.4468412399291992\n",
      "epoch 5: loss=1.3941391706466675\n",
      "epoch 6: loss=1.3071908950805664\n",
      "epoch 7: loss=1.239492654800415\n",
      "epoch 8: loss=1.1960657835006714\n",
      "epoch 9: loss=1.1692564487457275\n"
     ]
    }
   ],
   "source": [
    "patches, models =VGAE_patch_embeddings(patch_data, dim=2, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b07580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "95184389",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = double_intersections_nodes(patches)\n",
    "n_patches=len(patch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "700ea97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training patch with 267352 edges\n",
      "epoch 0: loss=3.3803491592407227\n",
      "epoch 1: loss=3.2017617225646973\n",
      "epoch 2: loss=2.9764645099639893\n",
      "epoch 3: loss=2.72951340675354\n",
      "epoch 4: loss=2.443437337875366\n",
      "epoch 5: loss=2.160477876663208\n",
      "epoch 6: loss=1.9462767839431763\n",
      "epoch 7: loss=1.8809869289398193\n",
      "epoch 8: loss=1.8086410760879517\n",
      "epoch 9: loss=1.745955228805542\n",
      "epoch 10: loss=1.6426869630813599\n",
      "epoch 11: loss=1.5413724184036255\n",
      "epoch 12: loss=1.4410698413848877\n",
      "epoch 13: loss=1.3566539287567139\n",
      "epoch 14: loss=1.2828290462493896\n",
      "epoch 15: loss=1.2329386472702026\n",
      "epoch 16: loss=1.2091323137283325\n",
      "epoch 17: loss=1.1904027462005615\n",
      "epoch 18: loss=1.1756504774093628\n",
      "epoch 19: loss=1.1506531238555908\n",
      "epoch 20: loss=1.1257439851760864\n",
      "epoch 21: loss=1.0951958894729614\n",
      "epoch 22: loss=1.062516450881958\n",
      "epoch 23: loss=1.038589596748352\n",
      "epoch 24: loss=1.0206166505813599\n",
      "epoch 25: loss=1.008203387260437\n",
      "epoch 26: loss=0.9912413954734802\n",
      "epoch 27: loss=0.9794285297393799\n",
      "epoch 28: loss=0.9683055877685547\n",
      "epoch 29: loss=0.9575007557868958\n",
      "epoch 30: loss=0.9496886134147644\n",
      "epoch 31: loss=0.9443352222442627\n",
      "epoch 32: loss=0.9391041398048401\n",
      "epoch 33: loss=0.9354267120361328\n",
      "epoch 34: loss=0.9318919777870178\n",
      "epoch 35: loss=0.9276536703109741\n",
      "epoch 36: loss=0.9266822934150696\n",
      "epoch 37: loss=0.9248571395874023\n",
      "epoch 38: loss=0.9228111505508423\n",
      "epoch 39: loss=0.9214503169059753\n",
      "epoch 40: loss=0.9196266531944275\n",
      "epoch 41: loss=0.9168151021003723\n",
      "epoch 42: loss=0.9153351783752441\n",
      "epoch 43: loss=0.9128603935241699\n",
      "epoch 44: loss=0.9120297431945801\n",
      "epoch 45: loss=0.9109212756156921\n",
      "epoch 46: loss=0.909640908241272\n",
      "epoch 47: loss=0.9088190793991089\n",
      "epoch 48: loss=0.906139612197876\n",
      "epoch 49: loss=0.905921459197998\n",
      "epoch 50: loss=0.90409255027771\n",
      "epoch 51: loss=0.903144121170044\n",
      "epoch 52: loss=0.9016555547714233\n",
      "epoch 53: loss=0.9018685221672058\n",
      "epoch 54: loss=0.901506245136261\n",
      "epoch 55: loss=0.9001007080078125\n",
      "epoch 56: loss=0.8997095823287964\n",
      "epoch 57: loss=0.8988814353942871\n",
      "epoch 58: loss=0.8978639841079712\n",
      "epoch 59: loss=0.897409975528717\n",
      "epoch 60: loss=0.8972719311714172\n",
      "epoch 61: loss=0.8955461382865906\n",
      "epoch 62: loss=0.8958649039268494\n",
      "epoch 63: loss=0.894871175289154\n",
      "epoch 64: loss=0.8951931595802307\n",
      "epoch 65: loss=0.893679141998291\n",
      "epoch 66: loss=0.8940486311912537\n",
      "epoch 67: loss=0.8927230834960938\n",
      "epoch 68: loss=0.8921180367469788\n",
      "epoch 69: loss=0.8922398686408997\n",
      "epoch 70: loss=0.8910846710205078\n",
      "epoch 71: loss=0.8900704979896545\n",
      "epoch 72: loss=0.8916043043136597\n",
      "epoch 73: loss=0.8903480172157288\n",
      "epoch 74: loss=0.8905143141746521\n",
      "epoch 75: loss=0.8888310194015503\n",
      "epoch 76: loss=0.8884665369987488\n",
      "epoch 77: loss=0.8898297548294067\n",
      "epoch 78: loss=0.88856041431427\n",
      "epoch 79: loss=0.8883930444717407\n",
      "epoch 80: loss=0.8879937529563904\n",
      "epoch 81: loss=0.8876278400421143\n",
      "epoch 82: loss=0.8869065642356873\n",
      "epoch 83: loss=0.8854729533195496\n",
      "epoch 84: loss=0.8852596879005432\n",
      "epoch 85: loss=0.8850013017654419\n",
      "epoch 86: loss=0.8852954506874084\n",
      "epoch 87: loss=0.8843300938606262\n",
      "epoch 88: loss=0.8838563561439514\n",
      "epoch 89: loss=0.8847082257270813\n",
      "epoch 90: loss=0.8828384280204773\n",
      "epoch 91: loss=0.8831402659416199\n",
      "epoch 92: loss=0.8824083209037781\n",
      "epoch 93: loss=0.8829841017723083\n",
      "epoch 94: loss=0.8816871643066406\n",
      "epoch 95: loss=0.8813834190368652\n",
      "epoch 96: loss=0.8824372887611389\n",
      "epoch 97: loss=0.8803635835647583\n",
      "epoch 98: loss=0.8807921409606934\n",
      "epoch 99: loss=0.8803579807281494\n",
      "epoch 100: loss=0.8797219395637512\n",
      "epoch 101: loss=0.8791074156761169\n",
      "epoch 102: loss=0.8791300654411316\n",
      "epoch 103: loss=0.8786717057228088\n",
      "epoch 104: loss=0.8777491450309753\n",
      "epoch 105: loss=0.8781043887138367\n",
      "epoch 106: loss=0.8768572807312012\n",
      "epoch 107: loss=0.8765428066253662\n",
      "epoch 108: loss=0.876122236251831\n",
      "epoch 109: loss=0.8755745887756348\n",
      "epoch 110: loss=0.8741483092308044\n",
      "epoch 111: loss=0.8736491203308105\n",
      "epoch 112: loss=0.8741707801818848\n",
      "epoch 113: loss=0.8742474317550659\n",
      "epoch 114: loss=0.8741769790649414\n",
      "epoch 115: loss=0.8720878958702087\n",
      "epoch 116: loss=0.8730339407920837\n",
      "epoch 117: loss=0.871177077293396\n",
      "epoch 118: loss=0.8706777095794678\n",
      "epoch 119: loss=0.8709269762039185\n",
      "epoch 120: loss=0.8702207803726196\n",
      "epoch 121: loss=0.8702970743179321\n",
      "epoch 122: loss=0.8680547475814819\n",
      "epoch 123: loss=0.8686035871505737\n",
      "epoch 124: loss=0.8682881593704224\n",
      "epoch 125: loss=0.8659374713897705\n",
      "epoch 126: loss=0.8661337494850159\n",
      "epoch 127: loss=0.8666112422943115\n",
      "epoch 128: loss=0.8657954931259155\n",
      "epoch 129: loss=0.8645939826965332\n",
      "epoch 130: loss=0.8638166189193726\n",
      "epoch 131: loss=0.8618916273117065\n",
      "epoch 132: loss=0.8634189963340759\n",
      "epoch 133: loss=0.860069751739502\n",
      "epoch 134: loss=0.8588499426841736\n",
      "epoch 135: loss=0.8598511219024658\n",
      "epoch 136: loss=0.8587732315063477\n",
      "epoch 137: loss=0.8579679131507874\n",
      "epoch 138: loss=0.8563268780708313\n",
      "epoch 139: loss=0.8552225828170776\n",
      "epoch 140: loss=0.8541994094848633\n",
      "epoch 141: loss=0.8539356589317322\n",
      "epoch 142: loss=0.8535381555557251\n",
      "epoch 143: loss=0.8527924418449402\n",
      "epoch 144: loss=0.8528025150299072\n",
      "epoch 145: loss=0.8519405722618103\n",
      "epoch 146: loss=0.8507934808731079\n",
      "epoch 147: loss=0.8519934415817261\n",
      "epoch 148: loss=0.8512973189353943\n",
      "epoch 149: loss=0.8505477905273438\n",
      "epoch 150: loss=0.8501360416412354\n",
      "epoch 151: loss=0.849327564239502\n",
      "epoch 152: loss=0.8477597236633301\n",
      "epoch 153: loss=0.8477652668952942\n",
      "epoch 154: loss=0.8473544120788574\n",
      "epoch 155: loss=0.8479611873626709\n",
      "epoch 156: loss=0.8466150164604187\n",
      "epoch 157: loss=0.8473045229911804\n",
      "epoch 158: loss=0.8459820747375488\n",
      "epoch 159: loss=0.8460137844085693\n",
      "epoch 160: loss=0.8443657159805298\n",
      "epoch 161: loss=0.8442734479904175\n",
      "epoch 162: loss=0.8454214334487915\n",
      "epoch 163: loss=0.8441963791847229\n",
      "epoch 164: loss=0.8429513573646545\n",
      "epoch 165: loss=0.8430814743041992\n",
      "epoch 166: loss=0.842369019985199\n",
      "epoch 167: loss=0.8427110910415649\n",
      "epoch 168: loss=0.8422486186027527\n",
      "epoch 169: loss=0.8420913815498352\n",
      "epoch 170: loss=0.8408600687980652\n",
      "epoch 171: loss=0.8407613635063171\n",
      "epoch 172: loss=0.8421056866645813\n",
      "epoch 173: loss=0.8405983448028564\n",
      "epoch 174: loss=0.838987410068512\n",
      "epoch 175: loss=0.8403366804122925\n",
      "epoch 176: loss=0.839028000831604\n",
      "epoch 177: loss=0.8398734331130981\n",
      "epoch 178: loss=0.839749276638031\n",
      "epoch 179: loss=0.8379232287406921\n",
      "epoch 180: loss=0.8384122252464294\n",
      "epoch 181: loss=0.837834358215332\n",
      "epoch 182: loss=0.8376315832138062\n",
      "epoch 183: loss=0.836577296257019\n",
      "epoch 184: loss=0.8381461501121521\n",
      "epoch 185: loss=0.8345323801040649\n",
      "epoch 186: loss=0.8357958793640137\n",
      "epoch 187: loss=0.8358035683631897\n",
      "epoch 188: loss=0.8372947573661804\n",
      "epoch 189: loss=0.8352360725402832\n",
      "epoch 190: loss=0.8354790806770325\n",
      "epoch 191: loss=0.8344742655754089\n",
      "epoch 192: loss=0.834658145904541\n",
      "epoch 193: loss=0.8344698548316956\n",
      "epoch 194: loss=0.8342055082321167\n",
      "epoch 195: loss=0.8329898118972778\n",
      "epoch 196: loss=0.8317086100578308\n",
      "epoch 197: loss=0.8321393132209778\n",
      "epoch 198: loss=0.8333283066749573\n",
      "epoch 199: loss=0.8329492807388306\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=3.350538730621338\n",
      "epoch 1: loss=3.270430564880371\n",
      "epoch 2: loss=3.09732723236084\n",
      "epoch 3: loss=2.8898017406463623\n",
      "epoch 4: loss=2.6682941913604736\n",
      "epoch 5: loss=2.4157650470733643\n",
      "epoch 6: loss=2.1687097549438477\n",
      "epoch 7: loss=1.977536678314209\n",
      "epoch 8: loss=1.877870798110962\n",
      "epoch 9: loss=1.8384974002838135\n",
      "epoch 10: loss=1.7914626598358154\n",
      "epoch 11: loss=1.7231525182724\n",
      "epoch 12: loss=1.6197768449783325\n",
      "epoch 13: loss=1.517305850982666\n",
      "epoch 14: loss=1.4281127452850342\n",
      "epoch 15: loss=1.3400362730026245\n",
      "epoch 16: loss=1.2840631008148193\n",
      "epoch 17: loss=1.2397814989089966\n",
      "epoch 18: loss=1.2244629859924316\n",
      "epoch 19: loss=1.2098252773284912\n",
      "epoch 20: loss=1.1922554969787598\n",
      "epoch 21: loss=1.1698341369628906\n",
      "epoch 22: loss=1.1385310888290405\n",
      "epoch 23: loss=1.1021610498428345\n",
      "epoch 24: loss=1.0720508098602295\n",
      "epoch 25: loss=1.0534064769744873\n",
      "epoch 26: loss=1.0437835454940796\n",
      "epoch 27: loss=1.028903841972351\n",
      "epoch 28: loss=1.0167009830474854\n",
      "epoch 29: loss=1.0003706216812134\n",
      "epoch 30: loss=0.9843600392341614\n",
      "epoch 31: loss=0.9686397910118103\n",
      "epoch 32: loss=0.9580966234207153\n",
      "epoch 33: loss=0.949083149433136\n",
      "epoch 34: loss=0.9494927525520325\n",
      "epoch 35: loss=0.9450165033340454\n",
      "epoch 36: loss=0.9422743916511536\n",
      "epoch 37: loss=0.9386741518974304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: loss=0.9358625411987305\n",
      "epoch 39: loss=0.9310645461082458\n",
      "epoch 40: loss=0.9253315329551697\n",
      "epoch 41: loss=0.922662079334259\n",
      "epoch 42: loss=0.9184541702270508\n",
      "epoch 43: loss=0.9162020087242126\n",
      "epoch 44: loss=0.9114007949829102\n",
      "epoch 45: loss=0.9126311540603638\n",
      "epoch 46: loss=0.909781813621521\n",
      "epoch 47: loss=0.9086111187934875\n",
      "epoch 48: loss=0.9077298641204834\n",
      "epoch 49: loss=0.9079844951629639\n",
      "epoch 50: loss=0.9087450504302979\n",
      "epoch 51: loss=0.9073195457458496\n",
      "epoch 52: loss=0.9038164019584656\n",
      "epoch 53: loss=0.903339147567749\n",
      "epoch 54: loss=0.9032831788063049\n",
      "epoch 55: loss=0.9022058844566345\n",
      "epoch 56: loss=0.9009082913398743\n",
      "epoch 57: loss=0.900631308555603\n",
      "epoch 58: loss=0.8982632160186768\n",
      "epoch 59: loss=0.8975989818572998\n",
      "epoch 60: loss=0.8982361555099487\n",
      "epoch 61: loss=0.896817684173584\n",
      "epoch 62: loss=0.8984781503677368\n",
      "epoch 63: loss=0.8966176509857178\n",
      "epoch 64: loss=0.8955023884773254\n",
      "epoch 65: loss=0.8952913880348206\n",
      "epoch 66: loss=0.8951047658920288\n",
      "epoch 67: loss=0.8949055075645447\n",
      "epoch 68: loss=0.8933093547821045\n",
      "epoch 69: loss=0.8933454155921936\n",
      "epoch 70: loss=0.8932374119758606\n",
      "epoch 71: loss=0.89279705286026\n",
      "epoch 72: loss=0.8929786086082458\n",
      "epoch 73: loss=0.8912164568901062\n",
      "epoch 74: loss=0.8911276459693909\n",
      "epoch 75: loss=0.8910499215126038\n",
      "epoch 76: loss=0.8912525773048401\n",
      "epoch 77: loss=0.8891124725341797\n",
      "epoch 78: loss=0.8892148733139038\n",
      "epoch 79: loss=0.8897207379341125\n",
      "epoch 80: loss=0.8895139098167419\n",
      "epoch 81: loss=0.8888053297996521\n",
      "epoch 82: loss=0.8883085250854492\n",
      "epoch 83: loss=0.8880997896194458\n",
      "epoch 84: loss=0.8883054852485657\n",
      "epoch 85: loss=0.887457013130188\n",
      "epoch 86: loss=0.8877440690994263\n",
      "epoch 87: loss=0.8879686594009399\n",
      "epoch 88: loss=0.8882017731666565\n",
      "epoch 89: loss=0.8881060481071472\n",
      "epoch 90: loss=0.8871649503707886\n",
      "epoch 91: loss=0.8846389651298523\n",
      "epoch 92: loss=0.884567141532898\n",
      "epoch 93: loss=0.8848087191581726\n",
      "epoch 94: loss=0.8849779963493347\n",
      "epoch 95: loss=0.8854815363883972\n",
      "epoch 96: loss=0.8835552930831909\n",
      "epoch 97: loss=0.8848640322685242\n",
      "epoch 98: loss=0.8839243650436401\n",
      "epoch 99: loss=0.8848568797111511\n",
      "epoch 100: loss=0.8825103044509888\n",
      "epoch 101: loss=0.8839964270591736\n",
      "epoch 102: loss=0.8825026154518127\n",
      "epoch 103: loss=0.8828972578048706\n",
      "epoch 104: loss=0.8804538249969482\n",
      "epoch 105: loss=0.8828182816505432\n",
      "epoch 106: loss=0.880967915058136\n",
      "epoch 107: loss=0.8812140226364136\n",
      "epoch 108: loss=0.8809442520141602\n",
      "epoch 109: loss=0.8802618980407715\n",
      "epoch 110: loss=0.879801332950592\n",
      "epoch 111: loss=0.8810140490531921\n",
      "epoch 112: loss=0.8803018927574158\n",
      "epoch 113: loss=0.8801296353340149\n",
      "epoch 114: loss=0.8789220452308655\n",
      "epoch 115: loss=0.8791032433509827\n",
      "epoch 116: loss=0.8798210620880127\n",
      "epoch 117: loss=0.8793336749076843\n",
      "epoch 118: loss=0.8781757354736328\n",
      "epoch 119: loss=0.8779833912849426\n",
      "epoch 120: loss=0.877578616142273\n",
      "epoch 121: loss=0.8786276578903198\n",
      "epoch 122: loss=0.8777280449867249\n",
      "epoch 123: loss=0.877943754196167\n",
      "epoch 124: loss=0.876401960849762\n",
      "epoch 125: loss=0.8764511346817017\n",
      "epoch 126: loss=0.8772798180580139\n",
      "epoch 127: loss=0.8757636547088623\n",
      "epoch 128: loss=0.8752939701080322\n",
      "epoch 129: loss=0.8756175637245178\n",
      "epoch 130: loss=0.8751517534255981\n",
      "epoch 131: loss=0.8752174973487854\n",
      "epoch 132: loss=0.8753688931465149\n",
      "epoch 133: loss=0.8763119578361511\n",
      "epoch 134: loss=0.8753645420074463\n",
      "epoch 135: loss=0.8742753863334656\n",
      "epoch 136: loss=0.8739320635795593\n",
      "epoch 137: loss=0.8732905983924866\n",
      "epoch 138: loss=0.8735598921775818\n",
      "epoch 139: loss=0.8731012940406799\n",
      "epoch 140: loss=0.8731440305709839\n",
      "epoch 141: loss=0.8729003667831421\n",
      "epoch 142: loss=0.8721464276313782\n",
      "epoch 143: loss=0.8726531863212585\n",
      "epoch 144: loss=0.8713434338569641\n",
      "epoch 145: loss=0.8712973594665527\n",
      "epoch 146: loss=0.8715153336524963\n",
      "epoch 147: loss=0.8703870177268982\n",
      "epoch 148: loss=0.8686853647232056\n",
      "epoch 149: loss=0.8702182173728943\n",
      "epoch 150: loss=0.8686836957931519\n",
      "epoch 151: loss=0.8690844774246216\n",
      "epoch 152: loss=0.8701426982879639\n",
      "epoch 153: loss=0.869861364364624\n",
      "epoch 154: loss=0.8679549098014832\n",
      "epoch 155: loss=0.8692289590835571\n",
      "epoch 156: loss=0.8680345416069031\n",
      "epoch 157: loss=0.8679338693618774\n",
      "epoch 158: loss=0.8674087524414062\n",
      "epoch 159: loss=0.8661542534828186\n",
      "epoch 160: loss=0.8675873279571533\n",
      "epoch 161: loss=0.8664478659629822\n",
      "epoch 162: loss=0.865426242351532\n",
      "epoch 163: loss=0.8634485602378845\n",
      "epoch 164: loss=0.8637682795524597\n",
      "epoch 165: loss=0.8633546233177185\n",
      "epoch 166: loss=0.8642528057098389\n",
      "epoch 167: loss=0.8637521862983704\n",
      "epoch 168: loss=0.8642617464065552\n",
      "epoch 169: loss=0.8635993003845215\n",
      "epoch 170: loss=0.8624475598335266\n",
      "epoch 171: loss=0.8606199026107788\n",
      "epoch 172: loss=0.8601784706115723\n",
      "epoch 173: loss=0.8598977327346802\n",
      "epoch 174: loss=0.8600146770477295\n",
      "epoch 175: loss=0.8592837452888489\n",
      "epoch 176: loss=0.8596897721290588\n",
      "epoch 177: loss=0.8591740131378174\n",
      "epoch 178: loss=0.8575978875160217\n",
      "epoch 179: loss=0.8587324023246765\n",
      "epoch 180: loss=0.8586997985839844\n",
      "epoch 181: loss=0.857769250869751\n",
      "epoch 182: loss=0.8561098575592041\n",
      "epoch 183: loss=0.8558091521263123\n",
      "epoch 184: loss=0.8541141748428345\n",
      "epoch 185: loss=0.8548026084899902\n",
      "epoch 186: loss=0.8544287085533142\n",
      "epoch 187: loss=0.8535792231559753\n",
      "epoch 188: loss=0.853807806968689\n",
      "epoch 189: loss=0.8535178303718567\n",
      "epoch 190: loss=0.8519810438156128\n",
      "epoch 191: loss=0.8495091199874878\n",
      "epoch 192: loss=0.8511136174201965\n",
      "epoch 193: loss=0.8497684597969055\n",
      "epoch 194: loss=0.8482362627983093\n",
      "epoch 195: loss=0.8470717668533325\n",
      "epoch 196: loss=0.8466252088546753\n",
      "epoch 197: loss=0.8466212153434753\n",
      "epoch 198: loss=0.8463095426559448\n",
      "epoch 199: loss=0.8449273109436035\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=3.3632631301879883\n",
      "epoch 1: loss=3.2883777618408203\n",
      "epoch 2: loss=3.0631110668182373\n",
      "epoch 3: loss=2.8820583820343018\n",
      "epoch 4: loss=2.671231508255005\n",
      "epoch 5: loss=2.4213414192199707\n",
      "epoch 6: loss=2.1938295364379883\n",
      "epoch 7: loss=1.9544404745101929\n",
      "epoch 8: loss=1.8411319255828857\n",
      "epoch 9: loss=1.7445672750473022\n",
      "epoch 10: loss=1.6695001125335693\n",
      "epoch 11: loss=1.581599235534668\n",
      "epoch 12: loss=1.4927952289581299\n",
      "epoch 13: loss=1.4041986465454102\n",
      "epoch 14: loss=1.3305832147598267\n",
      "epoch 15: loss=1.2672340869903564\n",
      "epoch 16: loss=1.2436314821243286\n",
      "epoch 17: loss=1.2170933485031128\n",
      "epoch 18: loss=1.2120767831802368\n",
      "epoch 19: loss=1.1816444396972656\n",
      "epoch 20: loss=1.1570320129394531\n",
      "epoch 21: loss=1.140212893486023\n",
      "epoch 22: loss=1.1260310411453247\n",
      "epoch 23: loss=1.111925482749939\n",
      "epoch 24: loss=1.0852679014205933\n",
      "epoch 25: loss=1.0644668340682983\n",
      "epoch 26: loss=1.0434913635253906\n",
      "epoch 27: loss=1.0260692834854126\n",
      "epoch 28: loss=1.0144140720367432\n",
      "epoch 29: loss=1.0032985210418701\n",
      "epoch 30: loss=0.9974485039710999\n",
      "epoch 31: loss=0.9922921657562256\n",
      "epoch 32: loss=0.9926488995552063\n",
      "epoch 33: loss=0.9849003553390503\n",
      "epoch 34: loss=0.9772529602050781\n",
      "epoch 35: loss=0.9712395668029785\n",
      "epoch 36: loss=0.9662843942642212\n",
      "epoch 37: loss=0.9615784287452698\n",
      "epoch 38: loss=0.9646990299224854\n",
      "epoch 39: loss=0.9571273922920227\n",
      "epoch 40: loss=0.9593401551246643\n",
      "epoch 41: loss=0.9590067863464355\n",
      "epoch 42: loss=0.9547281861305237\n",
      "epoch 43: loss=0.9570949077606201\n",
      "epoch 44: loss=0.9521101117134094\n",
      "epoch 45: loss=0.9497870802879333\n",
      "epoch 46: loss=0.9531452059745789\n",
      "epoch 47: loss=0.9473896622657776\n",
      "epoch 48: loss=0.9481068253517151\n",
      "epoch 49: loss=0.9484595656394958\n",
      "epoch 50: loss=0.943906843662262\n",
      "epoch 51: loss=0.9420050382614136\n",
      "epoch 52: loss=0.9395432472229004\n",
      "epoch 53: loss=0.9418560266494751\n",
      "epoch 54: loss=0.9382851123809814\n",
      "epoch 55: loss=0.9390467405319214\n",
      "epoch 56: loss=0.937705934047699\n",
      "epoch 57: loss=0.9389694929122925\n",
      "epoch 58: loss=0.9391940832138062\n",
      "epoch 59: loss=0.935854434967041\n",
      "epoch 60: loss=0.9317173361778259\n",
      "epoch 61: loss=0.9317588806152344\n",
      "epoch 62: loss=0.9310926198959351\n",
      "epoch 63: loss=0.9287663102149963\n",
      "epoch 64: loss=0.9290087819099426\n",
      "epoch 65: loss=0.9316269755363464\n",
      "epoch 66: loss=0.9270058870315552\n",
      "epoch 67: loss=0.924873948097229\n",
      "epoch 68: loss=0.9226459264755249\n",
      "epoch 69: loss=0.9252943396568298\n",
      "epoch 70: loss=0.9243083000183105\n",
      "epoch 71: loss=0.9224615693092346\n",
      "epoch 72: loss=0.9197430610656738\n",
      "epoch 73: loss=0.9157415628433228\n",
      "epoch 74: loss=0.9162499308586121\n",
      "epoch 75: loss=0.915308952331543\n",
      "epoch 76: loss=0.917123019695282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77: loss=0.9144214987754822\n",
      "epoch 78: loss=0.9099203944206238\n",
      "epoch 79: loss=0.9084603190422058\n",
      "epoch 80: loss=0.9119549989700317\n",
      "epoch 81: loss=0.908737063407898\n",
      "epoch 82: loss=0.9076113700866699\n",
      "epoch 83: loss=0.9055600166320801\n",
      "epoch 84: loss=0.9042571783065796\n",
      "epoch 85: loss=0.9045106172561646\n",
      "epoch 86: loss=0.9051721096038818\n",
      "epoch 87: loss=0.9046854972839355\n",
      "epoch 88: loss=0.9021345376968384\n",
      "epoch 89: loss=0.9026337265968323\n",
      "epoch 90: loss=0.9019650816917419\n",
      "epoch 91: loss=0.9007278084754944\n",
      "epoch 92: loss=0.8962216973304749\n",
      "epoch 93: loss=0.8966234922409058\n",
      "epoch 94: loss=0.8957875967025757\n",
      "epoch 95: loss=0.8955021500587463\n",
      "epoch 96: loss=0.8925356864929199\n",
      "epoch 97: loss=0.8953922986984253\n",
      "epoch 98: loss=0.892702043056488\n",
      "epoch 99: loss=0.8929826617240906\n",
      "epoch 100: loss=0.8915943503379822\n",
      "epoch 101: loss=0.8906749486923218\n",
      "epoch 102: loss=0.8906375169754028\n",
      "epoch 103: loss=0.8896006941795349\n",
      "epoch 104: loss=0.8900307416915894\n",
      "epoch 105: loss=0.8891599774360657\n",
      "epoch 106: loss=0.8883593082427979\n",
      "epoch 107: loss=0.8877301216125488\n",
      "epoch 108: loss=0.8864514827728271\n",
      "epoch 109: loss=0.8854061365127563\n",
      "epoch 110: loss=0.8847236633300781\n",
      "epoch 111: loss=0.8855431079864502\n",
      "epoch 112: loss=0.8835622668266296\n",
      "epoch 113: loss=0.8846685290336609\n",
      "epoch 114: loss=0.8839970827102661\n",
      "epoch 115: loss=0.8841747641563416\n",
      "epoch 116: loss=0.8811829090118408\n",
      "epoch 117: loss=0.8798971772193909\n",
      "epoch 118: loss=0.8804149031639099\n",
      "epoch 119: loss=0.8831022381782532\n",
      "epoch 120: loss=0.8796120285987854\n",
      "epoch 121: loss=0.8818619847297668\n",
      "epoch 122: loss=0.8785140514373779\n",
      "epoch 123: loss=0.8761122822761536\n",
      "epoch 124: loss=0.877887487411499\n",
      "epoch 125: loss=0.8754870891571045\n",
      "epoch 126: loss=0.876910924911499\n",
      "epoch 127: loss=0.8767806887626648\n",
      "epoch 128: loss=0.8757046461105347\n",
      "epoch 129: loss=0.8768466711044312\n",
      "epoch 130: loss=0.8763729333877563\n",
      "epoch 131: loss=0.8731802701950073\n",
      "epoch 132: loss=0.8760779500007629\n",
      "epoch 133: loss=0.8744237422943115\n",
      "epoch 134: loss=0.8750877380371094\n",
      "epoch 135: loss=0.8743609189987183\n",
      "epoch 136: loss=0.8712243437767029\n",
      "epoch 137: loss=0.8743994235992432\n",
      "epoch 138: loss=0.8725700974464417\n",
      "epoch 139: loss=0.8709167838096619\n",
      "epoch 140: loss=0.8716351985931396\n",
      "epoch 141: loss=0.8696285486221313\n",
      "epoch 142: loss=0.8692944049835205\n",
      "epoch 143: loss=0.8687616586685181\n",
      "epoch 144: loss=0.8685080409049988\n",
      "epoch 145: loss=0.8697865605354309\n",
      "epoch 146: loss=0.869547426700592\n",
      "epoch 147: loss=0.8683621287345886\n",
      "epoch 148: loss=0.868863046169281\n",
      "epoch 149: loss=0.8683311343193054\n",
      "epoch 150: loss=0.869940996170044\n",
      "epoch 151: loss=0.8662314414978027\n",
      "epoch 152: loss=0.8678804636001587\n",
      "epoch 153: loss=0.866653561592102\n",
      "epoch 154: loss=0.865811288356781\n",
      "epoch 155: loss=0.8670087456703186\n",
      "epoch 156: loss=0.8629616498947144\n",
      "epoch 157: loss=0.8674290180206299\n",
      "epoch 158: loss=0.868204653263092\n",
      "epoch 159: loss=0.8648006916046143\n",
      "epoch 160: loss=0.8659040331840515\n",
      "epoch 161: loss=0.8654451966285706\n",
      "epoch 162: loss=0.8638997077941895\n",
      "epoch 163: loss=0.8653629422187805\n",
      "epoch 164: loss=0.8652611374855042\n",
      "epoch 165: loss=0.8631926774978638\n",
      "epoch 166: loss=0.8645300269126892\n",
      "epoch 167: loss=0.8615901470184326\n",
      "epoch 168: loss=0.8657471537590027\n",
      "epoch 169: loss=0.862292468547821\n",
      "epoch 170: loss=0.8610573410987854\n",
      "epoch 171: loss=0.8601408004760742\n",
      "epoch 172: loss=0.862877368927002\n",
      "epoch 173: loss=0.8647537231445312\n",
      "epoch 174: loss=0.8623130917549133\n",
      "epoch 175: loss=0.8615473508834839\n",
      "epoch 176: loss=0.8630723357200623\n",
      "epoch 177: loss=0.8619917035102844\n",
      "epoch 178: loss=0.8605900406837463\n",
      "epoch 179: loss=0.8596432209014893\n",
      "epoch 180: loss=0.8590990900993347\n",
      "epoch 181: loss=0.8614156246185303\n",
      "epoch 182: loss=0.8592596054077148\n",
      "epoch 183: loss=0.8605776429176331\n",
      "epoch 184: loss=0.8607012033462524\n",
      "epoch 185: loss=0.8579025864601135\n",
      "epoch 186: loss=0.8579398393630981\n",
      "epoch 187: loss=0.8598349094390869\n",
      "epoch 188: loss=0.8580220937728882\n",
      "epoch 189: loss=0.8574745059013367\n",
      "epoch 190: loss=0.8586729168891907\n",
      "epoch 191: loss=0.8593265414237976\n",
      "epoch 192: loss=0.8601785898208618\n",
      "epoch 193: loss=0.8575090765953064\n",
      "epoch 194: loss=0.8577502965927124\n",
      "epoch 195: loss=0.8568271398544312\n",
      "epoch 196: loss=0.8588975071907043\n",
      "epoch 197: loss=0.8584553003311157\n",
      "epoch 198: loss=0.8581501245498657\n",
      "epoch 199: loss=0.8548536896705627\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=3.2692017555236816\n",
      "epoch 1: loss=3.384655237197876\n",
      "epoch 2: loss=3.221872568130493\n",
      "epoch 3: loss=3.51204776763916\n",
      "epoch 4: loss=2.6934728622436523\n",
      "epoch 5: loss=2.8590080738067627\n",
      "epoch 6: loss=2.609241485595703\n",
      "epoch 7: loss=2.9657487869262695\n",
      "epoch 8: loss=3.299304962158203\n",
      "epoch 9: loss=2.3172428607940674\n",
      "epoch 10: loss=2.3573992252349854\n",
      "epoch 11: loss=2.082885265350342\n",
      "epoch 12: loss=1.9124895334243774\n",
      "epoch 13: loss=2.101233959197998\n",
      "epoch 14: loss=1.6514370441436768\n",
      "epoch 15: loss=1.8431180715560913\n",
      "epoch 16: loss=1.8200794458389282\n",
      "epoch 17: loss=1.5887645483016968\n",
      "epoch 18: loss=1.5666453838348389\n",
      "epoch 19: loss=1.6990946531295776\n",
      "epoch 20: loss=1.538414478302002\n",
      "epoch 21: loss=1.5191264152526855\n",
      "epoch 22: loss=1.4410467147827148\n",
      "epoch 23: loss=1.5446488857269287\n",
      "epoch 24: loss=1.4867520332336426\n",
      "epoch 25: loss=1.4188121557235718\n",
      "epoch 26: loss=1.4434376955032349\n",
      "epoch 27: loss=1.4814962148666382\n",
      "epoch 28: loss=1.3700835704803467\n",
      "epoch 29: loss=1.3381072282791138\n",
      "epoch 30: loss=1.4227538108825684\n",
      "epoch 31: loss=1.5014472007751465\n",
      "epoch 32: loss=1.4079726934432983\n",
      "epoch 33: loss=1.4133480787277222\n",
      "epoch 34: loss=1.4234801530838013\n",
      "epoch 35: loss=1.3743778467178345\n",
      "epoch 36: loss=1.4436784982681274\n",
      "epoch 37: loss=1.4216159582138062\n",
      "epoch 38: loss=1.3700248003005981\n",
      "epoch 39: loss=1.4884780645370483\n",
      "epoch 40: loss=1.4878419637680054\n",
      "epoch 41: loss=1.359966516494751\n",
      "epoch 42: loss=1.43924880027771\n",
      "epoch 43: loss=1.4540928602218628\n",
      "epoch 44: loss=1.4382506608963013\n",
      "epoch 45: loss=1.379440188407898\n",
      "epoch 46: loss=1.4698301553726196\n",
      "epoch 47: loss=1.4333652257919312\n",
      "epoch 48: loss=1.3845345973968506\n",
      "epoch 49: loss=1.4053144454956055\n",
      "epoch 50: loss=1.4171133041381836\n",
      "epoch 51: loss=1.3749315738677979\n",
      "epoch 52: loss=1.367923378944397\n",
      "epoch 53: loss=1.371886134147644\n",
      "epoch 54: loss=1.4087445735931396\n",
      "epoch 55: loss=1.4487253427505493\n",
      "epoch 56: loss=1.3938812017440796\n",
      "epoch 57: loss=1.4285871982574463\n",
      "epoch 58: loss=1.3941563367843628\n",
      "epoch 59: loss=1.3759450912475586\n",
      "epoch 60: loss=1.458328366279602\n",
      "epoch 61: loss=1.4781969785690308\n",
      "epoch 62: loss=1.4248384237289429\n",
      "epoch 63: loss=1.3835896253585815\n",
      "epoch 64: loss=1.3691914081573486\n",
      "epoch 65: loss=1.4125938415527344\n",
      "epoch 66: loss=1.381090521812439\n",
      "epoch 67: loss=1.3971238136291504\n",
      "epoch 68: loss=1.3691571950912476\n",
      "epoch 69: loss=1.3798679113388062\n",
      "epoch 70: loss=1.399876356124878\n",
      "epoch 71: loss=1.352349042892456\n",
      "epoch 72: loss=1.4076499938964844\n",
      "epoch 73: loss=1.4694759845733643\n",
      "epoch 74: loss=1.4045708179473877\n",
      "epoch 75: loss=1.4101200103759766\n",
      "epoch 76: loss=1.4237945079803467\n",
      "epoch 77: loss=1.3915902376174927\n",
      "epoch 78: loss=1.3798304796218872\n",
      "epoch 79: loss=1.4176045656204224\n",
      "epoch 80: loss=1.4371345043182373\n",
      "epoch 81: loss=1.3851572275161743\n",
      "epoch 82: loss=1.4156646728515625\n",
      "epoch 83: loss=1.3555759191513062\n",
      "epoch 84: loss=1.3840383291244507\n",
      "epoch 85: loss=1.3566354513168335\n",
      "epoch 86: loss=1.4399675130844116\n",
      "epoch 87: loss=1.4224010705947876\n",
      "epoch 88: loss=1.3953652381896973\n",
      "epoch 89: loss=1.411049246788025\n",
      "epoch 90: loss=1.3604865074157715\n",
      "epoch 91: loss=1.4435497522354126\n",
      "epoch 92: loss=1.4325830936431885\n",
      "epoch 93: loss=1.489522099494934\n",
      "epoch 94: loss=1.4078603982925415\n",
      "epoch 95: loss=1.3374603986740112\n",
      "epoch 96: loss=1.406506896018982\n",
      "epoch 97: loss=1.3688311576843262\n",
      "epoch 98: loss=1.3820747137069702\n",
      "epoch 99: loss=1.3809611797332764\n",
      "epoch 100: loss=1.3303663730621338\n",
      "epoch 101: loss=1.3795018196105957\n",
      "epoch 102: loss=1.4545427560806274\n",
      "epoch 103: loss=1.4063692092895508\n",
      "epoch 104: loss=1.488914966583252\n",
      "epoch 105: loss=1.3902417421340942\n",
      "epoch 106: loss=1.362729787826538\n",
      "epoch 107: loss=1.3733309507369995\n",
      "epoch 108: loss=1.3592618703842163\n",
      "epoch 109: loss=1.356155514717102\n",
      "epoch 110: loss=1.416419506072998\n",
      "epoch 111: loss=1.3464308977127075\n",
      "epoch 112: loss=1.4411228895187378\n",
      "epoch 113: loss=1.4651343822479248\n",
      "epoch 114: loss=1.3561581373214722\n",
      "epoch 115: loss=1.3565077781677246\n",
      "epoch 116: loss=1.417514681816101\n",
      "epoch 117: loss=1.4212385416030884\n",
      "epoch 118: loss=1.3995345830917358\n",
      "epoch 119: loss=1.4540866613388062\n",
      "epoch 120: loss=1.356235384941101\n",
      "epoch 121: loss=1.4298051595687866\n",
      "epoch 122: loss=1.3940484523773193\n",
      "epoch 123: loss=1.4457062482833862\n",
      "epoch 124: loss=1.4294832944869995\n",
      "epoch 125: loss=1.350058913230896\n",
      "epoch 126: loss=1.4139823913574219\n",
      "epoch 127: loss=1.3554481267929077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 128: loss=1.403228759765625\n",
      "epoch 129: loss=1.403459906578064\n",
      "epoch 130: loss=1.3698235750198364\n",
      "epoch 131: loss=1.424861192703247\n",
      "epoch 132: loss=1.4801326990127563\n",
      "epoch 133: loss=1.4249908924102783\n",
      "epoch 134: loss=1.372894525527954\n",
      "epoch 135: loss=1.4384506940841675\n",
      "epoch 136: loss=1.3725333213806152\n",
      "epoch 137: loss=1.4035862684249878\n",
      "epoch 138: loss=1.448933720588684\n",
      "epoch 139: loss=1.4370428323745728\n",
      "epoch 140: loss=1.3864883184432983\n",
      "epoch 141: loss=1.4663569927215576\n",
      "epoch 142: loss=1.4175732135772705\n",
      "epoch 143: loss=1.3927245140075684\n",
      "epoch 144: loss=1.4315046072006226\n",
      "epoch 145: loss=1.4554017782211304\n",
      "epoch 146: loss=1.385933756828308\n",
      "epoch 147: loss=1.3707680702209473\n",
      "epoch 148: loss=1.3775897026062012\n",
      "epoch 149: loss=1.36544930934906\n",
      "epoch 150: loss=1.3964121341705322\n",
      "epoch 151: loss=1.3889758586883545\n",
      "epoch 152: loss=1.4027787446975708\n",
      "epoch 153: loss=1.43816339969635\n",
      "epoch 154: loss=1.5469025373458862\n",
      "epoch 155: loss=1.4495539665222168\n",
      "epoch 156: loss=1.3975011110305786\n",
      "epoch 157: loss=1.4117215871810913\n",
      "epoch 158: loss=1.4546812772750854\n",
      "epoch 159: loss=1.4482128620147705\n",
      "epoch 160: loss=1.3270971775054932\n",
      "epoch 161: loss=1.441908597946167\n",
      "epoch 162: loss=1.450331687927246\n",
      "epoch 163: loss=1.36564040184021\n",
      "epoch 164: loss=1.3827365636825562\n",
      "epoch 165: loss=1.4388068914413452\n",
      "epoch 166: loss=1.4081902503967285\n",
      "epoch 167: loss=1.4615540504455566\n",
      "epoch 168: loss=1.4336103200912476\n",
      "epoch 169: loss=1.4040977954864502\n",
      "epoch 170: loss=1.414994716644287\n",
      "epoch 171: loss=1.3734357357025146\n",
      "epoch 172: loss=1.3891183137893677\n",
      "epoch 173: loss=1.3717119693756104\n",
      "epoch 174: loss=1.4536170959472656\n",
      "epoch 175: loss=1.374748706817627\n",
      "epoch 176: loss=1.3774027824401855\n",
      "epoch 177: loss=1.4184907674789429\n",
      "epoch 178: loss=1.4771676063537598\n",
      "epoch 179: loss=1.4388697147369385\n",
      "epoch 180: loss=1.3891702890396118\n",
      "epoch 181: loss=1.3864482641220093\n",
      "epoch 182: loss=1.4200342893600464\n",
      "epoch 183: loss=1.4562071561813354\n",
      "epoch 184: loss=1.359086513519287\n",
      "epoch 185: loss=1.3528881072998047\n",
      "epoch 186: loss=1.4345844984054565\n",
      "epoch 187: loss=1.4236692190170288\n",
      "epoch 188: loss=1.39524245262146\n",
      "epoch 189: loss=1.3576934337615967\n",
      "epoch 190: loss=1.4240846633911133\n",
      "epoch 191: loss=1.3740659952163696\n",
      "epoch 192: loss=1.3740763664245605\n",
      "epoch 193: loss=1.401795744895935\n",
      "epoch 194: loss=1.3913131952285767\n",
      "epoch 195: loss=1.3968809843063354\n",
      "epoch 196: loss=1.3980416059494019\n",
      "epoch 197: loss=1.3857821226119995\n",
      "epoch 198: loss=1.3581836223602295\n",
      "epoch 199: loss=1.342565894126892\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=3.3997974395751953\n",
      "epoch 1: loss=3.2653470039367676\n",
      "epoch 2: loss=3.0738303661346436\n",
      "epoch 3: loss=2.902470111846924\n",
      "epoch 4: loss=2.699640989303589\n",
      "epoch 5: loss=2.4135656356811523\n",
      "epoch 6: loss=2.1539061069488525\n",
      "epoch 7: loss=2.005847454071045\n",
      "epoch 8: loss=1.9367260932922363\n",
      "epoch 9: loss=1.8938852548599243\n",
      "epoch 10: loss=1.8534806966781616\n",
      "epoch 11: loss=1.7733376026153564\n",
      "epoch 12: loss=1.6852689981460571\n",
      "epoch 13: loss=1.5755338668823242\n",
      "epoch 14: loss=1.485608696937561\n",
      "epoch 15: loss=1.4226667881011963\n",
      "epoch 16: loss=1.3621680736541748\n",
      "epoch 17: loss=1.3313055038452148\n",
      "epoch 18: loss=1.3081368207931519\n",
      "epoch 19: loss=1.2863469123840332\n",
      "epoch 20: loss=1.2595505714416504\n",
      "epoch 21: loss=1.226261854171753\n",
      "epoch 22: loss=1.1921098232269287\n",
      "epoch 23: loss=1.1527137756347656\n",
      "epoch 24: loss=1.1226180791854858\n",
      "epoch 25: loss=1.0970076322555542\n",
      "epoch 26: loss=1.0781850814819336\n",
      "epoch 27: loss=1.0591834783554077\n",
      "epoch 28: loss=1.0406395196914673\n",
      "epoch 29: loss=1.027590036392212\n",
      "epoch 30: loss=1.0068312883377075\n",
      "epoch 31: loss=0.9835018515586853\n",
      "epoch 32: loss=0.9683477282524109\n",
      "epoch 33: loss=0.9579275846481323\n",
      "epoch 34: loss=0.9506588578224182\n",
      "epoch 35: loss=0.9420238137245178\n",
      "epoch 36: loss=0.9344801306724548\n",
      "epoch 37: loss=0.9253385066986084\n",
      "epoch 38: loss=0.9175668358802795\n",
      "epoch 39: loss=0.9114780426025391\n",
      "epoch 40: loss=0.9048822522163391\n",
      "epoch 41: loss=0.9017177820205688\n",
      "epoch 42: loss=0.8966559767723083\n",
      "epoch 43: loss=0.8941616415977478\n",
      "epoch 44: loss=0.8899639844894409\n",
      "epoch 45: loss=0.8871015906333923\n",
      "epoch 46: loss=0.8844104409217834\n",
      "epoch 47: loss=0.8824246525764465\n",
      "epoch 48: loss=0.8795201182365417\n",
      "epoch 49: loss=0.8762466907501221\n",
      "epoch 50: loss=0.874875545501709\n",
      "epoch 51: loss=0.8723466396331787\n",
      "epoch 52: loss=0.8687286376953125\n",
      "epoch 53: loss=0.8684912919998169\n",
      "epoch 54: loss=0.867049515247345\n",
      "epoch 55: loss=0.8643500804901123\n",
      "epoch 56: loss=0.863879919052124\n",
      "epoch 57: loss=0.8621888756752014\n",
      "epoch 58: loss=0.860906720161438\n",
      "epoch 59: loss=0.8613373637199402\n",
      "epoch 60: loss=0.8593888282775879\n",
      "epoch 61: loss=0.856594443321228\n",
      "epoch 62: loss=0.8584651350975037\n",
      "epoch 63: loss=0.8575282692909241\n",
      "epoch 64: loss=0.8560619950294495\n",
      "epoch 65: loss=0.855385959148407\n",
      "epoch 66: loss=0.8546164631843567\n",
      "epoch 67: loss=0.8543147444725037\n",
      "epoch 68: loss=0.8512533903121948\n",
      "epoch 69: loss=0.8513554930686951\n",
      "epoch 70: loss=0.8509737849235535\n",
      "epoch 71: loss=0.8498515486717224\n",
      "epoch 72: loss=0.8500931262969971\n",
      "epoch 73: loss=0.8499470353126526\n",
      "epoch 74: loss=0.848092257976532\n",
      "epoch 75: loss=0.8486345410346985\n",
      "epoch 76: loss=0.847304105758667\n",
      "epoch 77: loss=0.8461769819259644\n",
      "epoch 78: loss=0.8455981016159058\n",
      "epoch 79: loss=0.8438697457313538\n",
      "epoch 80: loss=0.8432299494743347\n",
      "epoch 81: loss=0.8432003259658813\n",
      "epoch 82: loss=0.8425151705741882\n",
      "epoch 83: loss=0.840834379196167\n",
      "epoch 84: loss=0.841838002204895\n",
      "epoch 85: loss=0.8393672704696655\n",
      "epoch 86: loss=0.8402395844459534\n",
      "epoch 87: loss=0.8384290337562561\n",
      "epoch 88: loss=0.8400187492370605\n",
      "epoch 89: loss=0.8383076786994934\n",
      "epoch 90: loss=0.8364687561988831\n",
      "epoch 91: loss=0.8371840119361877\n",
      "epoch 92: loss=0.8361608982086182\n",
      "epoch 93: loss=0.8361819982528687\n",
      "epoch 94: loss=0.8355111479759216\n",
      "epoch 95: loss=0.834244966506958\n",
      "epoch 96: loss=0.8349458575248718\n",
      "epoch 97: loss=0.8352450728416443\n",
      "epoch 98: loss=0.8329420685768127\n",
      "epoch 99: loss=0.8336334824562073\n",
      "epoch 100: loss=0.8334406614303589\n",
      "epoch 101: loss=0.8331157565116882\n",
      "epoch 102: loss=0.832173764705658\n",
      "epoch 103: loss=0.8313084244728088\n",
      "epoch 104: loss=0.832074761390686\n",
      "epoch 105: loss=0.8317546248435974\n",
      "epoch 106: loss=0.8317297101020813\n",
      "epoch 107: loss=0.8290955424308777\n",
      "epoch 108: loss=0.8302235007286072\n",
      "epoch 109: loss=0.830066442489624\n",
      "epoch 110: loss=0.8301854729652405\n",
      "epoch 111: loss=0.8279041051864624\n",
      "epoch 112: loss=0.8297533392906189\n",
      "epoch 113: loss=0.8276060223579407\n",
      "epoch 114: loss=0.8280322551727295\n",
      "epoch 115: loss=0.8289515376091003\n",
      "epoch 116: loss=0.8292643427848816\n",
      "epoch 117: loss=0.8283662796020508\n",
      "epoch 118: loss=0.8262768983840942\n",
      "epoch 119: loss=0.8266299366950989\n",
      "epoch 120: loss=0.826926052570343\n",
      "epoch 121: loss=0.8271743655204773\n",
      "epoch 122: loss=0.826775312423706\n",
      "epoch 123: loss=0.8276686072349548\n",
      "epoch 124: loss=0.8273285031318665\n",
      "epoch 125: loss=0.8277067542076111\n",
      "epoch 126: loss=0.8270274996757507\n",
      "epoch 127: loss=0.828434407711029\n",
      "epoch 128: loss=0.825478196144104\n",
      "epoch 129: loss=0.8262000679969788\n",
      "epoch 130: loss=0.8266653418540955\n",
      "epoch 131: loss=0.8244723677635193\n",
      "epoch 132: loss=0.8268287777900696\n",
      "epoch 133: loss=0.8260473012924194\n",
      "epoch 134: loss=0.827042281627655\n",
      "epoch 135: loss=0.8265074491500854\n",
      "epoch 136: loss=0.8254333734512329\n",
      "epoch 137: loss=0.8251363039016724\n",
      "epoch 138: loss=0.8234399557113647\n",
      "epoch 139: loss=0.8247250318527222\n",
      "epoch 140: loss=0.8238043189048767\n",
      "epoch 141: loss=0.8262912631034851\n",
      "epoch 142: loss=0.8256120681762695\n",
      "epoch 143: loss=0.8230382800102234\n",
      "epoch 144: loss=0.8224584460258484\n",
      "epoch 145: loss=0.8250858783721924\n",
      "epoch 146: loss=0.8240658640861511\n",
      "epoch 147: loss=0.82490473985672\n",
      "epoch 148: loss=0.8254302144050598\n",
      "epoch 149: loss=0.8243932127952576\n",
      "epoch 150: loss=0.8224477767944336\n",
      "epoch 151: loss=0.8235310912132263\n",
      "epoch 152: loss=0.8236137628555298\n",
      "epoch 153: loss=0.8250254392623901\n",
      "epoch 154: loss=0.8225274085998535\n",
      "epoch 155: loss=0.8237358331680298\n",
      "epoch 156: loss=0.82232266664505\n",
      "epoch 157: loss=0.8222787976264954\n",
      "epoch 158: loss=0.8223152756690979\n",
      "epoch 159: loss=0.8215767741203308\n",
      "epoch 160: loss=0.8227673172950745\n",
      "epoch 161: loss=0.8228256106376648\n",
      "epoch 162: loss=0.823344886302948\n",
      "epoch 163: loss=0.8217251896858215\n",
      "epoch 164: loss=0.8233250379562378\n",
      "epoch 165: loss=0.823161244392395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 166: loss=0.8222336769104004\n",
      "epoch 167: loss=0.8215314149856567\n",
      "epoch 168: loss=0.8206286430358887\n",
      "epoch 169: loss=0.8226398229598999\n",
      "epoch 170: loss=0.8211331367492676\n",
      "epoch 171: loss=0.8208786249160767\n",
      "epoch 172: loss=0.8193814754486084\n",
      "epoch 173: loss=0.8212470412254333\n",
      "epoch 174: loss=0.8216792941093445\n",
      "epoch 175: loss=0.8204630017280579\n",
      "epoch 176: loss=0.8208373188972473\n",
      "epoch 177: loss=0.8213520050048828\n",
      "epoch 178: loss=0.8204180002212524\n",
      "epoch 179: loss=0.8201457262039185\n",
      "epoch 180: loss=0.8170930743217468\n",
      "epoch 181: loss=0.8206396698951721\n",
      "epoch 182: loss=0.8192713856697083\n",
      "epoch 183: loss=0.8188873529434204\n",
      "epoch 184: loss=0.8177047371864319\n",
      "epoch 185: loss=0.8197407722473145\n",
      "epoch 186: loss=0.8181790709495544\n",
      "epoch 187: loss=0.8195943832397461\n",
      "epoch 188: loss=0.8164938688278198\n",
      "epoch 189: loss=0.8184218406677246\n",
      "epoch 190: loss=0.8169859647750854\n",
      "epoch 191: loss=0.816106915473938\n",
      "epoch 192: loss=0.816874086856842\n",
      "epoch 193: loss=0.8170486092567444\n",
      "epoch 194: loss=0.8174766898155212\n",
      "epoch 195: loss=0.8165156841278076\n",
      "epoch 196: loss=0.8168039321899414\n",
      "epoch 197: loss=0.8154833912849426\n",
      "epoch 198: loss=0.8156332969665527\n",
      "epoch 199: loss=0.8162872791290283\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=3.4085512161254883\n",
      "epoch 1: loss=3.244391918182373\n",
      "epoch 2: loss=3.128108501434326\n",
      "epoch 3: loss=2.990294933319092\n",
      "epoch 4: loss=2.7843689918518066\n",
      "epoch 5: loss=2.6132431030273438\n",
      "epoch 6: loss=2.388414144515991\n",
      "epoch 7: loss=2.194493532180786\n",
      "epoch 8: loss=2.062917470932007\n",
      "epoch 9: loss=1.9162875413894653\n",
      "epoch 10: loss=1.8166086673736572\n",
      "epoch 11: loss=1.743315577507019\n",
      "epoch 12: loss=1.6593232154846191\n",
      "epoch 13: loss=1.5889846086502075\n",
      "epoch 14: loss=1.4876900911331177\n",
      "epoch 15: loss=1.393205165863037\n",
      "epoch 16: loss=1.3205314874649048\n",
      "epoch 17: loss=1.2594335079193115\n",
      "epoch 18: loss=1.2208101749420166\n",
      "epoch 19: loss=1.1935261487960815\n",
      "epoch 20: loss=1.159171223640442\n",
      "epoch 21: loss=1.1294457912445068\n",
      "epoch 22: loss=1.0952093601226807\n",
      "epoch 23: loss=1.057249903678894\n",
      "epoch 24: loss=1.0317946672439575\n",
      "epoch 25: loss=1.014502763748169\n",
      "epoch 26: loss=1.001767635345459\n",
      "epoch 27: loss=0.9903380870819092\n",
      "epoch 28: loss=0.9825652837753296\n",
      "epoch 29: loss=0.9663829803466797\n",
      "epoch 30: loss=0.956692099571228\n",
      "epoch 31: loss=0.9448025822639465\n",
      "epoch 32: loss=0.933864176273346\n",
      "epoch 33: loss=0.9286405444145203\n",
      "epoch 34: loss=0.9240785837173462\n",
      "epoch 35: loss=0.9220691919326782\n",
      "epoch 36: loss=0.9190843105316162\n",
      "epoch 37: loss=0.9128630757331848\n",
      "epoch 38: loss=0.9086812734603882\n",
      "epoch 39: loss=0.8990018367767334\n",
      "epoch 40: loss=0.8937098979949951\n",
      "epoch 41: loss=0.8942270874977112\n",
      "epoch 42: loss=0.892392098903656\n",
      "epoch 43: loss=0.8890218138694763\n",
      "epoch 44: loss=0.8869219422340393\n",
      "epoch 45: loss=0.884056568145752\n",
      "epoch 46: loss=0.8830081820487976\n",
      "epoch 47: loss=0.8798375129699707\n",
      "epoch 48: loss=0.8807317018508911\n",
      "epoch 49: loss=0.8794935941696167\n",
      "epoch 50: loss=0.8784738779067993\n",
      "epoch 51: loss=0.8753743767738342\n",
      "epoch 52: loss=0.8762381076812744\n",
      "epoch 53: loss=0.8751125335693359\n",
      "epoch 54: loss=0.8767016530036926\n",
      "epoch 55: loss=0.8752530813217163\n",
      "epoch 56: loss=0.8729620575904846\n",
      "epoch 57: loss=0.8737276196479797\n",
      "epoch 58: loss=0.8724606037139893\n",
      "epoch 59: loss=0.8706035614013672\n",
      "epoch 60: loss=0.8687729239463806\n",
      "epoch 61: loss=0.869789183139801\n",
      "epoch 62: loss=0.8694533109664917\n",
      "epoch 63: loss=0.8681437373161316\n",
      "epoch 64: loss=0.8696561455726624\n",
      "epoch 65: loss=0.8690642714500427\n",
      "epoch 66: loss=0.8661181926727295\n",
      "epoch 67: loss=0.8683855533599854\n",
      "epoch 68: loss=0.867368221282959\n",
      "epoch 69: loss=0.8658980131149292\n",
      "epoch 70: loss=0.8652506470680237\n",
      "epoch 71: loss=0.8657951951026917\n",
      "epoch 72: loss=0.8659238815307617\n",
      "epoch 73: loss=0.8649972081184387\n",
      "epoch 74: loss=0.8647435307502747\n",
      "epoch 75: loss=0.8646881580352783\n",
      "epoch 76: loss=0.8633849620819092\n",
      "epoch 77: loss=0.864415168762207\n",
      "epoch 78: loss=0.8645200133323669\n",
      "epoch 79: loss=0.8637270927429199\n",
      "epoch 80: loss=0.8630078434944153\n",
      "epoch 81: loss=0.864082396030426\n",
      "epoch 82: loss=0.861661970615387\n",
      "epoch 83: loss=0.8620799779891968\n",
      "epoch 84: loss=0.86350017786026\n",
      "epoch 85: loss=0.861133337020874\n",
      "epoch 86: loss=0.8630715608596802\n",
      "epoch 87: loss=0.8618927597999573\n",
      "epoch 88: loss=0.8619832396507263\n",
      "epoch 89: loss=0.8615164756774902\n",
      "epoch 90: loss=0.8593862652778625\n",
      "epoch 91: loss=0.8613657355308533\n",
      "epoch 92: loss=0.8613753914833069\n",
      "epoch 93: loss=0.8612660765647888\n",
      "epoch 94: loss=0.860520601272583\n",
      "epoch 95: loss=0.8604182004928589\n",
      "epoch 96: loss=0.8600512742996216\n",
      "epoch 97: loss=0.8594419956207275\n",
      "epoch 98: loss=0.858354926109314\n",
      "epoch 99: loss=0.8584998846054077\n",
      "epoch 100: loss=0.8589690327644348\n",
      "epoch 101: loss=0.8606516718864441\n",
      "epoch 102: loss=0.8592419028282166\n",
      "epoch 103: loss=0.8583933115005493\n",
      "epoch 104: loss=0.8589193820953369\n",
      "epoch 105: loss=0.8584789037704468\n",
      "epoch 106: loss=0.8597846627235413\n",
      "epoch 107: loss=0.8572552800178528\n",
      "epoch 108: loss=0.857708215713501\n",
      "epoch 109: loss=0.856187641620636\n",
      "epoch 110: loss=0.8574583530426025\n",
      "epoch 111: loss=0.8580042719841003\n",
      "epoch 112: loss=0.8562638759613037\n",
      "epoch 113: loss=0.8583926558494568\n",
      "epoch 114: loss=0.8581087589263916\n",
      "epoch 115: loss=0.8568359613418579\n",
      "epoch 116: loss=0.8568095564842224\n",
      "epoch 117: loss=0.8556864261627197\n",
      "epoch 118: loss=0.857573390007019\n",
      "epoch 119: loss=0.8562111854553223\n",
      "epoch 120: loss=0.8556979298591614\n",
      "epoch 121: loss=0.8556380867958069\n",
      "epoch 122: loss=0.8560068011283875\n",
      "epoch 123: loss=0.8563898205757141\n",
      "epoch 124: loss=0.854931652545929\n",
      "epoch 125: loss=0.8550460934638977\n",
      "epoch 126: loss=0.8546091318130493\n",
      "epoch 127: loss=0.854950487613678\n",
      "epoch 128: loss=0.8544270992279053\n",
      "epoch 129: loss=0.8543537259101868\n",
      "epoch 130: loss=0.8531166315078735\n",
      "epoch 131: loss=0.854928731918335\n",
      "epoch 132: loss=0.8534988164901733\n",
      "epoch 133: loss=0.8534610867500305\n",
      "epoch 134: loss=0.8538641929626465\n",
      "epoch 135: loss=0.8531134724617004\n",
      "epoch 136: loss=0.8525959849357605\n",
      "epoch 137: loss=0.854074239730835\n",
      "epoch 138: loss=0.853142499923706\n",
      "epoch 139: loss=0.8522079586982727\n",
      "epoch 140: loss=0.8538049459457397\n",
      "epoch 141: loss=0.852949321269989\n",
      "epoch 142: loss=0.8519744873046875\n",
      "epoch 143: loss=0.8517754673957825\n",
      "epoch 144: loss=0.8519933819770813\n",
      "epoch 145: loss=0.8525782823562622\n",
      "epoch 146: loss=0.8519673347473145\n",
      "epoch 147: loss=0.8498180508613586\n",
      "epoch 148: loss=0.8512366414070129\n",
      "epoch 149: loss=0.8503423929214478\n",
      "epoch 150: loss=0.8511939644813538\n",
      "epoch 151: loss=0.8509705662727356\n",
      "epoch 152: loss=0.849760890007019\n",
      "epoch 153: loss=0.8504858016967773\n",
      "epoch 154: loss=0.8513106107711792\n",
      "epoch 155: loss=0.8502488732337952\n",
      "epoch 156: loss=0.8499637246131897\n",
      "epoch 157: loss=0.8504603505134583\n",
      "epoch 158: loss=0.8508616089820862\n",
      "epoch 159: loss=0.8498594760894775\n",
      "epoch 160: loss=0.8494787216186523\n",
      "epoch 161: loss=0.8500213623046875\n",
      "epoch 162: loss=0.8484843969345093\n",
      "epoch 163: loss=0.849145770072937\n",
      "epoch 164: loss=0.8488057255744934\n",
      "epoch 165: loss=0.8483522534370422\n",
      "epoch 166: loss=0.8486830592155457\n",
      "epoch 167: loss=0.8492035865783691\n",
      "epoch 168: loss=0.8494071364402771\n",
      "epoch 169: loss=0.8472532033920288\n",
      "epoch 170: loss=0.8466632962226868\n",
      "epoch 171: loss=0.8456818461418152\n",
      "epoch 172: loss=0.8460081815719604\n",
      "epoch 173: loss=0.8472834825515747\n",
      "epoch 174: loss=0.8454148173332214\n",
      "epoch 175: loss=0.8449628353118896\n",
      "epoch 176: loss=0.8427208662033081\n",
      "epoch 177: loss=0.8449465036392212\n",
      "epoch 178: loss=0.8433307409286499\n",
      "epoch 179: loss=0.8434728980064392\n",
      "epoch 180: loss=0.8418092131614685\n",
      "epoch 181: loss=0.8424896597862244\n",
      "epoch 182: loss=0.843458890914917\n",
      "epoch 183: loss=0.8417857885360718\n",
      "epoch 184: loss=0.8409982919692993\n",
      "epoch 185: loss=0.8392593860626221\n",
      "epoch 186: loss=0.8392407894134521\n",
      "epoch 187: loss=0.8375127911567688\n",
      "epoch 188: loss=0.8394250869750977\n",
      "epoch 189: loss=0.8390282988548279\n",
      "epoch 190: loss=0.8363357782363892\n",
      "epoch 191: loss=0.8374165296554565\n",
      "epoch 192: loss=0.8367609977722168\n",
      "epoch 193: loss=0.8354559540748596\n",
      "epoch 194: loss=0.8357070088386536\n",
      "epoch 195: loss=0.8358995914459229\n",
      "epoch 196: loss=0.8337091207504272\n",
      "epoch 197: loss=0.8358380198478699\n",
      "epoch 198: loss=0.8352394700050354\n",
      "epoch 199: loss=0.8327281475067139\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=3.418334484100342\n",
      "epoch 1: loss=3.2427661418914795\n",
      "epoch 2: loss=3.1095130443573\n",
      "epoch 3: loss=2.861668109893799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: loss=2.6650497913360596\n",
      "epoch 5: loss=2.4037041664123535\n",
      "epoch 6: loss=2.163698196411133\n",
      "epoch 7: loss=1.9856301546096802\n",
      "epoch 8: loss=1.8421283960342407\n",
      "epoch 9: loss=1.7934921979904175\n",
      "epoch 10: loss=1.745097041130066\n",
      "epoch 11: loss=1.6746360063552856\n",
      "epoch 12: loss=1.5960750579833984\n",
      "epoch 13: loss=1.5057471990585327\n",
      "epoch 14: loss=1.415786623954773\n",
      "epoch 15: loss=1.3455109596252441\n",
      "epoch 16: loss=1.2869081497192383\n",
      "epoch 17: loss=1.2515312433242798\n",
      "epoch 18: loss=1.2247921228408813\n",
      "epoch 19: loss=1.2097601890563965\n",
      "epoch 20: loss=1.1946369409561157\n",
      "epoch 21: loss=1.178330898284912\n",
      "epoch 22: loss=1.147071123123169\n",
      "epoch 23: loss=1.1190619468688965\n",
      "epoch 24: loss=1.0892175436019897\n",
      "epoch 25: loss=1.0685361623764038\n",
      "epoch 26: loss=1.0548721551895142\n",
      "epoch 27: loss=1.040958285331726\n",
      "epoch 28: loss=1.0281586647033691\n",
      "epoch 29: loss=1.0146769285202026\n",
      "epoch 30: loss=0.9967154264450073\n",
      "epoch 31: loss=0.9825847744941711\n",
      "epoch 32: loss=0.9667924046516418\n",
      "epoch 33: loss=0.9581083655357361\n",
      "epoch 34: loss=0.9550601243972778\n",
      "epoch 35: loss=0.9511106014251709\n",
      "epoch 36: loss=0.9468766450881958\n",
      "epoch 37: loss=0.9381414651870728\n",
      "epoch 38: loss=0.9321491122245789\n",
      "epoch 39: loss=0.9248431324958801\n",
      "epoch 40: loss=0.9173675775527954\n",
      "epoch 41: loss=0.9112892150878906\n",
      "epoch 42: loss=0.9068560600280762\n",
      "epoch 43: loss=0.902243971824646\n",
      "epoch 44: loss=0.9004881978034973\n",
      "epoch 45: loss=0.9021549224853516\n",
      "epoch 46: loss=0.9024929404258728\n",
      "epoch 47: loss=0.8989298343658447\n",
      "epoch 48: loss=0.8968266248703003\n",
      "epoch 49: loss=0.8953905701637268\n",
      "epoch 50: loss=0.892525851726532\n",
      "epoch 51: loss=0.8911598324775696\n",
      "epoch 52: loss=0.8900927901268005\n",
      "epoch 53: loss=0.8881619572639465\n",
      "epoch 54: loss=0.8864430785179138\n",
      "epoch 55: loss=0.8854255676269531\n",
      "epoch 56: loss=0.8834197521209717\n",
      "epoch 57: loss=0.8821257948875427\n",
      "epoch 58: loss=0.88167804479599\n",
      "epoch 59: loss=0.8818172812461853\n",
      "epoch 60: loss=0.8770788908004761\n",
      "epoch 61: loss=0.8791730403900146\n",
      "epoch 62: loss=0.8784574270248413\n",
      "epoch 63: loss=0.8779324293136597\n",
      "epoch 64: loss=0.877700924873352\n",
      "epoch 65: loss=0.8776095509529114\n",
      "epoch 66: loss=0.8776553869247437\n",
      "epoch 67: loss=0.8783661127090454\n",
      "epoch 68: loss=0.8751463294029236\n",
      "epoch 69: loss=0.8761443495750427\n",
      "epoch 70: loss=0.8721923232078552\n",
      "epoch 71: loss=0.8743446469306946\n",
      "epoch 72: loss=0.8725602626800537\n",
      "epoch 73: loss=0.8726184964179993\n",
      "epoch 74: loss=0.8746731877326965\n",
      "epoch 75: loss=0.8725573420524597\n",
      "epoch 76: loss=0.8718786835670471\n",
      "epoch 77: loss=0.8729674816131592\n",
      "epoch 78: loss=0.8717796206474304\n",
      "epoch 79: loss=0.8698655962944031\n",
      "epoch 80: loss=0.8710720539093018\n",
      "epoch 81: loss=0.8709608316421509\n",
      "epoch 82: loss=0.8708859086036682\n",
      "epoch 83: loss=0.8694049715995789\n",
      "epoch 84: loss=0.8691701292991638\n",
      "epoch 85: loss=0.8693357110023499\n",
      "epoch 86: loss=0.8687461614608765\n",
      "epoch 87: loss=0.8686178922653198\n",
      "epoch 88: loss=0.8685625195503235\n",
      "epoch 89: loss=0.8713992238044739\n",
      "epoch 90: loss=0.8683738112449646\n",
      "epoch 91: loss=0.8669765591621399\n",
      "epoch 92: loss=0.8662318587303162\n",
      "epoch 93: loss=0.8691295981407166\n",
      "epoch 94: loss=0.8665286302566528\n",
      "epoch 95: loss=0.8678990602493286\n",
      "epoch 96: loss=0.8681747913360596\n",
      "epoch 97: loss=0.8656338453292847\n",
      "epoch 98: loss=0.8680790066719055\n",
      "epoch 99: loss=0.8660784363746643\n",
      "epoch 100: loss=0.8680298328399658\n",
      "epoch 101: loss=0.8671930432319641\n",
      "epoch 102: loss=0.8674232959747314\n",
      "epoch 103: loss=0.8663297295570374\n",
      "epoch 104: loss=0.8651662468910217\n",
      "epoch 105: loss=0.8651914000511169\n",
      "epoch 106: loss=0.8649242520332336\n",
      "epoch 107: loss=0.8660316467285156\n",
      "epoch 108: loss=0.8656583428382874\n",
      "epoch 109: loss=0.8656745553016663\n",
      "epoch 110: loss=0.864176332950592\n",
      "epoch 111: loss=0.8636502027511597\n",
      "epoch 112: loss=0.8636681437492371\n",
      "epoch 113: loss=0.8609383702278137\n",
      "epoch 114: loss=0.8629839420318604\n",
      "epoch 115: loss=0.8614985346794128\n",
      "epoch 116: loss=0.8607756495475769\n",
      "epoch 117: loss=0.8631743788719177\n",
      "epoch 118: loss=0.8631876707077026\n",
      "epoch 119: loss=0.8636708855628967\n",
      "epoch 120: loss=0.862878143787384\n",
      "epoch 121: loss=0.8631569743156433\n",
      "epoch 122: loss=0.863427996635437\n",
      "epoch 123: loss=0.8613473176956177\n",
      "epoch 124: loss=0.8627772927284241\n",
      "epoch 125: loss=0.8620989918708801\n",
      "epoch 126: loss=0.8623002767562866\n",
      "epoch 127: loss=0.8609811663627625\n",
      "epoch 128: loss=0.858819305896759\n",
      "epoch 129: loss=0.8613723516464233\n",
      "epoch 130: loss=0.8607809543609619\n",
      "epoch 131: loss=0.8601242303848267\n",
      "epoch 132: loss=0.8603368997573853\n",
      "epoch 133: loss=0.859876811504364\n",
      "epoch 134: loss=0.8594616055488586\n",
      "epoch 135: loss=0.8591679930686951\n",
      "epoch 136: loss=0.8575102090835571\n",
      "epoch 137: loss=0.8606153726577759\n",
      "epoch 138: loss=0.8570902943611145\n",
      "epoch 139: loss=0.8605953454971313\n",
      "epoch 140: loss=0.858365535736084\n",
      "epoch 141: loss=0.8570225238800049\n",
      "epoch 142: loss=0.8593325018882751\n",
      "epoch 143: loss=0.860101044178009\n",
      "epoch 144: loss=0.8590607643127441\n",
      "epoch 145: loss=0.857305645942688\n",
      "epoch 146: loss=0.8588476777076721\n",
      "epoch 147: loss=0.8581149578094482\n",
      "epoch 148: loss=0.8575046062469482\n",
      "epoch 149: loss=0.8558229207992554\n",
      "epoch 150: loss=0.8578855991363525\n",
      "epoch 151: loss=0.8584901690483093\n",
      "epoch 152: loss=0.8566245436668396\n",
      "epoch 153: loss=0.8573663234710693\n",
      "epoch 154: loss=0.8586877584457397\n",
      "epoch 155: loss=0.8564456701278687\n",
      "epoch 156: loss=0.8556069135665894\n",
      "epoch 157: loss=0.8555501103401184\n",
      "epoch 158: loss=0.8556815385818481\n",
      "epoch 159: loss=0.8578576445579529\n",
      "epoch 160: loss=0.8560091257095337\n",
      "epoch 161: loss=0.8555582165718079\n",
      "epoch 162: loss=0.856473982334137\n",
      "epoch 163: loss=0.855067253112793\n",
      "epoch 164: loss=0.8559550642967224\n",
      "epoch 165: loss=0.8574612140655518\n",
      "epoch 166: loss=0.8570642471313477\n",
      "epoch 167: loss=0.8558670282363892\n",
      "epoch 168: loss=0.8564755320549011\n",
      "epoch 169: loss=0.8560129404067993\n",
      "epoch 170: loss=0.8549221158027649\n",
      "epoch 171: loss=0.8552883267402649\n",
      "epoch 172: loss=0.8557556867599487\n",
      "epoch 173: loss=0.8554794192314148\n",
      "epoch 174: loss=0.853737473487854\n",
      "epoch 175: loss=0.8560181260108948\n",
      "epoch 176: loss=0.8542531728744507\n",
      "epoch 177: loss=0.8543874621391296\n",
      "epoch 178: loss=0.8547306060791016\n",
      "epoch 179: loss=0.8542183637619019\n",
      "epoch 180: loss=0.8558681011199951\n",
      "epoch 181: loss=0.8547930121421814\n",
      "epoch 182: loss=0.8545821905136108\n",
      "epoch 183: loss=0.8557844161987305\n",
      "epoch 184: loss=0.8546653389930725\n",
      "epoch 185: loss=0.8553083539009094\n",
      "epoch 186: loss=0.8544008135795593\n",
      "epoch 187: loss=0.8539913892745972\n",
      "epoch 188: loss=0.8547788858413696\n",
      "epoch 189: loss=0.8536112904548645\n",
      "epoch 190: loss=0.8517159223556519\n",
      "epoch 191: loss=0.8527007699012756\n",
      "epoch 192: loss=0.853211522102356\n",
      "epoch 193: loss=0.851671576499939\n",
      "epoch 194: loss=0.8539463877677917\n",
      "epoch 195: loss=0.8525247573852539\n",
      "epoch 196: loss=0.8543316721916199\n",
      "epoch 197: loss=0.8534188866615295\n",
      "epoch 198: loss=0.8528813719749451\n",
      "epoch 199: loss=0.8529813289642334\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=3.408780574798584\n",
      "epoch 1: loss=3.2071115970611572\n",
      "epoch 2: loss=3.0629215240478516\n",
      "epoch 3: loss=2.8576083183288574\n",
      "epoch 4: loss=2.5908613204956055\n",
      "epoch 5: loss=2.3070569038391113\n",
      "epoch 6: loss=2.105144739151001\n",
      "epoch 7: loss=1.9673237800598145\n",
      "epoch 8: loss=1.9239459037780762\n",
      "epoch 9: loss=1.8545364141464233\n",
      "epoch 10: loss=1.767700433731079\n",
      "epoch 11: loss=1.6651209592819214\n",
      "epoch 12: loss=1.5670393705368042\n",
      "epoch 13: loss=1.4623520374298096\n",
      "epoch 14: loss=1.3877203464508057\n",
      "epoch 15: loss=1.3374152183532715\n",
      "epoch 16: loss=1.2830561399459839\n",
      "epoch 17: loss=1.265046238899231\n",
      "epoch 18: loss=1.246916651725769\n",
      "epoch 19: loss=1.220600962638855\n",
      "epoch 20: loss=1.196824312210083\n",
      "epoch 21: loss=1.1633237600326538\n",
      "epoch 22: loss=1.1411242485046387\n",
      "epoch 23: loss=1.1092774868011475\n",
      "epoch 24: loss=1.0860850811004639\n",
      "epoch 25: loss=1.0692059993743896\n",
      "epoch 26: loss=1.054428219795227\n",
      "epoch 27: loss=1.0362894535064697\n",
      "epoch 28: loss=1.020850419998169\n",
      "epoch 29: loss=1.0036829710006714\n",
      "epoch 30: loss=0.9901536107063293\n",
      "epoch 31: loss=0.9794987440109253\n",
      "epoch 32: loss=0.9719415903091431\n",
      "epoch 33: loss=0.9682164788246155\n",
      "epoch 34: loss=0.9643593430519104\n",
      "epoch 35: loss=0.9595223069190979\n",
      "epoch 36: loss=0.9578537344932556\n",
      "epoch 37: loss=0.9479039311408997\n",
      "epoch 38: loss=0.9431768655776978\n",
      "epoch 39: loss=0.9406076669692993\n",
      "epoch 40: loss=0.9321545362472534\n",
      "epoch 41: loss=0.9281434416770935\n",
      "epoch 42: loss=0.9267565011978149\n",
      "epoch 43: loss=0.9220924377441406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44: loss=0.9234265089035034\n",
      "epoch 45: loss=0.9212979078292847\n",
      "epoch 46: loss=0.9196991920471191\n",
      "epoch 47: loss=0.9224351048469543\n",
      "epoch 48: loss=0.9179016351699829\n",
      "epoch 49: loss=0.917789876461029\n",
      "epoch 50: loss=0.9126793742179871\n",
      "epoch 51: loss=0.912865161895752\n",
      "epoch 52: loss=0.9125294089317322\n",
      "epoch 53: loss=0.9091896414756775\n",
      "epoch 54: loss=0.9097324013710022\n",
      "epoch 55: loss=0.9084344506263733\n",
      "epoch 56: loss=0.904858410358429\n",
      "epoch 57: loss=0.9064518213272095\n",
      "epoch 58: loss=0.9068124294281006\n",
      "epoch 59: loss=0.9033241271972656\n",
      "epoch 60: loss=0.9027683138847351\n",
      "epoch 61: loss=0.9034437537193298\n",
      "epoch 62: loss=0.902442991733551\n",
      "epoch 63: loss=0.9026429653167725\n",
      "epoch 64: loss=0.9006290435791016\n",
      "epoch 65: loss=0.9006749391555786\n",
      "epoch 66: loss=0.8998895287513733\n",
      "epoch 67: loss=0.8985588550567627\n",
      "epoch 68: loss=0.8980064988136292\n",
      "epoch 69: loss=0.8980503678321838\n",
      "epoch 70: loss=0.8966546654701233\n",
      "epoch 71: loss=0.8965921401977539\n",
      "epoch 72: loss=0.89798903465271\n",
      "epoch 73: loss=0.8967137336730957\n",
      "epoch 74: loss=0.8952510356903076\n",
      "epoch 75: loss=0.8966909646987915\n",
      "epoch 76: loss=0.8947228193283081\n",
      "epoch 77: loss=0.896247923374176\n",
      "epoch 78: loss=0.8954336643218994\n",
      "epoch 79: loss=0.8949809670448303\n",
      "epoch 80: loss=0.8917206525802612\n",
      "epoch 81: loss=0.8929337859153748\n",
      "epoch 82: loss=0.8946655988693237\n",
      "epoch 83: loss=0.8919874429702759\n",
      "epoch 84: loss=0.8917962908744812\n",
      "epoch 85: loss=0.8915563821792603\n",
      "epoch 86: loss=0.8911252021789551\n",
      "epoch 87: loss=0.8905397653579712\n",
      "epoch 88: loss=0.8900287747383118\n",
      "epoch 89: loss=0.8896914720535278\n",
      "epoch 90: loss=0.8917097449302673\n",
      "epoch 91: loss=0.8910305500030518\n",
      "epoch 92: loss=0.8899081349372864\n",
      "epoch 93: loss=0.8891417980194092\n",
      "epoch 94: loss=0.887250542640686\n",
      "epoch 95: loss=0.8869786858558655\n",
      "epoch 96: loss=0.8873732089996338\n",
      "epoch 97: loss=0.888631284236908\n",
      "epoch 98: loss=0.8873869180679321\n",
      "epoch 99: loss=0.8875628113746643\n",
      "epoch 100: loss=0.8862088918685913\n",
      "epoch 101: loss=0.88569176197052\n",
      "epoch 102: loss=0.8849312663078308\n",
      "epoch 103: loss=0.8822141289710999\n",
      "epoch 104: loss=0.885737955570221\n",
      "epoch 105: loss=0.8827223777770996\n",
      "epoch 106: loss=0.8861527442932129\n",
      "epoch 107: loss=0.8847384452819824\n",
      "epoch 108: loss=0.8811477422714233\n",
      "epoch 109: loss=0.8839055895805359\n",
      "epoch 110: loss=0.8820381760597229\n",
      "epoch 111: loss=0.8812507390975952\n",
      "epoch 112: loss=0.8814232349395752\n",
      "epoch 113: loss=0.8808961510658264\n",
      "epoch 114: loss=0.8816536664962769\n",
      "epoch 115: loss=0.8805373907089233\n",
      "epoch 116: loss=0.8798558712005615\n",
      "epoch 117: loss=0.8793694376945496\n",
      "epoch 118: loss=0.8779711723327637\n",
      "epoch 119: loss=0.8788277506828308\n",
      "epoch 120: loss=0.8784952759742737\n",
      "epoch 121: loss=0.8762226104736328\n",
      "epoch 122: loss=0.8801300525665283\n",
      "epoch 123: loss=0.878832221031189\n",
      "epoch 124: loss=0.8790616989135742\n",
      "epoch 125: loss=0.8770900368690491\n",
      "epoch 126: loss=0.8760130405426025\n",
      "epoch 127: loss=0.8786383867263794\n",
      "epoch 128: loss=0.8769985437393188\n",
      "epoch 129: loss=0.8755089044570923\n",
      "epoch 130: loss=0.8747461438179016\n",
      "epoch 131: loss=0.8740183711051941\n",
      "epoch 132: loss=0.8750820755958557\n",
      "epoch 133: loss=0.8740990161895752\n",
      "epoch 134: loss=0.8725907802581787\n",
      "epoch 135: loss=0.8743100166320801\n",
      "epoch 136: loss=0.8732532858848572\n",
      "epoch 137: loss=0.8751750588417053\n",
      "epoch 138: loss=0.8733909130096436\n",
      "epoch 139: loss=0.8719093203544617\n",
      "epoch 140: loss=0.8711830377578735\n",
      "epoch 141: loss=0.8730943202972412\n",
      "epoch 142: loss=0.8701378107070923\n",
      "epoch 143: loss=0.8725016713142395\n",
      "epoch 144: loss=0.8705816864967346\n",
      "epoch 145: loss=0.8709725737571716\n",
      "epoch 146: loss=0.869665801525116\n",
      "epoch 147: loss=0.8707337379455566\n",
      "epoch 148: loss=0.86993408203125\n",
      "epoch 149: loss=0.8689287304878235\n",
      "epoch 150: loss=0.8691952228546143\n",
      "epoch 151: loss=0.8713998198509216\n",
      "epoch 152: loss=0.8690685033798218\n",
      "epoch 153: loss=0.8685798048973083\n",
      "epoch 154: loss=0.868170440196991\n",
      "epoch 155: loss=0.8655526638031006\n",
      "epoch 156: loss=0.870378315448761\n",
      "epoch 157: loss=0.8683775067329407\n",
      "epoch 158: loss=0.8672528266906738\n",
      "epoch 159: loss=0.8663560152053833\n",
      "epoch 160: loss=0.866188108921051\n",
      "epoch 161: loss=0.8659727573394775\n",
      "epoch 162: loss=0.8657877445220947\n",
      "epoch 163: loss=0.8650290369987488\n",
      "epoch 164: loss=0.864534318447113\n",
      "epoch 165: loss=0.8648523092269897\n",
      "epoch 166: loss=0.8629407286643982\n",
      "epoch 167: loss=0.8628498315811157\n",
      "epoch 168: loss=0.86480313539505\n",
      "epoch 169: loss=0.862629234790802\n",
      "epoch 170: loss=0.8623524904251099\n",
      "epoch 171: loss=0.86266028881073\n",
      "epoch 172: loss=0.863847017288208\n",
      "epoch 173: loss=0.8615304231643677\n",
      "epoch 174: loss=0.8613480925559998\n",
      "epoch 175: loss=0.8615036606788635\n",
      "epoch 176: loss=0.8619800209999084\n",
      "epoch 177: loss=0.8586679100990295\n",
      "epoch 178: loss=0.8611359000205994\n",
      "epoch 179: loss=0.8626653552055359\n",
      "epoch 180: loss=0.8602216839790344\n",
      "epoch 181: loss=0.8610946536064148\n",
      "epoch 182: loss=0.8586198091506958\n",
      "epoch 183: loss=0.8596435785293579\n",
      "epoch 184: loss=0.85919588804245\n",
      "epoch 185: loss=0.8583250641822815\n",
      "epoch 186: loss=0.8589937686920166\n",
      "epoch 187: loss=0.8574654459953308\n",
      "epoch 188: loss=0.858400285243988\n",
      "epoch 189: loss=0.8583595752716064\n",
      "epoch 190: loss=0.8563780188560486\n",
      "epoch 191: loss=0.8567363619804382\n",
      "epoch 192: loss=0.8562264442443848\n",
      "epoch 193: loss=0.857347846031189\n",
      "epoch 194: loss=0.8576692938804626\n",
      "epoch 195: loss=0.8564951419830322\n",
      "epoch 196: loss=0.8577936291694641\n",
      "epoch 197: loss=0.856651246547699\n",
      "epoch 198: loss=0.8555202484130859\n",
      "epoch 199: loss=0.8561108112335205\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=3.41335391998291\n",
      "epoch 1: loss=3.26885986328125\n",
      "epoch 2: loss=3.034778118133545\n",
      "epoch 3: loss=2.8823628425598145\n",
      "epoch 4: loss=2.57344651222229\n",
      "epoch 5: loss=2.306713342666626\n",
      "epoch 6: loss=2.033191680908203\n",
      "epoch 7: loss=1.902069330215454\n",
      "epoch 8: loss=1.855855107307434\n",
      "epoch 9: loss=1.8007794618606567\n",
      "epoch 10: loss=1.7602977752685547\n",
      "epoch 11: loss=1.7003483772277832\n",
      "epoch 12: loss=1.59577476978302\n",
      "epoch 13: loss=1.5081443786621094\n",
      "epoch 14: loss=1.417237401008606\n",
      "epoch 15: loss=1.3467329740524292\n",
      "epoch 16: loss=1.2832666635513306\n",
      "epoch 17: loss=1.2392653226852417\n",
      "epoch 18: loss=1.2152421474456787\n",
      "epoch 19: loss=1.200382113456726\n",
      "epoch 20: loss=1.1818251609802246\n",
      "epoch 21: loss=1.1615393161773682\n",
      "epoch 22: loss=1.13966965675354\n",
      "epoch 23: loss=1.1136078834533691\n",
      "epoch 24: loss=1.076505422592163\n",
      "epoch 25: loss=1.0571154356002808\n",
      "epoch 26: loss=1.0387651920318604\n",
      "epoch 27: loss=1.0198493003845215\n",
      "epoch 28: loss=1.0049742460250854\n",
      "epoch 29: loss=0.9919286966323853\n",
      "epoch 30: loss=0.9762409329414368\n",
      "epoch 31: loss=0.9631617069244385\n",
      "epoch 32: loss=0.9521220922470093\n",
      "epoch 33: loss=0.9460445642471313\n",
      "epoch 34: loss=0.9414265751838684\n",
      "epoch 35: loss=0.942243754863739\n",
      "epoch 36: loss=0.9344375133514404\n",
      "epoch 37: loss=0.9331590533256531\n",
      "epoch 38: loss=0.9320157170295715\n",
      "epoch 39: loss=0.9243860244750977\n",
      "epoch 40: loss=0.9207784533500671\n",
      "epoch 41: loss=0.9162887334823608\n",
      "epoch 42: loss=0.9130988121032715\n",
      "epoch 43: loss=0.9152489900588989\n",
      "epoch 44: loss=0.9092671275138855\n",
      "epoch 45: loss=0.9099981784820557\n",
      "epoch 46: loss=0.9058477282524109\n",
      "epoch 47: loss=0.9049260020256042\n",
      "epoch 48: loss=0.9041145443916321\n",
      "epoch 49: loss=0.9039327502250671\n",
      "epoch 50: loss=0.9043950438499451\n",
      "epoch 51: loss=0.9020383954048157\n",
      "epoch 52: loss=0.899793267250061\n",
      "epoch 53: loss=0.8980785012245178\n",
      "epoch 54: loss=0.8969974517822266\n",
      "epoch 55: loss=0.895022988319397\n",
      "epoch 56: loss=0.8965386152267456\n",
      "epoch 57: loss=0.8932883739471436\n",
      "epoch 58: loss=0.8940598368644714\n",
      "epoch 59: loss=0.893330454826355\n",
      "epoch 60: loss=0.8918972015380859\n",
      "epoch 61: loss=0.891639232635498\n",
      "epoch 62: loss=0.8925604820251465\n",
      "epoch 63: loss=0.8930480480194092\n",
      "epoch 64: loss=0.8905382752418518\n",
      "epoch 65: loss=0.8884596824645996\n",
      "epoch 66: loss=0.8911609649658203\n",
      "epoch 67: loss=0.8878053426742554\n",
      "epoch 68: loss=0.8883419632911682\n",
      "epoch 69: loss=0.8878861665725708\n",
      "epoch 70: loss=0.888065755367279\n",
      "epoch 71: loss=0.8873472213745117\n",
      "epoch 72: loss=0.8875728249549866\n",
      "epoch 73: loss=0.8869409561157227\n",
      "epoch 74: loss=0.8849559426307678\n",
      "epoch 75: loss=0.8855363130569458\n",
      "epoch 76: loss=0.8856768608093262\n",
      "epoch 77: loss=0.885753333568573\n",
      "epoch 78: loss=0.8840327262878418\n",
      "epoch 79: loss=0.884691059589386\n",
      "epoch 80: loss=0.8853244781494141\n",
      "epoch 81: loss=0.8850291967391968\n",
      "epoch 82: loss=0.8845311999320984\n",
      "epoch 83: loss=0.8845201134681702\n",
      "epoch 84: loss=0.8831413388252258\n",
      "epoch 85: loss=0.8821509480476379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86: loss=0.882440984249115\n",
      "epoch 87: loss=0.8831139206886292\n",
      "epoch 88: loss=0.8829156756401062\n",
      "epoch 89: loss=0.8825123906135559\n",
      "epoch 90: loss=0.8807551860809326\n",
      "epoch 91: loss=0.8806307315826416\n",
      "epoch 92: loss=0.8822363615036011\n",
      "epoch 93: loss=0.8801677823066711\n",
      "epoch 94: loss=0.8828320503234863\n",
      "epoch 95: loss=0.8807642459869385\n",
      "epoch 96: loss=0.8808561563491821\n",
      "epoch 97: loss=0.880989670753479\n",
      "epoch 98: loss=0.8792816400527954\n",
      "epoch 99: loss=0.8784233331680298\n",
      "epoch 100: loss=0.8789570927619934\n",
      "epoch 101: loss=0.8795667886734009\n",
      "epoch 102: loss=0.8771572709083557\n",
      "epoch 103: loss=0.878426730632782\n",
      "epoch 104: loss=0.8779009580612183\n",
      "epoch 105: loss=0.8806297183036804\n",
      "epoch 106: loss=0.8801433444023132\n",
      "epoch 107: loss=0.8763933777809143\n",
      "epoch 108: loss=0.8771058917045593\n",
      "epoch 109: loss=0.8784858584403992\n",
      "epoch 110: loss=0.877678394317627\n",
      "epoch 111: loss=0.8773868083953857\n",
      "epoch 112: loss=0.8773137927055359\n",
      "epoch 113: loss=0.8764650821685791\n",
      "epoch 114: loss=0.8782241940498352\n",
      "epoch 115: loss=0.878428041934967\n",
      "epoch 116: loss=0.8762418627738953\n",
      "epoch 117: loss=0.8775266408920288\n",
      "epoch 118: loss=0.8747727274894714\n",
      "epoch 119: loss=0.8766852021217346\n",
      "epoch 120: loss=0.8757219314575195\n",
      "epoch 121: loss=0.8758059740066528\n",
      "epoch 122: loss=0.8763972520828247\n",
      "epoch 123: loss=0.8757811784744263\n",
      "epoch 124: loss=0.8750401139259338\n",
      "epoch 125: loss=0.8717991709709167\n",
      "epoch 126: loss=0.8769210577011108\n",
      "epoch 127: loss=0.8735441565513611\n",
      "epoch 128: loss=0.8755967617034912\n",
      "epoch 129: loss=0.8717713356018066\n",
      "epoch 130: loss=0.8730922341346741\n",
      "epoch 131: loss=0.8751338124275208\n",
      "epoch 132: loss=0.8729206323623657\n",
      "epoch 133: loss=0.8734420537948608\n",
      "epoch 134: loss=0.8742567896842957\n",
      "epoch 135: loss=0.8733771443367004\n",
      "epoch 136: loss=0.8723644018173218\n",
      "epoch 137: loss=0.8744621872901917\n",
      "epoch 138: loss=0.8743576407432556\n",
      "epoch 139: loss=0.8726388812065125\n",
      "epoch 140: loss=0.873394787311554\n",
      "epoch 141: loss=0.8715543746948242\n",
      "epoch 142: loss=0.8692567944526672\n",
      "epoch 143: loss=0.8719647526741028\n",
      "epoch 144: loss=0.8727955222129822\n",
      "epoch 145: loss=0.8693459033966064\n",
      "epoch 146: loss=0.8691844344139099\n",
      "epoch 147: loss=0.8702967762947083\n",
      "epoch 148: loss=0.8708699941635132\n",
      "epoch 149: loss=0.8708450198173523\n",
      "epoch 150: loss=0.8691304326057434\n",
      "epoch 151: loss=0.8704741597175598\n",
      "epoch 152: loss=0.8677071928977966\n",
      "epoch 153: loss=0.8686937093734741\n",
      "epoch 154: loss=0.8689666390419006\n",
      "epoch 155: loss=0.8703253865242004\n",
      "epoch 156: loss=0.8687961101531982\n",
      "epoch 157: loss=0.8685140013694763\n",
      "epoch 158: loss=0.8695360422134399\n",
      "epoch 159: loss=0.8660418391227722\n",
      "epoch 160: loss=0.8671764731407166\n",
      "epoch 161: loss=0.867962121963501\n",
      "epoch 162: loss=0.8665619492530823\n",
      "epoch 163: loss=0.8682146668434143\n",
      "epoch 164: loss=0.8687642216682434\n",
      "epoch 165: loss=0.8669113516807556\n",
      "epoch 166: loss=0.8671777844429016\n",
      "epoch 167: loss=0.8671794533729553\n",
      "epoch 168: loss=0.8648583292961121\n",
      "epoch 169: loss=0.8670366406440735\n",
      "epoch 170: loss=0.8667864203453064\n",
      "epoch 171: loss=0.8638417720794678\n",
      "epoch 172: loss=0.8664988875389099\n",
      "epoch 173: loss=0.8652625679969788\n",
      "epoch 174: loss=0.8659254908561707\n",
      "epoch 175: loss=0.8655604720115662\n",
      "epoch 176: loss=0.8644328117370605\n",
      "epoch 177: loss=0.8646160960197449\n",
      "epoch 178: loss=0.8635258674621582\n",
      "epoch 179: loss=0.8630906343460083\n",
      "epoch 180: loss=0.8632445335388184\n",
      "epoch 181: loss=0.8615908026695251\n",
      "epoch 182: loss=0.863206684589386\n",
      "epoch 183: loss=0.8602811694145203\n",
      "epoch 184: loss=0.8597198128700256\n",
      "epoch 185: loss=0.8625528216362\n",
      "epoch 186: loss=0.8612854480743408\n",
      "epoch 187: loss=0.860219419002533\n",
      "epoch 188: loss=0.8595588803291321\n",
      "epoch 189: loss=0.858210563659668\n",
      "epoch 190: loss=0.8582096695899963\n",
      "epoch 191: loss=0.8560948371887207\n",
      "epoch 192: loss=0.8571615219116211\n",
      "epoch 193: loss=0.8562958240509033\n",
      "epoch 194: loss=0.8562713861465454\n",
      "epoch 195: loss=0.8568312525749207\n",
      "epoch 196: loss=0.853962242603302\n",
      "epoch 197: loss=0.852150559425354\n",
      "epoch 198: loss=0.8530314564704895\n",
      "epoch 199: loss=0.8520905375480652\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=3.3801023960113525\n",
      "epoch 1: loss=3.259530782699585\n",
      "epoch 2: loss=3.044489622116089\n",
      "epoch 3: loss=2.8698465824127197\n",
      "epoch 4: loss=2.6250650882720947\n",
      "epoch 5: loss=2.460369348526001\n",
      "epoch 6: loss=2.186889171600342\n",
      "epoch 7: loss=1.9788042306900024\n",
      "epoch 8: loss=1.8227866888046265\n",
      "epoch 9: loss=1.780358076095581\n",
      "epoch 10: loss=1.7303797006607056\n",
      "epoch 11: loss=1.658981204032898\n",
      "epoch 12: loss=1.56879723072052\n",
      "epoch 13: loss=1.4698305130004883\n",
      "epoch 14: loss=1.3788959980010986\n",
      "epoch 15: loss=1.306836724281311\n",
      "epoch 16: loss=1.2481011152267456\n",
      "epoch 17: loss=1.2271329164505005\n",
      "epoch 18: loss=1.2150077819824219\n",
      "epoch 19: loss=1.2081990242004395\n",
      "epoch 20: loss=1.1821653842926025\n",
      "epoch 21: loss=1.149018406867981\n",
      "epoch 22: loss=1.1106951236724854\n",
      "epoch 23: loss=1.0733311176300049\n",
      "epoch 24: loss=1.0459176301956177\n",
      "epoch 25: loss=1.0366634130477905\n",
      "epoch 26: loss=1.0279806852340698\n",
      "epoch 27: loss=1.011610746383667\n",
      "epoch 28: loss=0.9934157133102417\n",
      "epoch 29: loss=0.9802571535110474\n",
      "epoch 30: loss=0.9572858810424805\n",
      "epoch 31: loss=0.9458879828453064\n",
      "epoch 32: loss=0.9395222663879395\n",
      "epoch 33: loss=0.940517008304596\n",
      "epoch 34: loss=0.9395976066589355\n",
      "epoch 35: loss=0.9336382746696472\n",
      "epoch 36: loss=0.9276978969573975\n",
      "epoch 37: loss=0.9199196100234985\n",
      "epoch 38: loss=0.9166748523712158\n",
      "epoch 39: loss=0.9138438105583191\n",
      "epoch 40: loss=0.916255533695221\n",
      "epoch 41: loss=0.9157790541648865\n",
      "epoch 42: loss=0.9124563932418823\n",
      "epoch 43: loss=0.911847710609436\n",
      "epoch 44: loss=0.9065934419631958\n",
      "epoch 45: loss=0.9053347706794739\n",
      "epoch 46: loss=0.9041388630867004\n",
      "epoch 47: loss=0.9062188863754272\n",
      "epoch 48: loss=0.9025124311447144\n",
      "epoch 49: loss=0.9010957479476929\n",
      "epoch 50: loss=0.8955071568489075\n",
      "epoch 51: loss=0.8983364105224609\n",
      "epoch 52: loss=0.8952608704566956\n",
      "epoch 53: loss=0.8928430080413818\n",
      "epoch 54: loss=0.8949325084686279\n",
      "epoch 55: loss=0.8968362212181091\n",
      "epoch 56: loss=0.8938762545585632\n",
      "epoch 57: loss=0.896111786365509\n",
      "epoch 58: loss=0.8955603837966919\n",
      "epoch 59: loss=0.894492506980896\n",
      "epoch 60: loss=0.8916125297546387\n",
      "epoch 61: loss=0.8884540796279907\n",
      "epoch 62: loss=0.8917933106422424\n",
      "epoch 63: loss=0.8928770422935486\n",
      "epoch 64: loss=0.8901768326759338\n",
      "epoch 65: loss=0.8908942937850952\n",
      "epoch 66: loss=0.8886165022850037\n",
      "epoch 67: loss=0.8878021836280823\n",
      "epoch 68: loss=0.8877924084663391\n",
      "epoch 69: loss=0.8887826204299927\n",
      "epoch 70: loss=0.8856642246246338\n",
      "epoch 71: loss=0.8858954906463623\n",
      "epoch 72: loss=0.887306809425354\n",
      "epoch 73: loss=0.8858079314231873\n",
      "epoch 74: loss=0.8854197263717651\n",
      "epoch 75: loss=0.8858524560928345\n",
      "epoch 76: loss=0.8837457299232483\n",
      "epoch 77: loss=0.8828468918800354\n",
      "epoch 78: loss=0.8838679194450378\n",
      "epoch 79: loss=0.8842266798019409\n",
      "epoch 80: loss=0.8819722533226013\n",
      "epoch 81: loss=0.8816488981246948\n",
      "epoch 82: loss=0.8830220103263855\n",
      "epoch 83: loss=0.8838647603988647\n",
      "epoch 84: loss=0.8841769099235535\n",
      "epoch 85: loss=0.8818674683570862\n",
      "epoch 86: loss=0.8826603889465332\n",
      "epoch 87: loss=0.880137026309967\n",
      "epoch 88: loss=0.8832564949989319\n",
      "epoch 89: loss=0.8802637457847595\n",
      "epoch 90: loss=0.8793920278549194\n",
      "epoch 91: loss=0.8808495998382568\n",
      "epoch 92: loss=0.8811044692993164\n",
      "epoch 93: loss=0.8815294504165649\n",
      "epoch 94: loss=0.8816565275192261\n",
      "epoch 95: loss=0.8816713690757751\n",
      "epoch 96: loss=0.8808838725090027\n",
      "epoch 97: loss=0.8795642852783203\n",
      "epoch 98: loss=0.8782598376274109\n",
      "epoch 99: loss=0.8785321116447449\n",
      "epoch 100: loss=0.8798660635948181\n",
      "epoch 101: loss=0.8799735307693481\n",
      "epoch 102: loss=0.8789435625076294\n",
      "epoch 103: loss=0.8758598566055298\n",
      "epoch 104: loss=0.8787080645561218\n",
      "epoch 105: loss=0.8780125379562378\n",
      "epoch 106: loss=0.8789524435997009\n",
      "epoch 107: loss=0.8769462704658508\n",
      "epoch 108: loss=0.8775644302368164\n",
      "epoch 109: loss=0.8772108554840088\n",
      "epoch 110: loss=0.8771775960922241\n",
      "epoch 111: loss=0.8757521510124207\n",
      "epoch 112: loss=0.8764862418174744\n",
      "epoch 113: loss=0.8761491179466248\n",
      "epoch 114: loss=0.8751606941223145\n",
      "epoch 115: loss=0.875037670135498\n",
      "epoch 116: loss=0.876318097114563\n",
      "epoch 117: loss=0.8732584714889526\n",
      "epoch 118: loss=0.8740177750587463\n",
      "epoch 119: loss=0.8758551478385925\n",
      "epoch 120: loss=0.875087559223175\n",
      "epoch 121: loss=0.8741928935050964\n",
      "epoch 122: loss=0.8743051290512085\n",
      "epoch 123: loss=0.8739501237869263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124: loss=0.8737784028053284\n",
      "epoch 125: loss=0.8740003705024719\n",
      "epoch 126: loss=0.8734650015830994\n",
      "epoch 127: loss=0.8729780316352844\n",
      "epoch 128: loss=0.8720861673355103\n",
      "epoch 129: loss=0.8721834421157837\n",
      "epoch 130: loss=0.8727312088012695\n",
      "epoch 131: loss=0.8721343874931335\n",
      "epoch 132: loss=0.8707138895988464\n",
      "epoch 133: loss=0.8730029463768005\n",
      "epoch 134: loss=0.8712852597236633\n",
      "epoch 135: loss=0.8713357448577881\n",
      "epoch 136: loss=0.8725274205207825\n",
      "epoch 137: loss=0.8717661499977112\n",
      "epoch 138: loss=0.8714858889579773\n",
      "epoch 139: loss=0.8668711185455322\n",
      "epoch 140: loss=0.8696119785308838\n",
      "epoch 141: loss=0.8724193572998047\n",
      "epoch 142: loss=0.8717387318611145\n",
      "epoch 143: loss=0.8708440065383911\n",
      "epoch 144: loss=0.868493914604187\n",
      "epoch 145: loss=0.8693253397941589\n",
      "epoch 146: loss=0.8671646118164062\n",
      "epoch 147: loss=0.8669858574867249\n",
      "epoch 148: loss=0.8706962466239929\n",
      "epoch 149: loss=0.8706303834915161\n",
      "epoch 150: loss=0.8686259984970093\n",
      "epoch 151: loss=0.8667829632759094\n",
      "epoch 152: loss=0.8683068156242371\n",
      "epoch 153: loss=0.8708746433258057\n",
      "epoch 154: loss=0.8688891530036926\n",
      "epoch 155: loss=0.8668490052223206\n",
      "epoch 156: loss=0.8663371801376343\n",
      "epoch 157: loss=0.8690814971923828\n",
      "epoch 158: loss=0.8661726117134094\n",
      "epoch 159: loss=0.8665714859962463\n",
      "epoch 160: loss=0.8690782189369202\n",
      "epoch 161: loss=0.8657276630401611\n",
      "epoch 162: loss=0.8662479519844055\n",
      "epoch 163: loss=0.8673518896102905\n",
      "epoch 164: loss=0.8669946193695068\n",
      "epoch 165: loss=0.8659241199493408\n",
      "epoch 166: loss=0.8669304251670837\n",
      "epoch 167: loss=0.8659018278121948\n",
      "epoch 168: loss=0.8646749258041382\n",
      "epoch 169: loss=0.8627061247825623\n",
      "epoch 170: loss=0.8644599318504333\n",
      "epoch 171: loss=0.8660145998001099\n",
      "epoch 172: loss=0.8629708290100098\n",
      "epoch 173: loss=0.8656849265098572\n",
      "epoch 174: loss=0.8645607829093933\n",
      "epoch 175: loss=0.8638111352920532\n",
      "epoch 176: loss=0.8623749017715454\n",
      "epoch 177: loss=0.8645835518836975\n",
      "epoch 178: loss=0.8622984886169434\n",
      "epoch 179: loss=0.8622248768806458\n",
      "epoch 180: loss=0.8621904253959656\n",
      "epoch 181: loss=0.8634916543960571\n",
      "epoch 182: loss=0.8649053573608398\n",
      "epoch 183: loss=0.8588483929634094\n",
      "epoch 184: loss=0.8637341260910034\n",
      "epoch 185: loss=0.861908495426178\n",
      "epoch 186: loss=0.8603298664093018\n",
      "epoch 187: loss=0.8627141714096069\n",
      "epoch 188: loss=0.8630226254463196\n",
      "epoch 189: loss=0.8593930006027222\n",
      "epoch 190: loss=0.8625654578208923\n",
      "epoch 191: loss=0.86152583360672\n",
      "epoch 192: loss=0.8620373010635376\n",
      "epoch 193: loss=0.8618951439857483\n",
      "epoch 194: loss=0.8596100211143494\n",
      "epoch 195: loss=0.8635932803153992\n",
      "epoch 196: loss=0.8606446385383606\n",
      "epoch 197: loss=0.8610946536064148\n",
      "epoch 198: loss=0.8618360757827759\n",
      "epoch 199: loss=0.8605490922927856\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=3.4084701538085938\n",
      "epoch 1: loss=3.2268943786621094\n",
      "epoch 2: loss=3.0552978515625\n",
      "epoch 3: loss=2.809670925140381\n",
      "epoch 4: loss=2.582706928253174\n",
      "epoch 5: loss=2.3302483558654785\n",
      "epoch 6: loss=2.076291084289551\n",
      "epoch 7: loss=1.8909436464309692\n",
      "epoch 8: loss=1.8071056604385376\n",
      "epoch 9: loss=1.7530754804611206\n",
      "epoch 10: loss=1.6852177381515503\n",
      "epoch 11: loss=1.611686110496521\n",
      "epoch 12: loss=1.499381422996521\n",
      "epoch 13: loss=1.3895906209945679\n",
      "epoch 14: loss=1.3153433799743652\n",
      "epoch 15: loss=1.2585042715072632\n",
      "epoch 16: loss=1.2292563915252686\n",
      "epoch 17: loss=1.2228903770446777\n",
      "epoch 18: loss=1.2154791355133057\n",
      "epoch 19: loss=1.194849967956543\n",
      "epoch 20: loss=1.1599234342575073\n",
      "epoch 21: loss=1.1290850639343262\n",
      "epoch 22: loss=1.0956840515136719\n",
      "epoch 23: loss=1.0802878141403198\n",
      "epoch 24: loss=1.0645443201065063\n",
      "epoch 25: loss=1.0570615530014038\n",
      "epoch 26: loss=1.051500678062439\n",
      "epoch 27: loss=1.0305033922195435\n",
      "epoch 28: loss=1.0129996538162231\n",
      "epoch 29: loss=0.9965859651565552\n",
      "epoch 30: loss=0.9839051961898804\n",
      "epoch 31: loss=0.9730693101882935\n",
      "epoch 32: loss=0.9675167202949524\n",
      "epoch 33: loss=0.9639813303947449\n",
      "epoch 34: loss=0.9572898149490356\n",
      "epoch 35: loss=0.9533959031105042\n",
      "epoch 36: loss=0.9482288956642151\n",
      "epoch 37: loss=0.9467254281044006\n",
      "epoch 38: loss=0.9391570091247559\n",
      "epoch 39: loss=0.9365837574005127\n",
      "epoch 40: loss=0.9285245537757874\n",
      "epoch 41: loss=0.9282135963439941\n",
      "epoch 42: loss=0.922694742679596\n",
      "epoch 43: loss=0.9229264855384827\n",
      "epoch 44: loss=0.9236359000205994\n",
      "epoch 45: loss=0.9238947629928589\n",
      "epoch 46: loss=0.9226088523864746\n",
      "epoch 47: loss=0.9228788018226624\n",
      "epoch 48: loss=0.9213634729385376\n",
      "epoch 49: loss=0.9210108518600464\n",
      "epoch 50: loss=0.9183241724967957\n",
      "epoch 51: loss=0.9159834384918213\n",
      "epoch 52: loss=0.9123610258102417\n",
      "epoch 53: loss=0.9129384160041809\n",
      "epoch 54: loss=0.9107380509376526\n",
      "epoch 55: loss=0.9087203145027161\n",
      "epoch 56: loss=0.90903240442276\n",
      "epoch 57: loss=0.9126068949699402\n",
      "epoch 58: loss=0.9085376858711243\n",
      "epoch 59: loss=0.9072176218032837\n",
      "epoch 60: loss=0.9064581990242004\n",
      "epoch 61: loss=0.9068660736083984\n",
      "epoch 62: loss=0.9050188660621643\n",
      "epoch 63: loss=0.9044109582901001\n",
      "epoch 64: loss=0.9042667746543884\n",
      "epoch 65: loss=0.904473602771759\n",
      "epoch 66: loss=0.9031108021736145\n",
      "epoch 67: loss=0.90267413854599\n",
      "epoch 68: loss=0.9042692184448242\n",
      "epoch 69: loss=0.9014961123466492\n",
      "epoch 70: loss=0.9003363251686096\n",
      "epoch 71: loss=0.9019020199775696\n",
      "epoch 72: loss=0.8999449610710144\n",
      "epoch 73: loss=0.9009597301483154\n",
      "epoch 74: loss=0.9011417031288147\n",
      "epoch 75: loss=0.9007349610328674\n",
      "epoch 76: loss=0.8988385796546936\n",
      "epoch 77: loss=0.8997641801834106\n",
      "epoch 78: loss=0.9003852009773254\n",
      "epoch 79: loss=0.8980506062507629\n",
      "epoch 80: loss=0.900393009185791\n",
      "epoch 81: loss=0.8979315161705017\n",
      "epoch 82: loss=0.8980677127838135\n",
      "epoch 83: loss=0.898287296295166\n",
      "epoch 84: loss=0.8938532471656799\n",
      "epoch 85: loss=0.8970401883125305\n",
      "epoch 86: loss=0.8961016535758972\n",
      "epoch 87: loss=0.8963387608528137\n",
      "epoch 88: loss=0.8948507308959961\n",
      "epoch 89: loss=0.8948470950126648\n",
      "epoch 90: loss=0.8941006064414978\n",
      "epoch 91: loss=0.8939011693000793\n",
      "epoch 92: loss=0.8922540545463562\n",
      "epoch 93: loss=0.8932970762252808\n",
      "epoch 94: loss=0.8919376730918884\n",
      "epoch 95: loss=0.8923792839050293\n",
      "epoch 96: loss=0.892012357711792\n",
      "epoch 97: loss=0.8931488990783691\n",
      "epoch 98: loss=0.8928037285804749\n",
      "epoch 99: loss=0.890819251537323\n",
      "epoch 100: loss=0.8926952481269836\n",
      "epoch 101: loss=0.8923593759536743\n",
      "epoch 102: loss=0.890335202217102\n",
      "epoch 103: loss=0.8914308547973633\n",
      "epoch 104: loss=0.8898033499717712\n",
      "epoch 105: loss=0.8912320137023926\n",
      "epoch 106: loss=0.8904587626457214\n",
      "epoch 107: loss=0.8908241987228394\n",
      "epoch 108: loss=0.8882594704627991\n",
      "epoch 109: loss=0.8889131546020508\n",
      "epoch 110: loss=0.8906452655792236\n",
      "epoch 111: loss=0.8893805146217346\n",
      "epoch 112: loss=0.8863919377326965\n",
      "epoch 113: loss=0.8866605162620544\n",
      "epoch 114: loss=0.88641756772995\n",
      "epoch 115: loss=0.8877373337745667\n",
      "epoch 116: loss=0.8871116638183594\n",
      "epoch 117: loss=0.8874549269676208\n",
      "epoch 118: loss=0.886052131652832\n",
      "epoch 119: loss=0.8862144351005554\n",
      "epoch 120: loss=0.8858857750892639\n",
      "epoch 121: loss=0.883883535861969\n",
      "epoch 122: loss=0.8854957818984985\n",
      "epoch 123: loss=0.8840600848197937\n",
      "epoch 124: loss=0.8852673768997192\n",
      "epoch 125: loss=0.884230375289917\n",
      "epoch 126: loss=0.8857831358909607\n",
      "epoch 127: loss=0.8834439516067505\n",
      "epoch 128: loss=0.8837132453918457\n",
      "epoch 129: loss=0.8843247294425964\n",
      "epoch 130: loss=0.8840531706809998\n",
      "epoch 131: loss=0.8840799927711487\n",
      "epoch 132: loss=0.8833954930305481\n",
      "epoch 133: loss=0.8832609057426453\n",
      "epoch 134: loss=0.8826872110366821\n",
      "epoch 135: loss=0.8826842904090881\n",
      "epoch 136: loss=0.8819767236709595\n",
      "epoch 137: loss=0.8818365931510925\n",
      "epoch 138: loss=0.8819657564163208\n",
      "epoch 139: loss=0.8823478817939758\n",
      "epoch 140: loss=0.882483720779419\n",
      "epoch 141: loss=0.8803694844245911\n",
      "epoch 142: loss=0.8804599642753601\n",
      "epoch 143: loss=0.8820405006408691\n",
      "epoch 144: loss=0.8809285163879395\n",
      "epoch 145: loss=0.8798667788505554\n",
      "epoch 146: loss=0.8804509043693542\n",
      "epoch 147: loss=0.879672646522522\n",
      "epoch 148: loss=0.8804378509521484\n",
      "epoch 149: loss=0.8790484666824341\n",
      "epoch 150: loss=0.8783221244812012\n",
      "epoch 151: loss=0.878699004650116\n",
      "epoch 152: loss=0.8797035813331604\n",
      "epoch 153: loss=0.8799842000007629\n",
      "epoch 154: loss=0.8788652420043945\n",
      "epoch 155: loss=0.8789563179016113\n",
      "epoch 156: loss=0.876815140247345\n",
      "epoch 157: loss=0.8807430863380432\n",
      "epoch 158: loss=0.8802889585494995\n",
      "epoch 159: loss=0.8760616183280945\n",
      "epoch 160: loss=0.8784973621368408\n",
      "epoch 161: loss=0.8787832260131836\n",
      "epoch 162: loss=0.8782004117965698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 163: loss=0.8769655227661133\n",
      "epoch 164: loss=0.8754913210868835\n",
      "epoch 165: loss=0.8757636547088623\n",
      "epoch 166: loss=0.8764020204544067\n",
      "epoch 167: loss=0.8765267729759216\n",
      "epoch 168: loss=0.8781284093856812\n",
      "epoch 169: loss=0.8769463300704956\n",
      "epoch 170: loss=0.8750534057617188\n",
      "epoch 171: loss=0.875176727771759\n",
      "epoch 172: loss=0.8763963580131531\n",
      "epoch 173: loss=0.8767045736312866\n",
      "epoch 174: loss=0.8760936260223389\n",
      "epoch 175: loss=0.8772964477539062\n",
      "epoch 176: loss=0.8774537444114685\n",
      "epoch 177: loss=0.8754714131355286\n",
      "epoch 178: loss=0.8776246905326843\n",
      "epoch 179: loss=0.876829206943512\n",
      "epoch 180: loss=0.8766036033630371\n",
      "epoch 181: loss=0.8764053583145142\n",
      "epoch 182: loss=0.874480128288269\n",
      "epoch 183: loss=0.8760537505149841\n",
      "epoch 184: loss=0.8756538033485413\n",
      "epoch 185: loss=0.8751910924911499\n",
      "epoch 186: loss=0.876029372215271\n",
      "epoch 187: loss=0.8748378157615662\n",
      "epoch 188: loss=0.875542938709259\n",
      "epoch 189: loss=0.8751617074012756\n",
      "epoch 190: loss=0.8747153282165527\n",
      "epoch 191: loss=0.8749598860740662\n",
      "epoch 192: loss=0.8760465979576111\n",
      "epoch 193: loss=0.8754714131355286\n",
      "epoch 194: loss=0.8761114478111267\n",
      "epoch 195: loss=0.8748598694801331\n",
      "epoch 196: loss=0.8741198182106018\n",
      "epoch 197: loss=0.8738268613815308\n",
      "epoch 198: loss=0.8726651668548584\n",
      "epoch 199: loss=0.8731244802474976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f856ab1774c4943af82167a108f5683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 97207.2421875\n",
      "Epoch 10, Loss: 21587.251953125\n",
      "Epoch 20, Loss: 15191.185546875\n",
      "Epoch 30, Loss: 13179.818359375\n",
      "Epoch 40, Loss: 12474.60546875\n",
      "Epoch 50, Loss: 12143.6083984375\n",
      "Epoch 60, Loss: 11986.3349609375\n",
      "Epoch 70, Loss: 11912.2734375\n",
      "Epoch 80, Loss: 11871.685546875\n",
      "Epoch 90, Loss: 11846.33203125\n",
      "Epoch 100, Loss: 11828.7919921875\n",
      "Epoch 110, Loss: 11815.45703125\n",
      "Epoch 120, Loss: 11804.75390625\n",
      "Epoch 130, Loss: 11795.8603515625\n",
      "Epoch 140, Loss: 11788.2734375\n",
      "Epoch 150, Loss: 11781.7197265625\n",
      "Epoch 160, Loss: 11776.0146484375\n",
      "Epoch 170, Loss: 11771.025390625\n",
      "Epoch 180, Loss: 11766.646484375\n",
      "Epoch 190, Loss: 11762.7958984375\n",
      "training patch with 267352 edges\n",
      "epoch 0: loss=4.612177848815918\n",
      "epoch 1: loss=4.504603385925293\n",
      "epoch 2: loss=4.265945911407471\n",
      "epoch 3: loss=4.096076011657715\n",
      "epoch 4: loss=3.846025228500366\n",
      "epoch 5: loss=3.5331151485443115\n",
      "epoch 6: loss=3.1815521717071533\n",
      "epoch 7: loss=2.8543035984039307\n",
      "epoch 8: loss=2.5904104709625244\n",
      "epoch 9: loss=2.45715069770813\n",
      "epoch 10: loss=2.3995583057403564\n",
      "epoch 11: loss=2.3064022064208984\n",
      "epoch 12: loss=2.171820640563965\n",
      "epoch 13: loss=2.0291800498962402\n",
      "epoch 14: loss=1.8696234226226807\n",
      "epoch 15: loss=1.732317328453064\n",
      "epoch 16: loss=1.6150879859924316\n",
      "epoch 17: loss=1.523402452468872\n",
      "epoch 18: loss=1.4615451097488403\n",
      "epoch 19: loss=1.4237033128738403\n",
      "epoch 20: loss=1.392362117767334\n",
      "epoch 21: loss=1.3600293397903442\n",
      "epoch 22: loss=1.3145203590393066\n",
      "epoch 23: loss=1.2721596956253052\n",
      "epoch 24: loss=1.214206337928772\n",
      "epoch 25: loss=1.1706321239471436\n",
      "epoch 26: loss=1.1338365077972412\n",
      "epoch 27: loss=1.1017751693725586\n",
      "epoch 28: loss=1.0759278535842896\n",
      "epoch 29: loss=1.0498096942901611\n",
      "epoch 30: loss=1.0322051048278809\n",
      "epoch 31: loss=1.0117862224578857\n",
      "epoch 32: loss=0.9967515468597412\n",
      "epoch 33: loss=0.9890825152397156\n",
      "epoch 34: loss=0.9789998531341553\n",
      "epoch 35: loss=0.9722943902015686\n",
      "epoch 36: loss=0.969526469707489\n",
      "epoch 37: loss=0.9639542698860168\n",
      "epoch 38: loss=0.9557033777236938\n",
      "epoch 39: loss=0.9480175971984863\n",
      "epoch 40: loss=0.939464271068573\n",
      "epoch 41: loss=0.9334760904312134\n",
      "epoch 42: loss=0.9313808083534241\n",
      "epoch 43: loss=0.9316525459289551\n",
      "epoch 44: loss=0.9293188452720642\n",
      "epoch 45: loss=0.9257967472076416\n",
      "epoch 46: loss=0.9220024943351746\n",
      "epoch 47: loss=0.9199321269989014\n",
      "epoch 48: loss=0.9186448454856873\n",
      "epoch 49: loss=0.9180701375007629\n",
      "epoch 50: loss=0.9153803586959839\n",
      "epoch 51: loss=0.9134942889213562\n",
      "epoch 52: loss=0.910589873790741\n",
      "epoch 53: loss=0.9100189208984375\n",
      "epoch 54: loss=0.9091977477073669\n",
      "epoch 55: loss=0.907392680644989\n",
      "epoch 56: loss=0.9070001840591431\n",
      "epoch 57: loss=0.9047666788101196\n",
      "epoch 58: loss=0.9045034646987915\n",
      "epoch 59: loss=0.9042681455612183\n",
      "epoch 60: loss=0.9034028649330139\n",
      "epoch 61: loss=0.9025980234146118\n",
      "epoch 62: loss=0.8998832702636719\n",
      "epoch 63: loss=0.8999171257019043\n",
      "epoch 64: loss=0.8999661207199097\n",
      "epoch 65: loss=0.9008972644805908\n",
      "epoch 66: loss=0.8982893824577332\n",
      "epoch 67: loss=0.8962038159370422\n",
      "epoch 68: loss=0.8966371417045593\n",
      "epoch 69: loss=0.8961578607559204\n",
      "epoch 70: loss=0.8952431082725525\n",
      "epoch 71: loss=0.8952563405036926\n",
      "epoch 72: loss=0.89399254322052\n",
      "epoch 73: loss=0.8952730894088745\n",
      "epoch 74: loss=0.8932110667228699\n",
      "epoch 75: loss=0.8930512070655823\n",
      "epoch 76: loss=0.8924468755722046\n",
      "epoch 77: loss=0.8922821283340454\n",
      "epoch 78: loss=0.8917810320854187\n",
      "epoch 79: loss=0.8916490077972412\n",
      "epoch 80: loss=0.8920071721076965\n",
      "epoch 81: loss=0.8891389966011047\n",
      "epoch 82: loss=0.8887345194816589\n",
      "epoch 83: loss=0.8897756338119507\n",
      "epoch 84: loss=0.8898372054100037\n",
      "epoch 85: loss=0.8879637718200684\n",
      "epoch 86: loss=0.8897092342376709\n",
      "epoch 87: loss=0.8884091377258301\n",
      "epoch 88: loss=0.8873835802078247\n",
      "epoch 89: loss=0.885706901550293\n",
      "epoch 90: loss=0.8857426643371582\n",
      "epoch 91: loss=0.8855429291725159\n",
      "epoch 92: loss=0.8851163983345032\n",
      "epoch 93: loss=0.8841132521629333\n",
      "epoch 94: loss=0.8838471174240112\n",
      "epoch 95: loss=0.8841840624809265\n",
      "epoch 96: loss=0.8836807012557983\n",
      "epoch 97: loss=0.8816133737564087\n",
      "epoch 98: loss=0.8814691305160522\n",
      "epoch 99: loss=0.881805956363678\n",
      "epoch 100: loss=0.8802047371864319\n",
      "epoch 101: loss=0.8794094324111938\n",
      "epoch 102: loss=0.880838930606842\n",
      "epoch 103: loss=0.8804466128349304\n",
      "epoch 104: loss=0.8787102699279785\n",
      "epoch 105: loss=0.87801194190979\n",
      "epoch 106: loss=0.875816285610199\n",
      "epoch 107: loss=0.8755789399147034\n",
      "epoch 108: loss=0.8758676052093506\n",
      "epoch 109: loss=0.874710202217102\n",
      "epoch 110: loss=0.8748270273208618\n",
      "epoch 111: loss=0.8737733960151672\n",
      "epoch 112: loss=0.8723803162574768\n",
      "epoch 113: loss=0.8715567588806152\n",
      "epoch 114: loss=0.871699869632721\n",
      "epoch 115: loss=0.8697649240493774\n",
      "epoch 116: loss=0.8691306114196777\n",
      "epoch 117: loss=0.8686749935150146\n",
      "epoch 118: loss=0.866653561592102\n",
      "epoch 119: loss=0.8671942949295044\n",
      "epoch 120: loss=0.866752028465271\n",
      "epoch 121: loss=0.8648890256881714\n",
      "epoch 122: loss=0.8640781044960022\n",
      "epoch 123: loss=0.8629806637763977\n",
      "epoch 124: loss=0.860611081123352\n",
      "epoch 125: loss=0.8611680269241333\n",
      "epoch 126: loss=0.8591845631599426\n",
      "epoch 127: loss=0.8577736020088196\n",
      "epoch 128: loss=0.856295645236969\n",
      "epoch 129: loss=0.8548613786697388\n",
      "epoch 130: loss=0.8538037538528442\n",
      "epoch 131: loss=0.8525853753089905\n",
      "epoch 132: loss=0.8534548878669739\n",
      "epoch 133: loss=0.8522819876670837\n",
      "epoch 134: loss=0.8501195311546326\n",
      "epoch 135: loss=0.8486695885658264\n",
      "epoch 136: loss=0.8497634530067444\n",
      "epoch 137: loss=0.8486998081207275\n",
      "epoch 138: loss=0.8474289178848267\n",
      "epoch 139: loss=0.8467115163803101\n",
      "epoch 140: loss=0.8450033664703369\n",
      "epoch 141: loss=0.8458907604217529\n",
      "epoch 142: loss=0.8441213369369507\n",
      "epoch 143: loss=0.8427186608314514\n",
      "epoch 144: loss=0.8427917957305908\n",
      "epoch 145: loss=0.841852605342865\n",
      "epoch 146: loss=0.8403919339179993\n",
      "epoch 147: loss=0.8397585153579712\n",
      "epoch 148: loss=0.8405731916427612\n",
      "epoch 149: loss=0.8396406173706055\n",
      "epoch 150: loss=0.8364200592041016\n",
      "epoch 151: loss=0.8366077542304993\n",
      "epoch 152: loss=0.836654007434845\n",
      "epoch 153: loss=0.8361704349517822\n",
      "epoch 154: loss=0.836780846118927\n",
      "epoch 155: loss=0.8343948125839233\n",
      "epoch 156: loss=0.834835410118103\n",
      "epoch 157: loss=0.8346197009086609\n",
      "epoch 158: loss=0.8328006267547607\n",
      "epoch 159: loss=0.8330206871032715\n",
      "epoch 160: loss=0.8323471546173096\n",
      "epoch 161: loss=0.8319186568260193\n",
      "epoch 162: loss=0.8316724300384521\n",
      "epoch 163: loss=0.8320695161819458\n",
      "epoch 164: loss=0.830851674079895\n",
      "epoch 165: loss=0.8301541209220886\n",
      "epoch 166: loss=0.8293859362602234\n",
      "epoch 167: loss=0.8283634781837463\n",
      "epoch 168: loss=0.8283085227012634\n",
      "epoch 169: loss=0.8286744952201843\n",
      "epoch 170: loss=0.8277670741081238\n",
      "epoch 171: loss=0.8269278407096863\n",
      "epoch 172: loss=0.827171802520752\n",
      "epoch 173: loss=0.8263436555862427\n",
      "epoch 174: loss=0.8250699639320374\n",
      "epoch 175: loss=0.8244990706443787\n",
      "epoch 176: loss=0.8247553110122681\n",
      "epoch 177: loss=0.8230128884315491\n",
      "epoch 178: loss=0.8226141333580017\n",
      "epoch 179: loss=0.8231180906295776\n",
      "epoch 180: loss=0.8228238821029663\n",
      "epoch 181: loss=0.8239054679870605\n",
      "epoch 182: loss=0.821707010269165\n",
      "epoch 183: loss=0.8220347762107849\n",
      "epoch 184: loss=0.822003185749054\n",
      "epoch 185: loss=0.8204739689826965\n",
      "epoch 186: loss=0.8203569054603577\n",
      "epoch 187: loss=0.8190076947212219\n",
      "epoch 188: loss=0.8189038634300232\n",
      "epoch 189: loss=0.8197129368782043\n",
      "epoch 190: loss=0.8187878131866455\n",
      "epoch 191: loss=0.8190876841545105\n",
      "epoch 192: loss=0.8178460597991943\n",
      "epoch 193: loss=0.8185475468635559\n",
      "epoch 194: loss=0.8174669146537781\n",
      "epoch 195: loss=0.8170602917671204\n",
      "epoch 196: loss=0.8174102902412415\n",
      "epoch 197: loss=0.8164477944374084\n",
      "epoch 198: loss=0.8145380616188049\n",
      "epoch 199: loss=0.8149636387825012\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=4.6519575119018555\n",
      "epoch 1: loss=4.524585723876953\n",
      "epoch 2: loss=4.37870979309082\n",
      "epoch 3: loss=4.18726110458374\n",
      "epoch 4: loss=3.9671311378479004\n",
      "epoch 5: loss=3.6242451667785645\n",
      "epoch 6: loss=3.324244976043701\n",
      "epoch 7: loss=3.0325169563293457\n",
      "epoch 8: loss=2.7780089378356934\n",
      "epoch 9: loss=2.612353801727295\n",
      "epoch 10: loss=2.545036554336548\n",
      "epoch 11: loss=2.495826005935669\n",
      "epoch 12: loss=2.359468936920166\n",
      "epoch 13: loss=2.2338409423828125\n",
      "epoch 14: loss=2.0821330547332764\n",
      "epoch 15: loss=1.9394102096557617\n",
      "epoch 16: loss=1.8200218677520752\n",
      "epoch 17: loss=1.725041151046753\n",
      "epoch 18: loss=1.6380823850631714\n",
      "epoch 19: loss=1.5773664712905884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss=1.5269536972045898\n",
      "epoch 21: loss=1.4980027675628662\n",
      "epoch 22: loss=1.4463587999343872\n",
      "epoch 23: loss=1.4185889959335327\n",
      "epoch 24: loss=1.3623391389846802\n",
      "epoch 25: loss=1.3111231327056885\n",
      "epoch 26: loss=1.2529550790786743\n",
      "epoch 27: loss=1.200234055519104\n",
      "epoch 28: loss=1.1594982147216797\n",
      "epoch 29: loss=1.1282124519348145\n",
      "epoch 30: loss=1.0995261669158936\n",
      "epoch 31: loss=1.0749562978744507\n",
      "epoch 32: loss=1.0507148504257202\n",
      "epoch 33: loss=1.0292631387710571\n",
      "epoch 34: loss=1.0065357685089111\n",
      "epoch 35: loss=0.9918495416641235\n",
      "epoch 36: loss=0.9802130460739136\n",
      "epoch 37: loss=0.9774495363235474\n",
      "epoch 38: loss=0.9698325395584106\n",
      "epoch 39: loss=0.96272873878479\n",
      "epoch 40: loss=0.9528959393501282\n",
      "epoch 41: loss=0.9399658441543579\n",
      "epoch 42: loss=0.9353577494621277\n",
      "epoch 43: loss=0.9274529814720154\n",
      "epoch 44: loss=0.9258549213409424\n",
      "epoch 45: loss=0.9236277937889099\n",
      "epoch 46: loss=0.9208487868309021\n",
      "epoch 47: loss=0.9145657420158386\n",
      "epoch 48: loss=0.9109350442886353\n",
      "epoch 49: loss=0.9108680486679077\n",
      "epoch 50: loss=0.9091356992721558\n",
      "epoch 51: loss=0.9070580005645752\n",
      "epoch 52: loss=0.9036841988563538\n",
      "epoch 53: loss=0.902306854724884\n",
      "epoch 54: loss=0.901373028755188\n",
      "epoch 55: loss=0.9010442495346069\n",
      "epoch 56: loss=0.8990928530693054\n",
      "epoch 57: loss=0.8969080448150635\n",
      "epoch 58: loss=0.896450400352478\n",
      "epoch 59: loss=0.8962369561195374\n",
      "epoch 60: loss=0.8940020799636841\n",
      "epoch 61: loss=0.8945887684822083\n",
      "epoch 62: loss=0.8927371501922607\n",
      "epoch 63: loss=0.8924846053123474\n",
      "epoch 64: loss=0.8915664553642273\n",
      "epoch 65: loss=0.8890020251274109\n",
      "epoch 66: loss=0.8909090757369995\n",
      "epoch 67: loss=0.889235258102417\n",
      "epoch 68: loss=0.888875424861908\n",
      "epoch 69: loss=0.8889181017875671\n",
      "epoch 70: loss=0.8878902792930603\n",
      "epoch 71: loss=0.8881763219833374\n",
      "epoch 72: loss=0.8877811431884766\n",
      "epoch 73: loss=0.8855555057525635\n",
      "epoch 74: loss=0.8851273655891418\n",
      "epoch 75: loss=0.8859440684318542\n",
      "epoch 76: loss=0.8858331441879272\n",
      "epoch 77: loss=0.8846092224121094\n",
      "epoch 78: loss=0.8835515379905701\n",
      "epoch 79: loss=0.8853117823600769\n",
      "epoch 80: loss=0.8826988935470581\n",
      "epoch 81: loss=0.8820486068725586\n",
      "epoch 82: loss=0.8824503421783447\n",
      "epoch 83: loss=0.8815158605575562\n",
      "epoch 84: loss=0.8815948963165283\n",
      "epoch 85: loss=0.8815677762031555\n",
      "epoch 86: loss=0.8813981413841248\n",
      "epoch 87: loss=0.881077229976654\n",
      "epoch 88: loss=0.8793622851371765\n",
      "epoch 89: loss=0.8795585632324219\n",
      "epoch 90: loss=0.8791245222091675\n",
      "epoch 91: loss=0.8786357045173645\n",
      "epoch 92: loss=0.8776949048042297\n",
      "epoch 93: loss=0.8786611557006836\n",
      "epoch 94: loss=0.8771650195121765\n",
      "epoch 95: loss=0.875698983669281\n",
      "epoch 96: loss=0.8759017586708069\n",
      "epoch 97: loss=0.8751899003982544\n",
      "epoch 98: loss=0.8763201236724854\n",
      "epoch 99: loss=0.8746302127838135\n",
      "epoch 100: loss=0.8744616508483887\n",
      "epoch 101: loss=0.8736615180969238\n",
      "epoch 102: loss=0.8734632134437561\n",
      "epoch 103: loss=0.8721291422843933\n",
      "epoch 104: loss=0.8721097707748413\n",
      "epoch 105: loss=0.8708946108818054\n",
      "epoch 106: loss=0.8702359795570374\n",
      "epoch 107: loss=0.8708416819572449\n",
      "epoch 108: loss=0.8705060482025146\n",
      "epoch 109: loss=0.8686712980270386\n",
      "epoch 110: loss=0.8683141469955444\n",
      "epoch 111: loss=0.8677530288696289\n",
      "epoch 112: loss=0.8674250841140747\n",
      "epoch 113: loss=0.866066575050354\n",
      "epoch 114: loss=0.8655461072921753\n",
      "epoch 115: loss=0.8665290474891663\n",
      "epoch 116: loss=0.8653472065925598\n",
      "epoch 117: loss=0.8639845252037048\n",
      "epoch 118: loss=0.8650738000869751\n",
      "epoch 119: loss=0.8643048405647278\n",
      "epoch 120: loss=0.8646059036254883\n",
      "epoch 121: loss=0.863490879535675\n",
      "epoch 122: loss=0.8629630208015442\n",
      "epoch 123: loss=0.861692488193512\n",
      "epoch 124: loss=0.8598225116729736\n",
      "epoch 125: loss=0.8616285920143127\n",
      "epoch 126: loss=0.8615318536758423\n",
      "epoch 127: loss=0.8592491149902344\n",
      "epoch 128: loss=0.8591582179069519\n",
      "epoch 129: loss=0.859483540058136\n",
      "epoch 130: loss=0.8577476143836975\n",
      "epoch 131: loss=0.8573929667472839\n",
      "epoch 132: loss=0.8561468124389648\n",
      "epoch 133: loss=0.8557919859886169\n",
      "epoch 134: loss=0.8545401692390442\n",
      "epoch 135: loss=0.8538731336593628\n",
      "epoch 136: loss=0.8524391651153564\n",
      "epoch 137: loss=0.8515055775642395\n",
      "epoch 138: loss=0.8508140444755554\n",
      "epoch 139: loss=0.8503255844116211\n",
      "epoch 140: loss=0.8498632311820984\n",
      "epoch 141: loss=0.848142147064209\n",
      "epoch 142: loss=0.8498606085777283\n",
      "epoch 143: loss=0.8467355966567993\n",
      "epoch 144: loss=0.8478399515151978\n",
      "epoch 145: loss=0.84644615650177\n",
      "epoch 146: loss=0.8445067405700684\n",
      "epoch 147: loss=0.8439930081367493\n",
      "epoch 148: loss=0.8432618379592896\n",
      "epoch 149: loss=0.8447067737579346\n",
      "epoch 150: loss=0.8425546884536743\n",
      "epoch 151: loss=0.8414521217346191\n",
      "epoch 152: loss=0.8404236435890198\n",
      "epoch 153: loss=0.8398619890213013\n",
      "epoch 154: loss=0.8399343490600586\n",
      "epoch 155: loss=0.8391615152359009\n",
      "epoch 156: loss=0.8392783403396606\n",
      "epoch 157: loss=0.837843656539917\n",
      "epoch 158: loss=0.8400471806526184\n",
      "epoch 159: loss=0.8397075533866882\n",
      "epoch 160: loss=0.8369858264923096\n",
      "epoch 161: loss=0.8406432867050171\n",
      "epoch 162: loss=0.8389676213264465\n",
      "epoch 163: loss=0.8384719491004944\n",
      "epoch 164: loss=0.837361216545105\n",
      "epoch 165: loss=0.8372479677200317\n",
      "epoch 166: loss=0.8371960520744324\n",
      "epoch 167: loss=0.8355896472930908\n",
      "epoch 168: loss=0.8361089825630188\n",
      "epoch 169: loss=0.83645099401474\n",
      "epoch 170: loss=0.835793673992157\n",
      "epoch 171: loss=0.836376965045929\n",
      "epoch 172: loss=0.8350517749786377\n",
      "epoch 173: loss=0.8357290029525757\n",
      "epoch 174: loss=0.8338263034820557\n",
      "epoch 175: loss=0.8350926041603088\n",
      "epoch 176: loss=0.8345578908920288\n",
      "epoch 177: loss=0.833384096622467\n",
      "epoch 178: loss=0.8340908885002136\n",
      "epoch 179: loss=0.833128809928894\n",
      "epoch 180: loss=0.8336221575737\n",
      "epoch 181: loss=0.8346814513206482\n",
      "epoch 182: loss=0.8322263956069946\n",
      "epoch 183: loss=0.8335775136947632\n",
      "epoch 184: loss=0.833781898021698\n",
      "epoch 185: loss=0.831203043460846\n",
      "epoch 186: loss=0.8318607211112976\n",
      "epoch 187: loss=0.8330394625663757\n",
      "epoch 188: loss=0.8314847350120544\n",
      "epoch 189: loss=0.8331624865531921\n",
      "epoch 190: loss=0.8326660990715027\n",
      "epoch 191: loss=0.8323642611503601\n",
      "epoch 192: loss=0.8312199115753174\n",
      "epoch 193: loss=0.8315673470497131\n",
      "epoch 194: loss=0.8310438990592957\n",
      "epoch 195: loss=0.8311808109283447\n",
      "epoch 196: loss=0.8312898874282837\n",
      "epoch 197: loss=0.8305895924568176\n",
      "epoch 198: loss=0.8310729265213013\n",
      "epoch 199: loss=0.8305932283401489\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=4.690889358520508\n",
      "epoch 1: loss=4.452322006225586\n",
      "epoch 2: loss=4.2242021560668945\n",
      "epoch 3: loss=3.964916467666626\n",
      "epoch 4: loss=3.6289918422698975\n",
      "epoch 5: loss=3.346355438232422\n",
      "epoch 6: loss=3.0151865482330322\n",
      "epoch 7: loss=2.678586721420288\n",
      "epoch 8: loss=2.5215439796447754\n",
      "epoch 9: loss=2.4285333156585693\n",
      "epoch 10: loss=2.3426406383514404\n",
      "epoch 11: loss=2.1973884105682373\n",
      "epoch 12: loss=2.01945161819458\n",
      "epoch 13: loss=1.8646159172058105\n",
      "epoch 14: loss=1.7144685983657837\n",
      "epoch 15: loss=1.6079201698303223\n",
      "epoch 16: loss=1.5415239334106445\n",
      "epoch 17: loss=1.4873266220092773\n",
      "epoch 18: loss=1.4623898267745972\n",
      "epoch 19: loss=1.4119701385498047\n",
      "epoch 20: loss=1.3893101215362549\n",
      "epoch 21: loss=1.3487181663513184\n",
      "epoch 22: loss=1.3175835609436035\n",
      "epoch 23: loss=1.275752067565918\n",
      "epoch 24: loss=1.2220247983932495\n",
      "epoch 25: loss=1.1916632652282715\n",
      "epoch 26: loss=1.1668246984481812\n",
      "epoch 27: loss=1.130635142326355\n",
      "epoch 28: loss=1.0975252389907837\n",
      "epoch 29: loss=1.0697441101074219\n",
      "epoch 30: loss=1.047190546989441\n",
      "epoch 31: loss=1.029432773590088\n",
      "epoch 32: loss=1.0181527137756348\n",
      "epoch 33: loss=1.0131621360778809\n",
      "epoch 34: loss=1.0151368379592896\n",
      "epoch 35: loss=1.005045771598816\n",
      "epoch 36: loss=1.000113606452942\n",
      "epoch 37: loss=0.9898711442947388\n",
      "epoch 38: loss=0.9899001717567444\n",
      "epoch 39: loss=0.984560489654541\n",
      "epoch 40: loss=0.9802255034446716\n",
      "epoch 41: loss=0.9741666913032532\n",
      "epoch 42: loss=0.9722698330879211\n",
      "epoch 43: loss=0.9767345190048218\n",
      "epoch 44: loss=0.9669908285140991\n",
      "epoch 45: loss=0.9721909165382385\n",
      "epoch 46: loss=0.9644620418548584\n",
      "epoch 47: loss=0.9646264314651489\n",
      "epoch 48: loss=0.9591809511184692\n",
      "epoch 49: loss=0.9568350911140442\n",
      "epoch 50: loss=0.9576311111450195\n",
      "epoch 51: loss=0.9554148316383362\n",
      "epoch 52: loss=0.9556058645248413\n",
      "epoch 53: loss=0.9544201493263245\n",
      "epoch 54: loss=0.9528552293777466\n",
      "epoch 55: loss=0.9506052732467651\n",
      "epoch 56: loss=0.9500696659088135\n",
      "epoch 57: loss=0.9481194615364075\n",
      "epoch 58: loss=0.9463228583335876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59: loss=0.9453089237213135\n",
      "epoch 60: loss=0.9430482983589172\n",
      "epoch 61: loss=0.9456353187561035\n",
      "epoch 62: loss=0.9436289668083191\n",
      "epoch 63: loss=0.942068874835968\n",
      "epoch 64: loss=0.9428364634513855\n",
      "epoch 65: loss=0.9428065419197083\n",
      "epoch 66: loss=0.9420298337936401\n",
      "epoch 67: loss=0.9415183067321777\n",
      "epoch 68: loss=0.9412574172019958\n",
      "epoch 69: loss=0.939270555973053\n",
      "epoch 70: loss=0.9391975402832031\n",
      "epoch 71: loss=0.9398877024650574\n",
      "epoch 72: loss=0.9372522830963135\n",
      "epoch 73: loss=0.9344562292098999\n",
      "epoch 74: loss=0.9359439611434937\n",
      "epoch 75: loss=0.9348883032798767\n",
      "epoch 76: loss=0.9341148734092712\n",
      "epoch 77: loss=0.9359493851661682\n",
      "epoch 78: loss=0.9340367913246155\n",
      "epoch 79: loss=0.9328848123550415\n",
      "epoch 80: loss=0.9317854046821594\n",
      "epoch 81: loss=0.9338986277580261\n",
      "epoch 82: loss=0.9330115914344788\n",
      "epoch 83: loss=0.9318273663520813\n",
      "epoch 84: loss=0.9300010800361633\n",
      "epoch 85: loss=0.9319599866867065\n",
      "epoch 86: loss=0.9290816187858582\n",
      "epoch 87: loss=0.9283549785614014\n",
      "epoch 88: loss=0.9297471046447754\n",
      "epoch 89: loss=0.9287669658660889\n",
      "epoch 90: loss=0.9262534379959106\n",
      "epoch 91: loss=0.9275808930397034\n",
      "epoch 92: loss=0.9285881519317627\n",
      "epoch 93: loss=0.9254140853881836\n",
      "epoch 94: loss=0.9259125590324402\n",
      "epoch 95: loss=0.926919162273407\n",
      "epoch 96: loss=0.9261364340782166\n",
      "epoch 97: loss=0.9264537692070007\n",
      "epoch 98: loss=0.926946759223938\n",
      "epoch 99: loss=0.925329864025116\n",
      "epoch 100: loss=0.9223371148109436\n",
      "epoch 101: loss=0.9243085980415344\n",
      "epoch 102: loss=0.9196979999542236\n",
      "epoch 103: loss=0.923437237739563\n",
      "epoch 104: loss=0.920417070388794\n",
      "epoch 105: loss=0.9230017066001892\n",
      "epoch 106: loss=0.9217284321784973\n",
      "epoch 107: loss=0.918924868106842\n",
      "epoch 108: loss=0.9196798801422119\n",
      "epoch 109: loss=0.918232262134552\n",
      "epoch 110: loss=0.9150347709655762\n",
      "epoch 111: loss=0.9198616743087769\n",
      "epoch 112: loss=0.9165457487106323\n",
      "epoch 113: loss=0.916178822517395\n",
      "epoch 114: loss=0.9165185689926147\n",
      "epoch 115: loss=0.916434645652771\n",
      "epoch 116: loss=0.9138831496238708\n",
      "epoch 117: loss=0.9139599204063416\n",
      "epoch 118: loss=0.9123581647872925\n",
      "epoch 119: loss=0.9118003249168396\n",
      "epoch 120: loss=0.9144484996795654\n",
      "epoch 121: loss=0.9118532538414001\n",
      "epoch 122: loss=0.9108291864395142\n",
      "epoch 123: loss=0.9082677364349365\n",
      "epoch 124: loss=0.9081808924674988\n",
      "epoch 125: loss=0.9073126316070557\n",
      "epoch 126: loss=0.9093309640884399\n",
      "epoch 127: loss=0.9078888297080994\n",
      "epoch 128: loss=0.9074655175209045\n",
      "epoch 129: loss=0.9055569767951965\n",
      "epoch 130: loss=0.907624363899231\n",
      "epoch 131: loss=0.9064388871192932\n",
      "epoch 132: loss=0.9057783484458923\n",
      "epoch 133: loss=0.9020934104919434\n",
      "epoch 134: loss=0.9013732075691223\n",
      "epoch 135: loss=0.9017082452774048\n",
      "epoch 136: loss=0.9021681547164917\n",
      "epoch 137: loss=0.9020567536354065\n",
      "epoch 138: loss=0.8992159962654114\n",
      "epoch 139: loss=0.8969597220420837\n",
      "epoch 140: loss=0.8959563374519348\n",
      "epoch 141: loss=0.895770788192749\n",
      "epoch 142: loss=0.8924242854118347\n",
      "epoch 143: loss=0.8921621441841125\n",
      "epoch 144: loss=0.8905650973320007\n",
      "epoch 145: loss=0.8866377472877502\n",
      "epoch 146: loss=0.8862918615341187\n",
      "epoch 147: loss=0.886768102645874\n",
      "epoch 148: loss=0.8867295384407043\n",
      "epoch 149: loss=0.8848240971565247\n",
      "epoch 150: loss=0.8836486339569092\n",
      "epoch 151: loss=0.8846737742424011\n",
      "epoch 152: loss=0.8829643726348877\n",
      "epoch 153: loss=0.8830269575119019\n",
      "epoch 154: loss=0.8815038800239563\n",
      "epoch 155: loss=0.8824244737625122\n",
      "epoch 156: loss=0.881229043006897\n",
      "epoch 157: loss=0.8829629421234131\n",
      "epoch 158: loss=0.8805045485496521\n",
      "epoch 159: loss=0.878777801990509\n",
      "epoch 160: loss=0.8800892233848572\n",
      "epoch 161: loss=0.8795769214630127\n",
      "epoch 162: loss=0.8783079385757446\n",
      "epoch 163: loss=0.8772335648536682\n",
      "epoch 164: loss=0.8779091238975525\n",
      "epoch 165: loss=0.8785929083824158\n",
      "epoch 166: loss=0.8748392462730408\n",
      "epoch 167: loss=0.8768008351325989\n",
      "epoch 168: loss=0.8739431500434875\n",
      "epoch 169: loss=0.8754172921180725\n",
      "epoch 170: loss=0.8759500980377197\n",
      "epoch 171: loss=0.8735519051551819\n",
      "epoch 172: loss=0.8744690418243408\n",
      "epoch 173: loss=0.871282696723938\n",
      "epoch 174: loss=0.8741390109062195\n",
      "epoch 175: loss=0.8727339506149292\n",
      "epoch 176: loss=0.8721163868904114\n",
      "epoch 177: loss=0.8718193769454956\n",
      "epoch 178: loss=0.8705351948738098\n",
      "epoch 179: loss=0.8698221445083618\n",
      "epoch 180: loss=0.8708446621894836\n",
      "epoch 181: loss=0.8689377903938293\n",
      "epoch 182: loss=0.8714058995246887\n",
      "epoch 183: loss=0.8710809946060181\n",
      "epoch 184: loss=0.8674408197402954\n",
      "epoch 185: loss=0.8701757788658142\n",
      "epoch 186: loss=0.8697901368141174\n",
      "epoch 187: loss=0.8684598207473755\n",
      "epoch 188: loss=0.8673981428146362\n",
      "epoch 189: loss=0.8661207556724548\n",
      "epoch 190: loss=0.8681399822235107\n",
      "epoch 191: loss=0.8642666935920715\n",
      "epoch 192: loss=0.8658366799354553\n",
      "epoch 193: loss=0.8661611676216125\n",
      "epoch 194: loss=0.8685072660446167\n",
      "epoch 195: loss=0.8679729104042053\n",
      "epoch 196: loss=0.8652242422103882\n",
      "epoch 197: loss=0.8693287968635559\n",
      "epoch 198: loss=0.865175187587738\n",
      "epoch 199: loss=0.8647758364677429\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=4.2992143630981445\n",
      "epoch 1: loss=4.107751846313477\n",
      "epoch 2: loss=3.8890860080718994\n",
      "epoch 3: loss=4.179312705993652\n",
      "epoch 4: loss=3.801750421524048\n",
      "epoch 5: loss=3.67582631111145\n",
      "epoch 6: loss=3.2833127975463867\n",
      "epoch 7: loss=2.904967784881592\n",
      "epoch 8: loss=3.1987667083740234\n",
      "epoch 9: loss=2.9918675422668457\n",
      "epoch 10: loss=3.1305394172668457\n",
      "epoch 11: loss=2.7298388481140137\n",
      "epoch 12: loss=2.6618921756744385\n",
      "epoch 13: loss=2.426412582397461\n",
      "epoch 14: loss=2.645932197570801\n",
      "epoch 15: loss=2.2067079544067383\n",
      "epoch 16: loss=2.092374086380005\n",
      "epoch 17: loss=1.9005826711654663\n",
      "epoch 18: loss=2.0928702354431152\n",
      "epoch 19: loss=1.819312334060669\n",
      "epoch 20: loss=1.6954317092895508\n",
      "epoch 21: loss=1.5497492551803589\n",
      "epoch 22: loss=1.6255558729171753\n",
      "epoch 23: loss=1.5221799612045288\n",
      "epoch 24: loss=1.5685420036315918\n",
      "epoch 25: loss=1.5691388845443726\n",
      "epoch 26: loss=1.7114094495773315\n",
      "epoch 27: loss=1.3750211000442505\n",
      "epoch 28: loss=1.547409176826477\n",
      "epoch 29: loss=1.5297902822494507\n",
      "epoch 30: loss=1.515756368637085\n",
      "epoch 31: loss=1.4548083543777466\n",
      "epoch 32: loss=1.5176832675933838\n",
      "epoch 33: loss=1.5489835739135742\n",
      "epoch 34: loss=1.4144983291625977\n",
      "epoch 35: loss=1.522693395614624\n",
      "epoch 36: loss=1.4306609630584717\n",
      "epoch 37: loss=1.506187915802002\n",
      "epoch 38: loss=1.4554436206817627\n",
      "epoch 39: loss=1.407230019569397\n",
      "epoch 40: loss=1.4804155826568604\n",
      "epoch 41: loss=1.4257760047912598\n",
      "epoch 42: loss=1.427147388458252\n",
      "epoch 43: loss=1.5214869976043701\n",
      "epoch 44: loss=1.4040876626968384\n",
      "epoch 45: loss=1.3792229890823364\n",
      "epoch 46: loss=1.441379427909851\n",
      "epoch 47: loss=1.5716599225997925\n",
      "epoch 48: loss=1.4397491216659546\n",
      "epoch 49: loss=1.3586416244506836\n",
      "epoch 50: loss=1.3823108673095703\n",
      "epoch 51: loss=1.4009233713150024\n",
      "epoch 52: loss=1.4174280166625977\n",
      "epoch 53: loss=1.3833121061325073\n",
      "epoch 54: loss=1.4535876512527466\n",
      "epoch 55: loss=1.428735613822937\n",
      "epoch 56: loss=1.4982314109802246\n",
      "epoch 57: loss=1.4696393013000488\n",
      "epoch 58: loss=1.465315580368042\n",
      "epoch 59: loss=1.4890495538711548\n",
      "epoch 60: loss=1.4664082527160645\n",
      "epoch 61: loss=1.3996918201446533\n",
      "epoch 62: loss=1.4099620580673218\n",
      "epoch 63: loss=1.405794382095337\n",
      "epoch 64: loss=1.4218730926513672\n",
      "epoch 65: loss=1.4256740808486938\n",
      "epoch 66: loss=1.4486290216445923\n",
      "epoch 67: loss=1.3956718444824219\n",
      "epoch 68: loss=1.432531476020813\n",
      "epoch 69: loss=1.3983592987060547\n",
      "epoch 70: loss=1.4467897415161133\n",
      "epoch 71: loss=1.4004881381988525\n",
      "epoch 72: loss=1.4750828742980957\n",
      "epoch 73: loss=1.4369041919708252\n",
      "epoch 74: loss=1.3969979286193848\n",
      "epoch 75: loss=1.4325969219207764\n",
      "epoch 76: loss=1.398351788520813\n",
      "epoch 77: loss=1.4171451330184937\n",
      "epoch 78: loss=1.4154102802276611\n",
      "epoch 79: loss=1.4308878183364868\n",
      "epoch 80: loss=1.4447224140167236\n",
      "epoch 81: loss=1.4139500856399536\n",
      "epoch 82: loss=1.4484277963638306\n",
      "epoch 83: loss=1.4014079570770264\n",
      "epoch 84: loss=1.4141287803649902\n",
      "epoch 85: loss=1.4177443981170654\n",
      "epoch 86: loss=1.4099922180175781\n",
      "epoch 87: loss=1.3775725364685059\n",
      "epoch 88: loss=1.5007566213607788\n",
      "epoch 89: loss=1.4481045007705688\n",
      "epoch 90: loss=1.399303674697876\n",
      "epoch 91: loss=1.4616734981536865\n",
      "epoch 92: loss=1.4335741996765137\n",
      "epoch 93: loss=1.4444018602371216\n",
      "epoch 94: loss=1.3999748229980469\n",
      "epoch 95: loss=1.3784761428833008\n",
      "epoch 96: loss=1.467099666595459\n",
      "epoch 97: loss=1.4489651918411255\n",
      "epoch 98: loss=1.3765373229980469\n",
      "epoch 99: loss=1.3857728242874146\n",
      "epoch 100: loss=1.4171634912490845\n",
      "epoch 101: loss=1.5022858381271362\n",
      "epoch 102: loss=1.3910735845565796\n",
      "epoch 103: loss=1.4203962087631226\n",
      "epoch 104: loss=1.5080677270889282\n",
      "epoch 105: loss=1.4180296659469604\n",
      "epoch 106: loss=1.4688520431518555\n",
      "epoch 107: loss=1.4315260648727417\n",
      "epoch 108: loss=1.4835336208343506\n",
      "epoch 109: loss=1.3935010433197021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110: loss=1.388581395149231\n",
      "epoch 111: loss=1.4163711071014404\n",
      "epoch 112: loss=1.3743184804916382\n",
      "epoch 113: loss=1.3654348850250244\n",
      "epoch 114: loss=1.4190939664840698\n",
      "epoch 115: loss=1.4122141599655151\n",
      "epoch 116: loss=1.504091501235962\n",
      "epoch 117: loss=1.468527913093567\n",
      "epoch 118: loss=1.4316033124923706\n",
      "epoch 119: loss=1.3690011501312256\n",
      "epoch 120: loss=1.4021025896072388\n",
      "epoch 121: loss=1.3984463214874268\n",
      "epoch 122: loss=1.4672356843948364\n",
      "epoch 123: loss=1.4087464809417725\n",
      "epoch 124: loss=1.4047163724899292\n",
      "epoch 125: loss=1.4423093795776367\n",
      "epoch 126: loss=1.4367929697036743\n",
      "epoch 127: loss=1.433138370513916\n",
      "epoch 128: loss=1.4493683576583862\n",
      "epoch 129: loss=1.4569587707519531\n",
      "epoch 130: loss=1.416630744934082\n",
      "epoch 131: loss=1.4688706398010254\n",
      "epoch 132: loss=1.2895525693893433\n",
      "epoch 133: loss=1.410771369934082\n",
      "epoch 134: loss=1.4195939302444458\n",
      "epoch 135: loss=1.3939189910888672\n",
      "epoch 136: loss=1.4217933416366577\n",
      "epoch 137: loss=1.4313433170318604\n",
      "epoch 138: loss=1.4713245630264282\n",
      "epoch 139: loss=1.4038586616516113\n",
      "epoch 140: loss=1.4561327695846558\n",
      "epoch 141: loss=1.4151359796524048\n",
      "epoch 142: loss=1.4501696825027466\n",
      "epoch 143: loss=1.3707505464553833\n",
      "epoch 144: loss=1.4244608879089355\n",
      "epoch 145: loss=1.4403403997421265\n",
      "epoch 146: loss=1.4138095378875732\n",
      "epoch 147: loss=1.402523159980774\n",
      "epoch 148: loss=1.450510859489441\n",
      "epoch 149: loss=1.4907214641571045\n",
      "epoch 150: loss=1.3962384462356567\n",
      "epoch 151: loss=1.4494366645812988\n",
      "epoch 152: loss=1.413082480430603\n",
      "epoch 153: loss=1.4348855018615723\n",
      "epoch 154: loss=1.3898005485534668\n",
      "epoch 155: loss=1.469301462173462\n",
      "epoch 156: loss=1.4602024555206299\n",
      "epoch 157: loss=1.4220874309539795\n",
      "epoch 158: loss=1.4287585020065308\n",
      "epoch 159: loss=1.4418928623199463\n",
      "epoch 160: loss=1.4052385091781616\n",
      "epoch 161: loss=1.420636773109436\n",
      "epoch 162: loss=1.4571990966796875\n",
      "epoch 163: loss=1.4350281953811646\n",
      "epoch 164: loss=1.4010539054870605\n",
      "epoch 165: loss=1.4255121946334839\n",
      "epoch 166: loss=1.3832372426986694\n",
      "epoch 167: loss=1.3862340450286865\n",
      "epoch 168: loss=1.4577550888061523\n",
      "epoch 169: loss=1.3693469762802124\n",
      "epoch 170: loss=1.3974324464797974\n",
      "epoch 171: loss=1.3740434646606445\n",
      "epoch 172: loss=1.4504921436309814\n",
      "epoch 173: loss=1.4482111930847168\n",
      "epoch 174: loss=1.428417444229126\n",
      "epoch 175: loss=1.4715849161148071\n",
      "epoch 176: loss=1.343498945236206\n",
      "epoch 177: loss=1.3683401346206665\n",
      "epoch 178: loss=1.383231282234192\n",
      "epoch 179: loss=1.473750352859497\n",
      "epoch 180: loss=1.404674768447876\n",
      "epoch 181: loss=1.3689502477645874\n",
      "epoch 182: loss=1.483973503112793\n",
      "epoch 183: loss=1.444345235824585\n",
      "epoch 184: loss=1.3915711641311646\n",
      "epoch 185: loss=1.4201598167419434\n",
      "epoch 186: loss=1.3909038305282593\n",
      "epoch 187: loss=1.4095017910003662\n",
      "epoch 188: loss=1.4198493957519531\n",
      "epoch 189: loss=1.4466707706451416\n",
      "epoch 190: loss=1.3621636629104614\n",
      "epoch 191: loss=1.4183932542800903\n",
      "epoch 192: loss=1.392366647720337\n",
      "epoch 193: loss=1.4237544536590576\n",
      "epoch 194: loss=1.4210562705993652\n",
      "epoch 195: loss=1.369263768196106\n",
      "epoch 196: loss=1.4047149419784546\n",
      "epoch 197: loss=1.464769721031189\n",
      "epoch 198: loss=1.4399908781051636\n",
      "epoch 199: loss=1.4147576093673706\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=4.705955505371094\n",
      "epoch 1: loss=4.459597587585449\n",
      "epoch 2: loss=4.218183994293213\n",
      "epoch 3: loss=3.877782106399536\n",
      "epoch 4: loss=3.44464111328125\n",
      "epoch 5: loss=3.020207643508911\n",
      "epoch 6: loss=2.657435417175293\n",
      "epoch 7: loss=2.513664722442627\n",
      "epoch 8: loss=2.4612314701080322\n",
      "epoch 9: loss=2.3943445682525635\n",
      "epoch 10: loss=2.2874903678894043\n",
      "epoch 11: loss=2.123372793197632\n",
      "epoch 12: loss=1.9548711776733398\n",
      "epoch 13: loss=1.80156409740448\n",
      "epoch 14: loss=1.6788126230239868\n",
      "epoch 15: loss=1.591705560684204\n",
      "epoch 16: loss=1.531485915184021\n",
      "epoch 17: loss=1.4929779767990112\n",
      "epoch 18: loss=1.4677146673202515\n",
      "epoch 19: loss=1.4448045492172241\n",
      "epoch 20: loss=1.3984496593475342\n",
      "epoch 21: loss=1.3558340072631836\n",
      "epoch 22: loss=1.299890398979187\n",
      "epoch 23: loss=1.241297721862793\n",
      "epoch 24: loss=1.2043037414550781\n",
      "epoch 25: loss=1.169113039970398\n",
      "epoch 26: loss=1.1508532762527466\n",
      "epoch 27: loss=1.119611382484436\n",
      "epoch 28: loss=1.0932384729385376\n",
      "epoch 29: loss=1.060325026512146\n",
      "epoch 30: loss=1.0309453010559082\n",
      "epoch 31: loss=1.0013577938079834\n",
      "epoch 32: loss=0.989205002784729\n",
      "epoch 33: loss=0.9758924841880798\n",
      "epoch 34: loss=0.9655396938323975\n",
      "epoch 35: loss=0.9595126509666443\n",
      "epoch 36: loss=0.9530182480812073\n",
      "epoch 37: loss=0.946540117263794\n",
      "epoch 38: loss=0.9395515322685242\n",
      "epoch 39: loss=0.928824245929718\n",
      "epoch 40: loss=0.9216269254684448\n",
      "epoch 41: loss=0.9131097197532654\n",
      "epoch 42: loss=0.9060351252555847\n",
      "epoch 43: loss=0.8993180394172668\n",
      "epoch 44: loss=0.8968226909637451\n",
      "epoch 45: loss=0.8917844891548157\n",
      "epoch 46: loss=0.8902510404586792\n",
      "epoch 47: loss=0.8855904340744019\n",
      "epoch 48: loss=0.8875201940536499\n",
      "epoch 49: loss=0.8840556144714355\n",
      "epoch 50: loss=0.8820028901100159\n",
      "epoch 51: loss=0.8801320791244507\n",
      "epoch 52: loss=0.8784973621368408\n",
      "epoch 53: loss=0.874937117099762\n",
      "epoch 54: loss=0.8731712102890015\n",
      "epoch 55: loss=0.8719846606254578\n",
      "epoch 56: loss=0.8705612421035767\n",
      "epoch 57: loss=0.8684142231941223\n",
      "epoch 58: loss=0.8678675293922424\n",
      "epoch 59: loss=0.8663891553878784\n",
      "epoch 60: loss=0.8661625385284424\n",
      "epoch 61: loss=0.8632888793945312\n",
      "epoch 62: loss=0.8638485074043274\n",
      "epoch 63: loss=0.8617377877235413\n",
      "epoch 64: loss=0.8614710569381714\n",
      "epoch 65: loss=0.8587870597839355\n",
      "epoch 66: loss=0.859497606754303\n",
      "epoch 67: loss=0.858258843421936\n",
      "epoch 68: loss=0.8564171195030212\n",
      "epoch 69: loss=0.8561719655990601\n",
      "epoch 70: loss=0.8548358082771301\n",
      "epoch 71: loss=0.8555685877799988\n",
      "epoch 72: loss=0.8551203012466431\n",
      "epoch 73: loss=0.8531327247619629\n",
      "epoch 74: loss=0.8536373376846313\n",
      "epoch 75: loss=0.8520228266716003\n",
      "epoch 76: loss=0.8513473272323608\n",
      "epoch 77: loss=0.8519874215126038\n",
      "epoch 78: loss=0.8513067960739136\n",
      "epoch 79: loss=0.8498117327690125\n",
      "epoch 80: loss=0.8495251536369324\n",
      "epoch 81: loss=0.8490689992904663\n",
      "epoch 82: loss=0.848544716835022\n",
      "epoch 83: loss=0.848314106464386\n",
      "epoch 84: loss=0.8470028638839722\n",
      "epoch 85: loss=0.8467397689819336\n",
      "epoch 86: loss=0.845725417137146\n",
      "epoch 87: loss=0.8454058170318604\n",
      "epoch 88: loss=0.8452711701393127\n",
      "epoch 89: loss=0.8436301350593567\n",
      "epoch 90: loss=0.8431265354156494\n",
      "epoch 91: loss=0.8432930707931519\n",
      "epoch 92: loss=0.8423928618431091\n",
      "epoch 93: loss=0.8418004512786865\n",
      "epoch 94: loss=0.8420675992965698\n",
      "epoch 95: loss=0.8426163792610168\n",
      "epoch 96: loss=0.8401188850402832\n",
      "epoch 97: loss=0.8399370312690735\n",
      "epoch 98: loss=0.8397862315177917\n",
      "epoch 99: loss=0.8392030596733093\n",
      "epoch 100: loss=0.8371595144271851\n",
      "epoch 101: loss=0.8383704423904419\n",
      "epoch 102: loss=0.8367190957069397\n",
      "epoch 103: loss=0.8383162617683411\n",
      "epoch 104: loss=0.8372172713279724\n",
      "epoch 105: loss=0.8352392911911011\n",
      "epoch 106: loss=0.8355764746665955\n",
      "epoch 107: loss=0.8365145325660706\n",
      "epoch 108: loss=0.8347063064575195\n",
      "epoch 109: loss=0.8348206877708435\n",
      "epoch 110: loss=0.8344992399215698\n",
      "epoch 111: loss=0.8342320322990417\n",
      "epoch 112: loss=0.8340502381324768\n",
      "epoch 113: loss=0.8326232433319092\n",
      "epoch 114: loss=0.8316794633865356\n",
      "epoch 115: loss=0.8316108584403992\n",
      "epoch 116: loss=0.8332028985023499\n",
      "epoch 117: loss=0.8309159874916077\n",
      "epoch 118: loss=0.8323716521263123\n",
      "epoch 119: loss=0.8317956328392029\n",
      "epoch 120: loss=0.8302062153816223\n",
      "epoch 121: loss=0.8302885890007019\n",
      "epoch 122: loss=0.829974353313446\n",
      "epoch 123: loss=0.8296812772750854\n",
      "epoch 124: loss=0.8288232684135437\n",
      "epoch 125: loss=0.8309453725814819\n",
      "epoch 126: loss=0.8303562998771667\n",
      "epoch 127: loss=0.8283483982086182\n",
      "epoch 128: loss=0.8301493525505066\n",
      "epoch 129: loss=0.8301902413368225\n",
      "epoch 130: loss=0.8280861377716064\n",
      "epoch 131: loss=0.8278196454048157\n",
      "epoch 132: loss=0.8282264471054077\n",
      "epoch 133: loss=0.8285727500915527\n",
      "epoch 134: loss=0.827987551689148\n",
      "epoch 135: loss=0.8276646733283997\n",
      "epoch 136: loss=0.8272672295570374\n",
      "epoch 137: loss=0.8274749517440796\n",
      "epoch 138: loss=0.8273348808288574\n",
      "epoch 139: loss=0.8288133144378662\n",
      "epoch 140: loss=0.8265661597251892\n",
      "epoch 141: loss=0.82701575756073\n",
      "epoch 142: loss=0.8266612887382507\n",
      "epoch 143: loss=0.8283416032791138\n",
      "epoch 144: loss=0.8272018432617188\n",
      "epoch 145: loss=0.8254167437553406\n",
      "epoch 146: loss=0.8293309807777405\n",
      "epoch 147: loss=0.8270655274391174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 148: loss=0.8271594643592834\n",
      "epoch 149: loss=0.8264183402061462\n",
      "epoch 150: loss=0.8260393738746643\n",
      "epoch 151: loss=0.8250260949134827\n",
      "epoch 152: loss=0.8263924717903137\n",
      "epoch 153: loss=0.8277398943901062\n",
      "epoch 154: loss=0.8263505697250366\n",
      "epoch 155: loss=0.8245023488998413\n",
      "epoch 156: loss=0.8247127532958984\n",
      "epoch 157: loss=0.8254498839378357\n",
      "epoch 158: loss=0.8246276378631592\n",
      "epoch 159: loss=0.8247042894363403\n",
      "epoch 160: loss=0.8237774968147278\n",
      "epoch 161: loss=0.8233165740966797\n",
      "epoch 162: loss=0.8251137733459473\n",
      "epoch 163: loss=0.822791576385498\n",
      "epoch 164: loss=0.8234464526176453\n",
      "epoch 165: loss=0.8237303495407104\n",
      "epoch 166: loss=0.8232677578926086\n",
      "epoch 167: loss=0.8240652680397034\n",
      "epoch 168: loss=0.8227554559707642\n",
      "epoch 169: loss=0.8233563303947449\n",
      "epoch 170: loss=0.8244273066520691\n",
      "epoch 171: loss=0.8214028477668762\n",
      "epoch 172: loss=0.8234838843345642\n",
      "epoch 173: loss=0.8222730755805969\n",
      "epoch 174: loss=0.8199906945228577\n",
      "epoch 175: loss=0.8192248344421387\n",
      "epoch 176: loss=0.8197330832481384\n",
      "epoch 177: loss=0.8210074305534363\n",
      "epoch 178: loss=0.8217713832855225\n",
      "epoch 179: loss=0.8210264444351196\n",
      "epoch 180: loss=0.8195726275444031\n",
      "epoch 181: loss=0.8176829814910889\n",
      "epoch 182: loss=0.8202080726623535\n",
      "epoch 183: loss=0.8186474442481995\n",
      "epoch 184: loss=0.8177129030227661\n",
      "epoch 185: loss=0.8172721266746521\n",
      "epoch 186: loss=0.8169863224029541\n",
      "epoch 187: loss=0.8181536197662354\n",
      "epoch 188: loss=0.8161792159080505\n",
      "epoch 189: loss=0.8172934651374817\n",
      "epoch 190: loss=0.8164512515068054\n",
      "epoch 191: loss=0.8151716589927673\n",
      "epoch 192: loss=0.8165591359138489\n",
      "epoch 193: loss=0.8152653574943542\n",
      "epoch 194: loss=0.8161982893943787\n",
      "epoch 195: loss=0.8138356804847717\n",
      "epoch 196: loss=0.8139782547950745\n",
      "epoch 197: loss=0.8130218386650085\n",
      "epoch 198: loss=0.8134880661964417\n",
      "epoch 199: loss=0.8143554925918579\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=4.667166709899902\n",
      "epoch 1: loss=4.486387729644775\n",
      "epoch 2: loss=4.261362552642822\n",
      "epoch 3: loss=4.002147674560547\n",
      "epoch 4: loss=3.719231605529785\n",
      "epoch 5: loss=3.274561643600464\n",
      "epoch 6: loss=2.926187515258789\n",
      "epoch 7: loss=2.6099562644958496\n",
      "epoch 8: loss=2.455782413482666\n",
      "epoch 9: loss=2.3934829235076904\n",
      "epoch 10: loss=2.312976837158203\n",
      "epoch 11: loss=2.168715715408325\n",
      "epoch 12: loss=2.0164401531219482\n",
      "epoch 13: loss=1.8794937133789062\n",
      "epoch 14: loss=1.7510523796081543\n",
      "epoch 15: loss=1.6398191452026367\n",
      "epoch 16: loss=1.5280638933181763\n",
      "epoch 17: loss=1.470009446144104\n",
      "epoch 18: loss=1.4185880422592163\n",
      "epoch 19: loss=1.387026071548462\n",
      "epoch 20: loss=1.3532766103744507\n",
      "epoch 21: loss=1.3212918043136597\n",
      "epoch 22: loss=1.2678636312484741\n",
      "epoch 23: loss=1.223598599433899\n",
      "epoch 24: loss=1.182641863822937\n",
      "epoch 25: loss=1.1413713693618774\n",
      "epoch 26: loss=1.1133761405944824\n",
      "epoch 27: loss=1.0782897472381592\n",
      "epoch 28: loss=1.0396523475646973\n",
      "epoch 29: loss=1.0109814405441284\n",
      "epoch 30: loss=0.9799923896789551\n",
      "epoch 31: loss=0.9652722477912903\n",
      "epoch 32: loss=0.955324649810791\n",
      "epoch 33: loss=0.9506651759147644\n",
      "epoch 34: loss=0.9478203058242798\n",
      "epoch 35: loss=0.9422380924224854\n",
      "epoch 36: loss=0.9329302310943604\n",
      "epoch 37: loss=0.9264326095581055\n",
      "epoch 38: loss=0.9146648645401001\n",
      "epoch 39: loss=0.9079285264015198\n",
      "epoch 40: loss=0.9036609530448914\n",
      "epoch 41: loss=0.9004495739936829\n",
      "epoch 42: loss=0.8980135917663574\n",
      "epoch 43: loss=0.8955397605895996\n",
      "epoch 44: loss=0.8926888704299927\n",
      "epoch 45: loss=0.8894056677818298\n",
      "epoch 46: loss=0.8891560435295105\n",
      "epoch 47: loss=0.8843037486076355\n",
      "epoch 48: loss=0.8819037675857544\n",
      "epoch 49: loss=0.8808018565177917\n",
      "epoch 50: loss=0.8787885308265686\n",
      "epoch 51: loss=0.8779170513153076\n",
      "epoch 52: loss=0.8756691813468933\n",
      "epoch 53: loss=0.8750844597816467\n",
      "epoch 54: loss=0.873931884765625\n",
      "epoch 55: loss=0.8736379146575928\n",
      "epoch 56: loss=0.872098982334137\n",
      "epoch 57: loss=0.87326979637146\n",
      "epoch 58: loss=0.8702250123023987\n",
      "epoch 59: loss=0.8702945113182068\n",
      "epoch 60: loss=0.8683369755744934\n",
      "epoch 61: loss=0.8665832281112671\n",
      "epoch 62: loss=0.8656384944915771\n",
      "epoch 63: loss=0.866700291633606\n",
      "epoch 64: loss=0.8660632371902466\n",
      "epoch 65: loss=0.8661863207817078\n",
      "epoch 66: loss=0.8655884265899658\n",
      "epoch 67: loss=0.8651710748672485\n",
      "epoch 68: loss=0.8646128177642822\n",
      "epoch 69: loss=0.8646352887153625\n",
      "epoch 70: loss=0.8635279536247253\n",
      "epoch 71: loss=0.8625971078872681\n",
      "epoch 72: loss=0.8630090355873108\n",
      "epoch 73: loss=0.8625637888908386\n",
      "epoch 74: loss=0.8600410223007202\n",
      "epoch 75: loss=0.8614832162857056\n",
      "epoch 76: loss=0.8608119487762451\n",
      "epoch 77: loss=0.860762894153595\n",
      "epoch 78: loss=0.8599696159362793\n",
      "epoch 79: loss=0.8597545623779297\n",
      "epoch 80: loss=0.8594083189964294\n",
      "epoch 81: loss=0.8587033748626709\n",
      "epoch 82: loss=0.8586268424987793\n",
      "epoch 83: loss=0.8587419390678406\n",
      "epoch 84: loss=0.8579902052879333\n",
      "epoch 85: loss=0.8572913408279419\n",
      "epoch 86: loss=0.8578712344169617\n",
      "epoch 87: loss=0.8580716252326965\n",
      "epoch 88: loss=0.8576520085334778\n",
      "epoch 89: loss=0.8570085763931274\n",
      "epoch 90: loss=0.8566454648971558\n",
      "epoch 91: loss=0.8552074432373047\n",
      "epoch 92: loss=0.8551090359687805\n",
      "epoch 93: loss=0.8565998673439026\n",
      "epoch 94: loss=0.8548997044563293\n",
      "epoch 95: loss=0.855387806892395\n",
      "epoch 96: loss=0.8538422584533691\n",
      "epoch 97: loss=0.8534506559371948\n",
      "epoch 98: loss=0.8539668321609497\n",
      "epoch 99: loss=0.8538090586662292\n",
      "epoch 100: loss=0.8513817191123962\n",
      "epoch 101: loss=0.8530390858650208\n",
      "epoch 102: loss=0.8514462113380432\n",
      "epoch 103: loss=0.8518266677856445\n",
      "epoch 104: loss=0.8515422344207764\n",
      "epoch 105: loss=0.8508419990539551\n",
      "epoch 106: loss=0.8499262928962708\n",
      "epoch 107: loss=0.8512354493141174\n",
      "epoch 108: loss=0.850932776927948\n",
      "epoch 109: loss=0.8498026132583618\n",
      "epoch 110: loss=0.8502212166786194\n",
      "epoch 111: loss=0.8499486446380615\n",
      "epoch 112: loss=0.8496659994125366\n",
      "epoch 113: loss=0.8492094278335571\n",
      "epoch 114: loss=0.8498475551605225\n",
      "epoch 115: loss=0.8502171635627747\n",
      "epoch 116: loss=0.84930819272995\n",
      "epoch 117: loss=0.8484801650047302\n",
      "epoch 118: loss=0.847834587097168\n",
      "epoch 119: loss=0.8467959761619568\n",
      "epoch 120: loss=0.847939670085907\n",
      "epoch 121: loss=0.8479437828063965\n",
      "epoch 122: loss=0.8491867184638977\n",
      "epoch 123: loss=0.8470525741577148\n",
      "epoch 124: loss=0.8474313616752625\n",
      "epoch 125: loss=0.8480427265167236\n",
      "epoch 126: loss=0.8471129536628723\n",
      "epoch 127: loss=0.8466335535049438\n",
      "epoch 128: loss=0.8474757671356201\n",
      "epoch 129: loss=0.8464840650558472\n",
      "epoch 130: loss=0.8461549282073975\n",
      "epoch 131: loss=0.8459687232971191\n",
      "epoch 132: loss=0.8455829620361328\n",
      "epoch 133: loss=0.8447398543357849\n",
      "epoch 134: loss=0.8447623252868652\n",
      "epoch 135: loss=0.8446065187454224\n",
      "epoch 136: loss=0.8447501063346863\n",
      "epoch 137: loss=0.8418857455253601\n",
      "epoch 138: loss=0.8436490297317505\n",
      "epoch 139: loss=0.842498242855072\n",
      "epoch 140: loss=0.8424085974693298\n",
      "epoch 141: loss=0.8436247706413269\n",
      "epoch 142: loss=0.8421257138252258\n",
      "epoch 143: loss=0.8431764245033264\n",
      "epoch 144: loss=0.8409351110458374\n",
      "epoch 145: loss=0.8415178656578064\n",
      "epoch 146: loss=0.8396292328834534\n",
      "epoch 147: loss=0.8408515453338623\n",
      "epoch 148: loss=0.8421724438667297\n",
      "epoch 149: loss=0.839031994342804\n",
      "epoch 150: loss=0.8398594856262207\n",
      "epoch 151: loss=0.8396748304367065\n",
      "epoch 152: loss=0.8390399217605591\n",
      "epoch 153: loss=0.8395305275917053\n",
      "epoch 154: loss=0.8382757902145386\n",
      "epoch 155: loss=0.837873101234436\n",
      "epoch 156: loss=0.8389695882797241\n",
      "epoch 157: loss=0.8371031284332275\n",
      "epoch 158: loss=0.8384014368057251\n",
      "epoch 159: loss=0.8374696969985962\n",
      "epoch 160: loss=0.8370466232299805\n",
      "epoch 161: loss=0.8361295461654663\n",
      "epoch 162: loss=0.8358469605445862\n",
      "epoch 163: loss=0.8349573612213135\n",
      "epoch 164: loss=0.8354191184043884\n",
      "epoch 165: loss=0.8343215584754944\n",
      "epoch 166: loss=0.8350953459739685\n",
      "epoch 167: loss=0.8337002992630005\n",
      "epoch 168: loss=0.833307147026062\n",
      "epoch 169: loss=0.8341385126113892\n",
      "epoch 170: loss=0.8331311345100403\n",
      "epoch 171: loss=0.8336036801338196\n",
      "epoch 172: loss=0.8315689563751221\n",
      "epoch 173: loss=0.8324331641197205\n",
      "epoch 174: loss=0.8311859369277954\n",
      "epoch 175: loss=0.8312833905220032\n",
      "epoch 176: loss=0.8320303559303284\n",
      "epoch 177: loss=0.8302150964736938\n",
      "epoch 178: loss=0.8304899334907532\n",
      "epoch 179: loss=0.8297990560531616\n",
      "epoch 180: loss=0.8300842642784119\n",
      "epoch 181: loss=0.8279812932014465\n",
      "epoch 182: loss=0.829369843006134\n",
      "epoch 183: loss=0.8295624852180481\n",
      "epoch 184: loss=0.8274720311164856\n",
      "epoch 185: loss=0.8280358910560608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 186: loss=0.8254252076148987\n",
      "epoch 187: loss=0.8259677886962891\n",
      "epoch 188: loss=0.8259154558181763\n",
      "epoch 189: loss=0.8260868191719055\n",
      "epoch 190: loss=0.8270679116249084\n",
      "epoch 191: loss=0.8257479667663574\n",
      "epoch 192: loss=0.8238871693611145\n",
      "epoch 193: loss=0.82429438829422\n",
      "epoch 194: loss=0.8242138028144836\n",
      "epoch 195: loss=0.8227076530456543\n",
      "epoch 196: loss=0.8228635191917419\n",
      "epoch 197: loss=0.8228833079338074\n",
      "epoch 198: loss=0.824042558670044\n",
      "epoch 199: loss=0.8236658573150635\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=4.65402889251709\n",
      "epoch 1: loss=4.4261474609375\n",
      "epoch 2: loss=4.151549339294434\n",
      "epoch 3: loss=3.8165924549102783\n",
      "epoch 4: loss=3.3386099338531494\n",
      "epoch 5: loss=2.9759891033172607\n",
      "epoch 6: loss=2.6109395027160645\n",
      "epoch 7: loss=2.431314706802368\n",
      "epoch 8: loss=2.364388942718506\n",
      "epoch 9: loss=2.27908992767334\n",
      "epoch 10: loss=2.155170440673828\n",
      "epoch 11: loss=2.0248236656188965\n",
      "epoch 12: loss=1.8308265209197998\n",
      "epoch 13: loss=1.6809091567993164\n",
      "epoch 14: loss=1.566015601158142\n",
      "epoch 15: loss=1.4747377634048462\n",
      "epoch 16: loss=1.4293047189712524\n",
      "epoch 17: loss=1.404878854751587\n",
      "epoch 18: loss=1.3826448917388916\n",
      "epoch 19: loss=1.355922818183899\n",
      "epoch 20: loss=1.3134492635726929\n",
      "epoch 21: loss=1.2851790189743042\n",
      "epoch 22: loss=1.2367494106292725\n",
      "epoch 23: loss=1.201600432395935\n",
      "epoch 24: loss=1.1788973808288574\n",
      "epoch 25: loss=1.1475313901901245\n",
      "epoch 26: loss=1.1096333265304565\n",
      "epoch 27: loss=1.0798083543777466\n",
      "epoch 28: loss=1.0511823892593384\n",
      "epoch 29: loss=1.0265918970108032\n",
      "epoch 30: loss=1.0085420608520508\n",
      "epoch 31: loss=0.9955368041992188\n",
      "epoch 32: loss=0.9857649803161621\n",
      "epoch 33: loss=0.98210209608078\n",
      "epoch 34: loss=0.9758836030960083\n",
      "epoch 35: loss=0.9674199223518372\n",
      "epoch 36: loss=0.9576860666275024\n",
      "epoch 37: loss=0.9502115249633789\n",
      "epoch 38: loss=0.9403157234191895\n",
      "epoch 39: loss=0.9307832717895508\n",
      "epoch 40: loss=0.9245254397392273\n",
      "epoch 41: loss=0.9185627102851868\n",
      "epoch 42: loss=0.915755033493042\n",
      "epoch 43: loss=0.9126575589179993\n",
      "epoch 44: loss=0.9102779030799866\n",
      "epoch 45: loss=0.9080051779747009\n",
      "epoch 46: loss=0.9056721925735474\n",
      "epoch 47: loss=0.9052098393440247\n",
      "epoch 48: loss=0.9036627411842346\n",
      "epoch 49: loss=0.8996154069900513\n",
      "epoch 50: loss=0.8968197107315063\n",
      "epoch 51: loss=0.8960369825363159\n",
      "epoch 52: loss=0.8950573801994324\n",
      "epoch 53: loss=0.8940955996513367\n",
      "epoch 54: loss=0.8930107355117798\n",
      "epoch 55: loss=0.8894758820533752\n",
      "epoch 56: loss=0.8904377818107605\n",
      "epoch 57: loss=0.8882138133049011\n",
      "epoch 58: loss=0.8883623480796814\n",
      "epoch 59: loss=0.8875644207000732\n",
      "epoch 60: loss=0.8862170577049255\n",
      "epoch 61: loss=0.8877097368240356\n",
      "epoch 62: loss=0.8844209909439087\n",
      "epoch 63: loss=0.8849914073944092\n",
      "epoch 64: loss=0.8834621906280518\n",
      "epoch 65: loss=0.8822665214538574\n",
      "epoch 66: loss=0.8815416693687439\n",
      "epoch 67: loss=0.8815035820007324\n",
      "epoch 68: loss=0.8814458250999451\n",
      "epoch 69: loss=0.8802461624145508\n",
      "epoch 70: loss=0.8802518844604492\n",
      "epoch 71: loss=0.8777572512626648\n",
      "epoch 72: loss=0.8793519139289856\n",
      "epoch 73: loss=0.8785311579704285\n",
      "epoch 74: loss=0.8784471154212952\n",
      "epoch 75: loss=0.8779937624931335\n",
      "epoch 76: loss=0.8753792643547058\n",
      "epoch 77: loss=0.8747311234474182\n",
      "epoch 78: loss=0.8758379817008972\n",
      "epoch 79: loss=0.8730775713920593\n",
      "epoch 80: loss=0.8744218945503235\n",
      "epoch 81: loss=0.8748810887336731\n",
      "epoch 82: loss=0.8732443451881409\n",
      "epoch 83: loss=0.8744742274284363\n",
      "epoch 84: loss=0.8739903569221497\n",
      "epoch 85: loss=0.8724533319473267\n",
      "epoch 86: loss=0.8734715580940247\n",
      "epoch 87: loss=0.8728620409965515\n",
      "epoch 88: loss=0.8710663318634033\n",
      "epoch 89: loss=0.8723989725112915\n",
      "epoch 90: loss=0.8702670931816101\n",
      "epoch 91: loss=0.8714329600334167\n",
      "epoch 92: loss=0.8709739446640015\n",
      "epoch 93: loss=0.8693562746047974\n",
      "epoch 94: loss=0.8683492541313171\n",
      "epoch 95: loss=0.8688416481018066\n",
      "epoch 96: loss=0.8694403767585754\n",
      "epoch 97: loss=0.8695821166038513\n",
      "epoch 98: loss=0.8688643574714661\n",
      "epoch 99: loss=0.8694809675216675\n",
      "epoch 100: loss=0.8710727095603943\n",
      "epoch 101: loss=0.8678281903266907\n",
      "epoch 102: loss=0.8667567372322083\n",
      "epoch 103: loss=0.8675053715705872\n",
      "epoch 104: loss=0.8669119477272034\n",
      "epoch 105: loss=0.8652768135070801\n",
      "epoch 106: loss=0.8658412098884583\n",
      "epoch 107: loss=0.8650355935096741\n",
      "epoch 108: loss=0.864545464515686\n",
      "epoch 109: loss=0.8648881316184998\n",
      "epoch 110: loss=0.8629679083824158\n",
      "epoch 111: loss=0.8654850721359253\n",
      "epoch 112: loss=0.864844560623169\n",
      "epoch 113: loss=0.8664238452911377\n",
      "epoch 114: loss=0.8659554719924927\n",
      "epoch 115: loss=0.8634856343269348\n",
      "epoch 116: loss=0.8652471899986267\n",
      "epoch 117: loss=0.8641545176506042\n",
      "epoch 118: loss=0.8623865842819214\n",
      "epoch 119: loss=0.8629726767539978\n",
      "epoch 120: loss=0.8641688823699951\n",
      "epoch 121: loss=0.8622370958328247\n",
      "epoch 122: loss=0.8628455996513367\n",
      "epoch 123: loss=0.8615906238555908\n",
      "epoch 124: loss=0.8636626601219177\n",
      "epoch 125: loss=0.8622753024101257\n",
      "epoch 126: loss=0.8633524179458618\n",
      "epoch 127: loss=0.8599148392677307\n",
      "epoch 128: loss=0.8612852692604065\n",
      "epoch 129: loss=0.861595630645752\n",
      "epoch 130: loss=0.8618966937065125\n",
      "epoch 131: loss=0.861003577709198\n",
      "epoch 132: loss=0.8611140847206116\n",
      "epoch 133: loss=0.8600331544876099\n",
      "epoch 134: loss=0.8603745698928833\n",
      "epoch 135: loss=0.8599568605422974\n",
      "epoch 136: loss=0.8601507544517517\n",
      "epoch 137: loss=0.8612338900566101\n",
      "epoch 138: loss=0.8600836992263794\n",
      "epoch 139: loss=0.8590207099914551\n",
      "epoch 140: loss=0.8585154414176941\n",
      "epoch 141: loss=0.8585385680198669\n",
      "epoch 142: loss=0.8586651682853699\n",
      "epoch 143: loss=0.8599970936775208\n",
      "epoch 144: loss=0.859072208404541\n",
      "epoch 145: loss=0.8579296469688416\n",
      "epoch 146: loss=0.8585188388824463\n",
      "epoch 147: loss=0.8606840968132019\n",
      "epoch 148: loss=0.8586423397064209\n",
      "epoch 149: loss=0.8608992099761963\n",
      "epoch 150: loss=0.858544111251831\n",
      "epoch 151: loss=0.8576682209968567\n",
      "epoch 152: loss=0.8577269315719604\n",
      "epoch 153: loss=0.8575338125228882\n",
      "epoch 154: loss=0.8587358593940735\n",
      "epoch 155: loss=0.8577465415000916\n",
      "epoch 156: loss=0.8591540455818176\n",
      "epoch 157: loss=0.8581112027168274\n",
      "epoch 158: loss=0.857388436794281\n",
      "epoch 159: loss=0.8570424914360046\n",
      "epoch 160: loss=0.855718195438385\n",
      "epoch 161: loss=0.8565902709960938\n",
      "epoch 162: loss=0.8558131456375122\n",
      "epoch 163: loss=0.856338381767273\n",
      "epoch 164: loss=0.855512797832489\n",
      "epoch 165: loss=0.8553321957588196\n",
      "epoch 166: loss=0.8544356226921082\n",
      "epoch 167: loss=0.8548555970191956\n",
      "epoch 168: loss=0.8553406596183777\n",
      "epoch 169: loss=0.8554812669754028\n",
      "epoch 170: loss=0.8557292222976685\n",
      "epoch 171: loss=0.8538563847541809\n",
      "epoch 172: loss=0.8535897731781006\n",
      "epoch 173: loss=0.8547803163528442\n",
      "epoch 174: loss=0.852827250957489\n",
      "epoch 175: loss=0.8535905480384827\n",
      "epoch 176: loss=0.8534536957740784\n",
      "epoch 177: loss=0.8516515493392944\n",
      "epoch 178: loss=0.8521032333374023\n",
      "epoch 179: loss=0.8523234724998474\n",
      "epoch 180: loss=0.8519173860549927\n",
      "epoch 181: loss=0.8518885374069214\n",
      "epoch 182: loss=0.8518742322921753\n",
      "epoch 183: loss=0.851040244102478\n",
      "epoch 184: loss=0.8503206968307495\n",
      "epoch 185: loss=0.8489663600921631\n",
      "epoch 186: loss=0.8521336317062378\n",
      "epoch 187: loss=0.8497563004493713\n",
      "epoch 188: loss=0.8489201068878174\n",
      "epoch 189: loss=0.8477730751037598\n",
      "epoch 190: loss=0.8505172729492188\n",
      "epoch 191: loss=0.8508378863334656\n",
      "epoch 192: loss=0.8482450246810913\n",
      "epoch 193: loss=0.8494272828102112\n",
      "epoch 194: loss=0.8471203446388245\n",
      "epoch 195: loss=0.8481470942497253\n",
      "epoch 196: loss=0.8474025130271912\n",
      "epoch 197: loss=0.8469460010528564\n",
      "epoch 198: loss=0.8491494059562683\n",
      "epoch 199: loss=0.8483887910842896\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=4.694171905517578\n",
      "epoch 1: loss=4.45747709274292\n",
      "epoch 2: loss=4.355207920074463\n",
      "epoch 3: loss=4.065025806427002\n",
      "epoch 4: loss=3.8392724990844727\n",
      "epoch 5: loss=3.46238112449646\n",
      "epoch 6: loss=3.159897804260254\n",
      "epoch 7: loss=2.896080493927002\n",
      "epoch 8: loss=2.6508877277374268\n",
      "epoch 9: loss=2.5201051235198975\n",
      "epoch 10: loss=2.459435224533081\n",
      "epoch 11: loss=2.366084575653076\n",
      "epoch 12: loss=2.2522196769714355\n",
      "epoch 13: loss=2.10734224319458\n",
      "epoch 14: loss=1.957576870918274\n",
      "epoch 15: loss=1.8049991130828857\n",
      "epoch 16: loss=1.6880720853805542\n",
      "epoch 17: loss=1.5982017517089844\n",
      "epoch 18: loss=1.5241703987121582\n",
      "epoch 19: loss=1.4945130348205566\n",
      "epoch 20: loss=1.4727299213409424\n",
      "epoch 21: loss=1.4416968822479248\n",
      "epoch 22: loss=1.395768642425537\n",
      "epoch 23: loss=1.3650321960449219\n",
      "epoch 24: loss=1.3162868022918701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: loss=1.2706561088562012\n",
      "epoch 26: loss=1.2162458896636963\n",
      "epoch 27: loss=1.1770195960998535\n",
      "epoch 28: loss=1.1439449787139893\n",
      "epoch 29: loss=1.1248294115066528\n",
      "epoch 30: loss=1.1040019989013672\n",
      "epoch 31: loss=1.0798465013504028\n",
      "epoch 32: loss=1.052608609199524\n",
      "epoch 33: loss=1.0327781438827515\n",
      "epoch 34: loss=1.0189622640609741\n",
      "epoch 35: loss=1.002929449081421\n",
      "epoch 36: loss=0.996931254863739\n",
      "epoch 37: loss=0.9929008483886719\n",
      "epoch 38: loss=0.9898833632469177\n",
      "epoch 39: loss=0.986458957195282\n",
      "epoch 40: loss=0.9761772751808167\n",
      "epoch 41: loss=0.9640887379646301\n",
      "epoch 42: loss=0.9511709213256836\n",
      "epoch 43: loss=0.950112521648407\n",
      "epoch 44: loss=0.9409446716308594\n",
      "epoch 45: loss=0.9395558834075928\n",
      "epoch 46: loss=0.9359796643257141\n",
      "epoch 47: loss=0.9319838285446167\n",
      "epoch 48: loss=0.935297429561615\n",
      "epoch 49: loss=0.9275227189064026\n",
      "epoch 50: loss=0.9289035201072693\n",
      "epoch 51: loss=0.9303858280181885\n",
      "epoch 52: loss=0.9224831461906433\n",
      "epoch 53: loss=0.9213101267814636\n",
      "epoch 54: loss=0.9167459011077881\n",
      "epoch 55: loss=0.9167674779891968\n",
      "epoch 56: loss=0.915657639503479\n",
      "epoch 57: loss=0.9158321022987366\n",
      "epoch 58: loss=0.9151854515075684\n",
      "epoch 59: loss=0.9151513576507568\n",
      "epoch 60: loss=0.909417986869812\n",
      "epoch 61: loss=0.9127848148345947\n",
      "epoch 62: loss=0.9091230630874634\n",
      "epoch 63: loss=0.9083635210990906\n",
      "epoch 64: loss=0.9082297086715698\n",
      "epoch 65: loss=0.9083778262138367\n",
      "epoch 66: loss=0.9063056111335754\n",
      "epoch 67: loss=0.9077293872833252\n",
      "epoch 68: loss=0.9061416983604431\n",
      "epoch 69: loss=0.9033752679824829\n",
      "epoch 70: loss=0.9040125608444214\n",
      "epoch 71: loss=0.9040054082870483\n",
      "epoch 72: loss=0.9044246673583984\n",
      "epoch 73: loss=0.9031672477722168\n",
      "epoch 74: loss=0.9026511311531067\n",
      "epoch 75: loss=0.9021127820014954\n",
      "epoch 76: loss=0.901625394821167\n",
      "epoch 77: loss=0.9013055562973022\n",
      "epoch 78: loss=0.9018938541412354\n",
      "epoch 79: loss=0.9018219709396362\n",
      "epoch 80: loss=0.9006893634796143\n",
      "epoch 81: loss=0.9019120335578918\n",
      "epoch 82: loss=0.8997969627380371\n",
      "epoch 83: loss=0.8995627760887146\n",
      "epoch 84: loss=0.9010298848152161\n",
      "epoch 85: loss=0.90013188123703\n",
      "epoch 86: loss=0.8977639675140381\n",
      "epoch 87: loss=0.8986486792564392\n",
      "epoch 88: loss=0.8954693675041199\n",
      "epoch 89: loss=0.8975711464881897\n",
      "epoch 90: loss=0.8974857330322266\n",
      "epoch 91: loss=0.896267831325531\n",
      "epoch 92: loss=0.8980172872543335\n",
      "epoch 93: loss=0.8957704305648804\n",
      "epoch 94: loss=0.8960528373718262\n",
      "epoch 95: loss=0.8960546255111694\n",
      "epoch 96: loss=0.8956632614135742\n",
      "epoch 97: loss=0.8946677446365356\n",
      "epoch 98: loss=0.8916712403297424\n",
      "epoch 99: loss=0.8963247537612915\n",
      "epoch 100: loss=0.8921037316322327\n",
      "epoch 101: loss=0.8926743865013123\n",
      "epoch 102: loss=0.8937541842460632\n",
      "epoch 103: loss=0.8940757513046265\n",
      "epoch 104: loss=0.8903167247772217\n",
      "epoch 105: loss=0.8931373953819275\n",
      "epoch 106: loss=0.8906291127204895\n",
      "epoch 107: loss=0.8900913000106812\n",
      "epoch 108: loss=0.8911616802215576\n",
      "epoch 109: loss=0.8903713226318359\n",
      "epoch 110: loss=0.8907710313796997\n",
      "epoch 111: loss=0.889805793762207\n",
      "epoch 112: loss=0.888273298740387\n",
      "epoch 113: loss=0.8887581825256348\n",
      "epoch 114: loss=0.887488842010498\n",
      "epoch 115: loss=0.8876617550849915\n",
      "epoch 116: loss=0.8906325697898865\n",
      "epoch 117: loss=0.8878328800201416\n",
      "epoch 118: loss=0.8884264826774597\n",
      "epoch 119: loss=0.8881670236587524\n",
      "epoch 120: loss=0.8866556286811829\n",
      "epoch 121: loss=0.8873580098152161\n",
      "epoch 122: loss=0.8871170282363892\n",
      "epoch 123: loss=0.8843852281570435\n",
      "epoch 124: loss=0.886249840259552\n",
      "epoch 125: loss=0.8844970464706421\n",
      "epoch 126: loss=0.8837684392929077\n",
      "epoch 127: loss=0.8840867877006531\n",
      "epoch 128: loss=0.8843191862106323\n",
      "epoch 129: loss=0.8857306241989136\n",
      "epoch 130: loss=0.8833897709846497\n",
      "epoch 131: loss=0.8816289901733398\n",
      "epoch 132: loss=0.8826345801353455\n",
      "epoch 133: loss=0.8827347159385681\n",
      "epoch 134: loss=0.8817766308784485\n",
      "epoch 135: loss=0.8808931708335876\n",
      "epoch 136: loss=0.8818655610084534\n",
      "epoch 137: loss=0.8815865516662598\n",
      "epoch 138: loss=0.8810076713562012\n",
      "epoch 139: loss=0.8806666731834412\n",
      "epoch 140: loss=0.8807938694953918\n",
      "epoch 141: loss=0.8781936168670654\n",
      "epoch 142: loss=0.8784764409065247\n",
      "epoch 143: loss=0.8773500323295593\n",
      "epoch 144: loss=0.8780821561813354\n",
      "epoch 145: loss=0.8764107823371887\n",
      "epoch 146: loss=0.8782138228416443\n",
      "epoch 147: loss=0.8763278722763062\n",
      "epoch 148: loss=0.8780820965766907\n",
      "epoch 149: loss=0.8770548701286316\n",
      "epoch 150: loss=0.8764692544937134\n",
      "epoch 151: loss=0.8767507672309875\n",
      "epoch 152: loss=0.8750095367431641\n",
      "epoch 153: loss=0.8752368092536926\n",
      "epoch 154: loss=0.8743885159492493\n",
      "epoch 155: loss=0.8755179643630981\n",
      "epoch 156: loss=0.877813458442688\n",
      "epoch 157: loss=0.8742955327033997\n",
      "epoch 158: loss=0.8743946552276611\n",
      "epoch 159: loss=0.8726640939712524\n",
      "epoch 160: loss=0.875800609588623\n",
      "epoch 161: loss=0.8718334436416626\n",
      "epoch 162: loss=0.8708602786064148\n",
      "epoch 163: loss=0.8735811114311218\n",
      "epoch 164: loss=0.8706921935081482\n",
      "epoch 165: loss=0.8725960850715637\n",
      "epoch 166: loss=0.870694637298584\n",
      "epoch 167: loss=0.871179461479187\n",
      "epoch 168: loss=0.8715496063232422\n",
      "epoch 169: loss=0.8690881729125977\n",
      "epoch 170: loss=0.868211030960083\n",
      "epoch 171: loss=0.868488609790802\n",
      "epoch 172: loss=0.8673202991485596\n",
      "epoch 173: loss=0.8664914965629578\n",
      "epoch 174: loss=0.867401659488678\n",
      "epoch 175: loss=0.869071900844574\n",
      "epoch 176: loss=0.8660328388214111\n",
      "epoch 177: loss=0.8644475340843201\n",
      "epoch 178: loss=0.8653558492660522\n",
      "epoch 179: loss=0.8647754788398743\n",
      "epoch 180: loss=0.8626142740249634\n",
      "epoch 181: loss=0.8643654584884644\n",
      "epoch 182: loss=0.8644090294837952\n",
      "epoch 183: loss=0.8626804351806641\n",
      "epoch 184: loss=0.8632225394248962\n",
      "epoch 185: loss=0.8628454804420471\n",
      "epoch 186: loss=0.8633702993392944\n",
      "epoch 187: loss=0.8646730184555054\n",
      "epoch 188: loss=0.862483024597168\n",
      "epoch 189: loss=0.8617005348205566\n",
      "epoch 190: loss=0.8600040674209595\n",
      "epoch 191: loss=0.8594547510147095\n",
      "epoch 192: loss=0.8619189262390137\n",
      "epoch 193: loss=0.8593665957450867\n",
      "epoch 194: loss=0.860578179359436\n",
      "epoch 195: loss=0.8592272400856018\n",
      "epoch 196: loss=0.8585280179977417\n",
      "epoch 197: loss=0.8607884645462036\n",
      "epoch 198: loss=0.8597933053970337\n",
      "epoch 199: loss=0.8603063225746155\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=4.664251327514648\n",
      "epoch 1: loss=4.4101033210754395\n",
      "epoch 2: loss=4.171355247497559\n",
      "epoch 3: loss=3.8416876792907715\n",
      "epoch 4: loss=3.46159029006958\n",
      "epoch 5: loss=3.0598642826080322\n",
      "epoch 6: loss=2.654057741165161\n",
      "epoch 7: loss=2.37435245513916\n",
      "epoch 8: loss=2.29459285736084\n",
      "epoch 9: loss=2.2093617916107178\n",
      "epoch 10: loss=2.140653133392334\n",
      "epoch 11: loss=1.9762675762176514\n",
      "epoch 12: loss=1.8221760988235474\n",
      "epoch 13: loss=1.6631028652191162\n",
      "epoch 14: loss=1.5322563648223877\n",
      "epoch 15: loss=1.4419584274291992\n",
      "epoch 16: loss=1.3667998313903809\n",
      "epoch 17: loss=1.3289308547973633\n",
      "epoch 18: loss=1.3113280534744263\n",
      "epoch 19: loss=1.2847375869750977\n",
      "epoch 20: loss=1.2663863897323608\n",
      "epoch 21: loss=1.2189565896987915\n",
      "epoch 22: loss=1.1901381015777588\n",
      "epoch 23: loss=1.15909743309021\n",
      "epoch 24: loss=1.1277015209197998\n",
      "epoch 25: loss=1.0904381275177002\n",
      "epoch 26: loss=1.051877498626709\n",
      "epoch 27: loss=1.022835373878479\n",
      "epoch 28: loss=1.0030806064605713\n",
      "epoch 29: loss=0.9881899356842041\n",
      "epoch 30: loss=0.9793333411216736\n",
      "epoch 31: loss=0.9786692261695862\n",
      "epoch 32: loss=0.9737613201141357\n",
      "epoch 33: loss=0.9731370806694031\n",
      "epoch 34: loss=0.9678032994270325\n",
      "epoch 35: loss=0.9592165946960449\n",
      "epoch 36: loss=0.9515170454978943\n",
      "epoch 37: loss=0.946543276309967\n",
      "epoch 38: loss=0.9411857724189758\n",
      "epoch 39: loss=0.9345852136611938\n",
      "epoch 40: loss=0.9320369958877563\n",
      "epoch 41: loss=0.927801251411438\n",
      "epoch 42: loss=0.926456093788147\n",
      "epoch 43: loss=0.9198424220085144\n",
      "epoch 44: loss=0.9168991446495056\n",
      "epoch 45: loss=0.9153363704681396\n",
      "epoch 46: loss=0.9129465222358704\n",
      "epoch 47: loss=0.9132019877433777\n",
      "epoch 48: loss=0.9135026931762695\n",
      "epoch 49: loss=0.9107807874679565\n",
      "epoch 50: loss=0.9114696383476257\n",
      "epoch 51: loss=0.9067997932434082\n",
      "epoch 52: loss=0.9076834321022034\n",
      "epoch 53: loss=0.9081360697746277\n",
      "epoch 54: loss=0.9050533771514893\n",
      "epoch 55: loss=0.9027885794639587\n",
      "epoch 56: loss=0.9035394191741943\n",
      "epoch 57: loss=0.9017313122749329\n",
      "epoch 58: loss=0.9021262526512146\n",
      "epoch 59: loss=0.905226469039917\n",
      "epoch 60: loss=0.8983885049819946\n",
      "epoch 61: loss=0.8995326161384583\n",
      "epoch 62: loss=0.8957809805870056\n",
      "epoch 63: loss=0.8983590006828308\n",
      "epoch 64: loss=0.8947464227676392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65: loss=0.8973097205162048\n",
      "epoch 66: loss=0.8967315554618835\n",
      "epoch 67: loss=0.8959197998046875\n",
      "epoch 68: loss=0.8972015380859375\n",
      "epoch 69: loss=0.8919994831085205\n",
      "epoch 70: loss=0.89507657289505\n",
      "epoch 71: loss=0.8940085172653198\n",
      "epoch 72: loss=0.8931328058242798\n",
      "epoch 73: loss=0.8919690847396851\n",
      "epoch 74: loss=0.8935226202011108\n",
      "epoch 75: loss=0.8912474513053894\n",
      "epoch 76: loss=0.8912582993507385\n",
      "epoch 77: loss=0.8901544213294983\n",
      "epoch 78: loss=0.890009880065918\n",
      "epoch 79: loss=0.8886979818344116\n",
      "epoch 80: loss=0.8879858255386353\n",
      "epoch 81: loss=0.8890961408615112\n",
      "epoch 82: loss=0.8887976408004761\n",
      "epoch 83: loss=0.8885986804962158\n",
      "epoch 84: loss=0.8884810209274292\n",
      "epoch 85: loss=0.8873860239982605\n",
      "epoch 86: loss=0.8882125616073608\n",
      "epoch 87: loss=0.8870651125907898\n",
      "epoch 88: loss=0.8869566321372986\n",
      "epoch 89: loss=0.8849655389785767\n",
      "epoch 90: loss=0.8861166834831238\n",
      "epoch 91: loss=0.8855429887771606\n",
      "epoch 92: loss=0.8827834725379944\n",
      "epoch 93: loss=0.8850646615028381\n",
      "epoch 94: loss=0.8849615454673767\n",
      "epoch 95: loss=0.8830623030662537\n",
      "epoch 96: loss=0.8841803073883057\n",
      "epoch 97: loss=0.8832495808601379\n",
      "epoch 98: loss=0.8832645416259766\n",
      "epoch 99: loss=0.8830375075340271\n",
      "epoch 100: loss=0.8842450976371765\n",
      "epoch 101: loss=0.8845624923706055\n",
      "epoch 102: loss=0.8835012912750244\n",
      "epoch 103: loss=0.8813647031784058\n",
      "epoch 104: loss=0.8832263946533203\n",
      "epoch 105: loss=0.8825884461402893\n",
      "epoch 106: loss=0.8810814619064331\n",
      "epoch 107: loss=0.8806644678115845\n",
      "epoch 108: loss=0.8814080953598022\n",
      "epoch 109: loss=0.8801489472389221\n",
      "epoch 110: loss=0.8815523386001587\n",
      "epoch 111: loss=0.880422055721283\n",
      "epoch 112: loss=0.8814854025840759\n",
      "epoch 113: loss=0.8793453574180603\n",
      "epoch 114: loss=0.8795099854469299\n",
      "epoch 115: loss=0.8785822987556458\n",
      "epoch 116: loss=0.8799006938934326\n",
      "epoch 117: loss=0.8788806796073914\n",
      "epoch 118: loss=0.879263162612915\n",
      "epoch 119: loss=0.8776131868362427\n",
      "epoch 120: loss=0.8789180517196655\n",
      "epoch 121: loss=0.8768892884254456\n",
      "epoch 122: loss=0.8770906329154968\n",
      "epoch 123: loss=0.8770135045051575\n",
      "epoch 124: loss=0.8783308863639832\n",
      "epoch 125: loss=0.8788014054298401\n",
      "epoch 126: loss=0.8787049055099487\n",
      "epoch 127: loss=0.8770336508750916\n",
      "epoch 128: loss=0.8754794001579285\n",
      "epoch 129: loss=0.876986563205719\n",
      "epoch 130: loss=0.8779499530792236\n",
      "epoch 131: loss=0.8778678178787231\n",
      "epoch 132: loss=0.8759493827819824\n",
      "epoch 133: loss=0.8775455951690674\n",
      "epoch 134: loss=0.8747687339782715\n",
      "epoch 135: loss=0.8755267262458801\n",
      "epoch 136: loss=0.8750124573707581\n",
      "epoch 137: loss=0.8755071759223938\n",
      "epoch 138: loss=0.874544620513916\n",
      "epoch 139: loss=0.8752544522285461\n",
      "epoch 140: loss=0.8752990365028381\n",
      "epoch 141: loss=0.8738210201263428\n",
      "epoch 142: loss=0.8740602731704712\n",
      "epoch 143: loss=0.8735077977180481\n",
      "epoch 144: loss=0.873298704624176\n",
      "epoch 145: loss=0.8710841536521912\n",
      "epoch 146: loss=0.8744521737098694\n",
      "epoch 147: loss=0.8732247948646545\n",
      "epoch 148: loss=0.8727009296417236\n",
      "epoch 149: loss=0.8710335493087769\n",
      "epoch 150: loss=0.8739776015281677\n",
      "epoch 151: loss=0.8701750040054321\n",
      "epoch 152: loss=0.8724036812782288\n",
      "epoch 153: loss=0.8711487054824829\n",
      "epoch 154: loss=0.8705121874809265\n",
      "epoch 155: loss=0.8676500916481018\n",
      "epoch 156: loss=0.8711628317832947\n",
      "epoch 157: loss=0.86845862865448\n",
      "epoch 158: loss=0.8695001602172852\n",
      "epoch 159: loss=0.867117702960968\n",
      "epoch 160: loss=0.8677049279212952\n",
      "epoch 161: loss=0.8673253059387207\n",
      "epoch 162: loss=0.8680261373519897\n",
      "epoch 163: loss=0.8669754862785339\n",
      "epoch 164: loss=0.8676117658615112\n",
      "epoch 165: loss=0.8651739954948425\n",
      "epoch 166: loss=0.865662693977356\n",
      "epoch 167: loss=0.8661558628082275\n",
      "epoch 168: loss=0.8660196661949158\n",
      "epoch 169: loss=0.8642417192459106\n",
      "epoch 170: loss=0.8652186989784241\n",
      "epoch 171: loss=0.8626288771629333\n",
      "epoch 172: loss=0.8627408742904663\n",
      "epoch 173: loss=0.8643542528152466\n",
      "epoch 174: loss=0.8638526201248169\n",
      "epoch 175: loss=0.862191915512085\n",
      "epoch 176: loss=0.8622516393661499\n",
      "epoch 177: loss=0.8623586297035217\n",
      "epoch 178: loss=0.8596312403678894\n",
      "epoch 179: loss=0.8642596006393433\n",
      "epoch 180: loss=0.8608839511871338\n",
      "epoch 181: loss=0.8608105778694153\n",
      "epoch 182: loss=0.8619238138198853\n",
      "epoch 183: loss=0.8596851229667664\n",
      "epoch 184: loss=0.8590357303619385\n",
      "epoch 185: loss=0.8607645034790039\n",
      "epoch 186: loss=0.8604748845100403\n",
      "epoch 187: loss=0.859514057636261\n",
      "epoch 188: loss=0.8583624362945557\n",
      "epoch 189: loss=0.8572725057601929\n",
      "epoch 190: loss=0.8586646318435669\n",
      "epoch 191: loss=0.8583325147628784\n",
      "epoch 192: loss=0.8569679260253906\n",
      "epoch 193: loss=0.8555105328559875\n",
      "epoch 194: loss=0.8549293279647827\n",
      "epoch 195: loss=0.8532129526138306\n",
      "epoch 196: loss=0.8533943295478821\n",
      "epoch 197: loss=0.8529300093650818\n",
      "epoch 198: loss=0.8563321828842163\n",
      "epoch 199: loss=0.8542044162750244\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=4.6569085121154785\n",
      "epoch 1: loss=4.549119472503662\n",
      "epoch 2: loss=4.28713321685791\n",
      "epoch 3: loss=3.9867517948150635\n",
      "epoch 4: loss=3.673524856567383\n",
      "epoch 5: loss=3.339123249053955\n",
      "epoch 6: loss=2.9751951694488525\n",
      "epoch 7: loss=2.713298797607422\n",
      "epoch 8: loss=2.5869061946868896\n",
      "epoch 9: loss=2.504054307937622\n",
      "epoch 10: loss=2.3994007110595703\n",
      "epoch 11: loss=2.210052967071533\n",
      "epoch 12: loss=2.067023992538452\n",
      "epoch 13: loss=1.8753581047058105\n",
      "epoch 14: loss=1.7419172525405884\n",
      "epoch 15: loss=1.632814645767212\n",
      "epoch 16: loss=1.5632835626602173\n",
      "epoch 17: loss=1.508887767791748\n",
      "epoch 18: loss=1.4733960628509521\n",
      "epoch 19: loss=1.4538360834121704\n",
      "epoch 20: loss=1.4049423933029175\n",
      "epoch 21: loss=1.3492101430892944\n",
      "epoch 22: loss=1.2912635803222656\n",
      "epoch 23: loss=1.2196316719055176\n",
      "epoch 24: loss=1.1717272996902466\n",
      "epoch 25: loss=1.1330718994140625\n",
      "epoch 26: loss=1.101185917854309\n",
      "epoch 27: loss=1.0839720964431763\n",
      "epoch 28: loss=1.0554842948913574\n",
      "epoch 29: loss=1.0261443853378296\n",
      "epoch 30: loss=1.0020710229873657\n",
      "epoch 31: loss=0.9818207621574402\n",
      "epoch 32: loss=0.9747335910797119\n",
      "epoch 33: loss=0.9746344089508057\n",
      "epoch 34: loss=0.9648775458335876\n",
      "epoch 35: loss=0.9524271488189697\n",
      "epoch 36: loss=0.941288948059082\n",
      "epoch 37: loss=0.9346219897270203\n",
      "epoch 38: loss=0.9331791996955872\n",
      "epoch 39: loss=0.9253087043762207\n",
      "epoch 40: loss=0.9274557828903198\n",
      "epoch 41: loss=0.9207607507705688\n",
      "epoch 42: loss=0.9160459041595459\n",
      "epoch 43: loss=0.9111889004707336\n",
      "epoch 44: loss=0.9114537835121155\n",
      "epoch 45: loss=0.9083862900733948\n",
      "epoch 46: loss=0.9057672619819641\n",
      "epoch 47: loss=0.9077441096305847\n",
      "epoch 48: loss=0.9012389183044434\n",
      "epoch 49: loss=0.900077223777771\n",
      "epoch 50: loss=0.896671712398529\n",
      "epoch 51: loss=0.8987240791320801\n",
      "epoch 52: loss=0.8957083821296692\n",
      "epoch 53: loss=0.8963632583618164\n",
      "epoch 54: loss=0.8933905959129333\n",
      "epoch 55: loss=0.8932562470436096\n",
      "epoch 56: loss=0.8943542838096619\n",
      "epoch 57: loss=0.8923667073249817\n",
      "epoch 58: loss=0.8935333490371704\n",
      "epoch 59: loss=0.8899096250534058\n",
      "epoch 60: loss=0.8905327320098877\n",
      "epoch 61: loss=0.890337347984314\n",
      "epoch 62: loss=0.8895777463912964\n",
      "epoch 63: loss=0.8889721035957336\n",
      "epoch 64: loss=0.8874492645263672\n",
      "epoch 65: loss=0.8874069452285767\n",
      "epoch 66: loss=0.886749267578125\n",
      "epoch 67: loss=0.886731743812561\n",
      "epoch 68: loss=0.885230541229248\n",
      "epoch 69: loss=0.8870899081230164\n",
      "epoch 70: loss=0.88545823097229\n",
      "epoch 71: loss=0.8840484023094177\n",
      "epoch 72: loss=0.8853937983512878\n",
      "epoch 73: loss=0.884628176689148\n",
      "epoch 74: loss=0.8817494511604309\n",
      "epoch 75: loss=0.8805989623069763\n",
      "epoch 76: loss=0.8811383247375488\n",
      "epoch 77: loss=0.8842387199401855\n",
      "epoch 78: loss=0.8838135004043579\n",
      "epoch 79: loss=0.8832184672355652\n",
      "epoch 80: loss=0.8816060423851013\n",
      "epoch 81: loss=0.8817720413208008\n",
      "epoch 82: loss=0.8813443183898926\n",
      "epoch 83: loss=0.8811815977096558\n",
      "epoch 84: loss=0.8804734349250793\n",
      "epoch 85: loss=0.8835895657539368\n",
      "epoch 86: loss=0.8798424601554871\n",
      "epoch 87: loss=0.8798980116844177\n",
      "epoch 88: loss=0.8783650994300842\n",
      "epoch 89: loss=0.8808810710906982\n",
      "epoch 90: loss=0.8787031173706055\n",
      "epoch 91: loss=0.8813807368278503\n",
      "epoch 92: loss=0.8806894421577454\n",
      "epoch 93: loss=0.8819399476051331\n",
      "epoch 94: loss=0.8808392286300659\n",
      "epoch 95: loss=0.8786369562149048\n",
      "epoch 96: loss=0.8783720135688782\n",
      "epoch 97: loss=0.8775798082351685\n",
      "epoch 98: loss=0.8793601989746094\n",
      "epoch 99: loss=0.8778429627418518\n",
      "epoch 100: loss=0.8802753686904907\n",
      "epoch 101: loss=0.8780264854431152\n",
      "epoch 102: loss=0.8768166899681091\n",
      "epoch 103: loss=0.8785808086395264\n",
      "epoch 104: loss=0.8771544098854065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 105: loss=0.8784377574920654\n",
      "epoch 106: loss=0.8767684102058411\n",
      "epoch 107: loss=0.874022901058197\n",
      "epoch 108: loss=0.8752598762512207\n",
      "epoch 109: loss=0.8759281635284424\n",
      "epoch 110: loss=0.8786908984184265\n",
      "epoch 111: loss=0.8744621276855469\n",
      "epoch 112: loss=0.8770614862442017\n",
      "epoch 113: loss=0.8770796060562134\n",
      "epoch 114: loss=0.8762810230255127\n",
      "epoch 115: loss=0.8747966885566711\n",
      "epoch 116: loss=0.8747941255569458\n",
      "epoch 117: loss=0.875089168548584\n",
      "epoch 118: loss=0.8726224899291992\n",
      "epoch 119: loss=0.8732908964157104\n",
      "epoch 120: loss=0.8729326128959656\n",
      "epoch 121: loss=0.8732678890228271\n",
      "epoch 122: loss=0.8722840547561646\n",
      "epoch 123: loss=0.8747309446334839\n",
      "epoch 124: loss=0.8735998272895813\n",
      "epoch 125: loss=0.8724068999290466\n",
      "epoch 126: loss=0.8721141219139099\n",
      "epoch 127: loss=0.8728490471839905\n",
      "epoch 128: loss=0.8702783584594727\n",
      "epoch 129: loss=0.8734800815582275\n",
      "epoch 130: loss=0.872764527797699\n",
      "epoch 131: loss=0.8710581660270691\n",
      "epoch 132: loss=0.8704035878181458\n",
      "epoch 133: loss=0.8706244826316833\n",
      "epoch 134: loss=0.8696462512016296\n",
      "epoch 135: loss=0.8683674335479736\n",
      "epoch 136: loss=0.8702283501625061\n",
      "epoch 137: loss=0.8690803647041321\n",
      "epoch 138: loss=0.8680703043937683\n",
      "epoch 139: loss=0.868602454662323\n",
      "epoch 140: loss=0.8705683350563049\n",
      "epoch 141: loss=0.8669477701187134\n",
      "epoch 142: loss=0.8674701452255249\n",
      "epoch 143: loss=0.8664101362228394\n",
      "epoch 144: loss=0.8670432567596436\n",
      "epoch 145: loss=0.8657217621803284\n",
      "epoch 146: loss=0.8686015009880066\n",
      "epoch 147: loss=0.8687337636947632\n",
      "epoch 148: loss=0.8672666549682617\n",
      "epoch 149: loss=0.8640048503875732\n",
      "epoch 150: loss=0.8653640747070312\n",
      "epoch 151: loss=0.8663027882575989\n",
      "epoch 152: loss=0.8652461767196655\n",
      "epoch 153: loss=0.8637121319770813\n",
      "epoch 154: loss=0.8641501665115356\n",
      "epoch 155: loss=0.86374431848526\n",
      "epoch 156: loss=0.8626002669334412\n",
      "epoch 157: loss=0.8635327219963074\n",
      "epoch 158: loss=0.8622744083404541\n",
      "epoch 159: loss=0.8626284003257751\n",
      "epoch 160: loss=0.8627922534942627\n",
      "epoch 161: loss=0.8617056012153625\n",
      "epoch 162: loss=0.8624790906906128\n",
      "epoch 163: loss=0.8612484335899353\n",
      "epoch 164: loss=0.8598385453224182\n",
      "epoch 165: loss=0.8573232889175415\n",
      "epoch 166: loss=0.8587490320205688\n",
      "epoch 167: loss=0.860159158706665\n",
      "epoch 168: loss=0.8590152859687805\n",
      "epoch 169: loss=0.8576498627662659\n",
      "epoch 170: loss=0.8560856580734253\n",
      "epoch 171: loss=0.8538155555725098\n",
      "epoch 172: loss=0.8532009124755859\n",
      "epoch 173: loss=0.8552605509757996\n",
      "epoch 174: loss=0.853156566619873\n",
      "epoch 175: loss=0.8545413017272949\n",
      "epoch 176: loss=0.8523183465003967\n",
      "epoch 177: loss=0.8541602492332458\n",
      "epoch 178: loss=0.8498683571815491\n",
      "epoch 179: loss=0.8527202010154724\n",
      "epoch 180: loss=0.8501492738723755\n",
      "epoch 181: loss=0.8497118353843689\n",
      "epoch 182: loss=0.8512515425682068\n",
      "epoch 183: loss=0.8493274450302124\n",
      "epoch 184: loss=0.8479596376419067\n",
      "epoch 185: loss=0.8498988151550293\n",
      "epoch 186: loss=0.8470645546913147\n",
      "epoch 187: loss=0.8488975763320923\n",
      "epoch 188: loss=0.8504384756088257\n",
      "epoch 189: loss=0.8458369374275208\n",
      "epoch 190: loss=0.8462876677513123\n",
      "epoch 191: loss=0.8465628027915955\n",
      "epoch 192: loss=0.845307469367981\n",
      "epoch 193: loss=0.8459029793739319\n",
      "epoch 194: loss=0.8452176451683044\n",
      "epoch 195: loss=0.8447084426879883\n",
      "epoch 196: loss=0.8440096378326416\n",
      "epoch 197: loss=0.8448516726493835\n",
      "epoch 198: loss=0.8457568883895874\n",
      "epoch 199: loss=0.8441618084907532\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=4.690552711486816\n",
      "epoch 1: loss=4.433281898498535\n",
      "epoch 2: loss=4.137340545654297\n",
      "epoch 3: loss=3.7536120414733887\n",
      "epoch 4: loss=3.391425848007202\n",
      "epoch 5: loss=2.9799654483795166\n",
      "epoch 6: loss=2.603081464767456\n",
      "epoch 7: loss=2.4794540405273438\n",
      "epoch 8: loss=2.3841679096221924\n",
      "epoch 9: loss=2.3044188022613525\n",
      "epoch 10: loss=2.119600772857666\n",
      "epoch 11: loss=1.953731894493103\n",
      "epoch 12: loss=1.797891616821289\n",
      "epoch 13: loss=1.6430639028549194\n",
      "epoch 14: loss=1.532389521598816\n",
      "epoch 15: loss=1.4608694314956665\n",
      "epoch 16: loss=1.4290130138397217\n",
      "epoch 17: loss=1.3983750343322754\n",
      "epoch 18: loss=1.3587915897369385\n",
      "epoch 19: loss=1.310861349105835\n",
      "epoch 20: loss=1.2644709348678589\n",
      "epoch 21: loss=1.226894497871399\n",
      "epoch 22: loss=1.2003614902496338\n",
      "epoch 23: loss=1.1867241859436035\n",
      "epoch 24: loss=1.1531250476837158\n",
      "epoch 25: loss=1.1102983951568604\n",
      "epoch 26: loss=1.0713257789611816\n",
      "epoch 27: loss=1.0457055568695068\n",
      "epoch 28: loss=1.0205634832382202\n",
      "epoch 29: loss=1.007849097251892\n",
      "epoch 30: loss=0.9946412444114685\n",
      "epoch 31: loss=0.9859523773193359\n",
      "epoch 32: loss=0.9828191995620728\n",
      "epoch 33: loss=0.9795699715614319\n",
      "epoch 34: loss=0.9764646291732788\n",
      "epoch 35: loss=0.9695720672607422\n",
      "epoch 36: loss=0.9613539576530457\n",
      "epoch 37: loss=0.9558244347572327\n",
      "epoch 38: loss=0.94952392578125\n",
      "epoch 39: loss=0.9412119388580322\n",
      "epoch 40: loss=0.9390331506729126\n",
      "epoch 41: loss=0.9363414645195007\n",
      "epoch 42: loss=0.9329363703727722\n",
      "epoch 43: loss=0.934773325920105\n",
      "epoch 44: loss=0.9327744245529175\n",
      "epoch 45: loss=0.9302919507026672\n",
      "epoch 46: loss=0.9247481226921082\n",
      "epoch 47: loss=0.9279302954673767\n",
      "epoch 48: loss=0.920930027961731\n",
      "epoch 49: loss=0.9180893898010254\n",
      "epoch 50: loss=0.9165449738502502\n",
      "epoch 51: loss=0.9159701466560364\n",
      "epoch 52: loss=0.9157816767692566\n",
      "epoch 53: loss=0.9148147106170654\n",
      "epoch 54: loss=0.9139196276664734\n",
      "epoch 55: loss=0.9124287962913513\n",
      "epoch 56: loss=0.9123781323432922\n",
      "epoch 57: loss=0.9124918580055237\n",
      "epoch 58: loss=0.907993495464325\n",
      "epoch 59: loss=0.9099089503288269\n",
      "epoch 60: loss=0.9100452065467834\n",
      "epoch 61: loss=0.9089263677597046\n",
      "epoch 62: loss=0.9051549434661865\n",
      "epoch 63: loss=0.9059959650039673\n",
      "epoch 64: loss=0.9083067178726196\n",
      "epoch 65: loss=0.905093789100647\n",
      "epoch 66: loss=0.9046856760978699\n",
      "epoch 67: loss=0.9045478701591492\n",
      "epoch 68: loss=0.9047508835792542\n",
      "epoch 69: loss=0.9048856496810913\n",
      "epoch 70: loss=0.902681291103363\n",
      "epoch 71: loss=0.9030162692070007\n",
      "epoch 72: loss=0.9006108641624451\n",
      "epoch 73: loss=0.9039428234100342\n",
      "epoch 74: loss=0.8987640142440796\n",
      "epoch 75: loss=0.8984564542770386\n",
      "epoch 76: loss=0.9008066654205322\n",
      "epoch 77: loss=0.9003949761390686\n",
      "epoch 78: loss=0.8987525701522827\n",
      "epoch 79: loss=0.9000735282897949\n",
      "epoch 80: loss=0.8987825512886047\n",
      "epoch 81: loss=0.8983266353607178\n",
      "epoch 82: loss=0.8987138271331787\n",
      "epoch 83: loss=0.8979809880256653\n",
      "epoch 84: loss=0.8958900570869446\n",
      "epoch 85: loss=0.896523654460907\n",
      "epoch 86: loss=0.8956909775733948\n",
      "epoch 87: loss=0.8990674614906311\n",
      "epoch 88: loss=0.8966081142425537\n",
      "epoch 89: loss=0.8965972661972046\n",
      "epoch 90: loss=0.8969469666481018\n",
      "epoch 91: loss=0.896248996257782\n",
      "epoch 92: loss=0.8942791223526001\n",
      "epoch 93: loss=0.894368588924408\n",
      "epoch 94: loss=0.8962781429290771\n",
      "epoch 95: loss=0.8944450616836548\n",
      "epoch 96: loss=0.8943272829055786\n",
      "epoch 97: loss=0.8953695297241211\n",
      "epoch 98: loss=0.8928114771842957\n",
      "epoch 99: loss=0.8929668664932251\n",
      "epoch 100: loss=0.8941463828086853\n",
      "epoch 101: loss=0.893439531326294\n",
      "epoch 102: loss=0.8928984999656677\n",
      "epoch 103: loss=0.8924695253372192\n",
      "epoch 104: loss=0.8919295072555542\n",
      "epoch 105: loss=0.8910172581672668\n",
      "epoch 106: loss=0.8891587257385254\n",
      "epoch 107: loss=0.8886306881904602\n",
      "epoch 108: loss=0.8900985717773438\n",
      "epoch 109: loss=0.8900121450424194\n",
      "epoch 110: loss=0.8894740343093872\n",
      "epoch 111: loss=0.8890664577484131\n",
      "epoch 112: loss=0.8884199261665344\n",
      "epoch 113: loss=0.8896464109420776\n",
      "epoch 114: loss=0.8886183500289917\n",
      "epoch 115: loss=0.8892218470573425\n",
      "epoch 116: loss=0.8874900341033936\n",
      "epoch 117: loss=0.8887544274330139\n",
      "epoch 118: loss=0.8887768387794495\n",
      "epoch 119: loss=0.8880532383918762\n",
      "epoch 120: loss=0.8887523412704468\n",
      "epoch 121: loss=0.8887348771095276\n",
      "epoch 122: loss=0.8873550891876221\n",
      "epoch 123: loss=0.8857229351997375\n",
      "epoch 124: loss=0.8866857886314392\n",
      "epoch 125: loss=0.8859199285507202\n",
      "epoch 126: loss=0.8878152966499329\n",
      "epoch 127: loss=0.8860146999359131\n",
      "epoch 128: loss=0.8844427466392517\n",
      "epoch 129: loss=0.8854157328605652\n",
      "epoch 130: loss=0.8844919204711914\n",
      "epoch 131: loss=0.8868233561515808\n",
      "epoch 132: loss=0.8856401443481445\n",
      "epoch 133: loss=0.885967493057251\n",
      "epoch 134: loss=0.8844972848892212\n",
      "epoch 135: loss=0.8845759630203247\n",
      "epoch 136: loss=0.8880432844161987\n",
      "epoch 137: loss=0.8857845067977905\n",
      "epoch 138: loss=0.8845306634902954\n",
      "epoch 139: loss=0.8860154151916504\n",
      "epoch 140: loss=0.8858730792999268\n",
      "epoch 141: loss=0.883753776550293\n",
      "epoch 142: loss=0.88355952501297\n",
      "epoch 143: loss=0.8851370811462402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 144: loss=0.881874144077301\n",
      "epoch 145: loss=0.8824092745780945\n",
      "epoch 146: loss=0.8835161328315735\n",
      "epoch 147: loss=0.8840320110321045\n",
      "epoch 148: loss=0.8818091750144958\n",
      "epoch 149: loss=0.8826576471328735\n",
      "epoch 150: loss=0.8813666105270386\n",
      "epoch 151: loss=0.8807931542396545\n",
      "epoch 152: loss=0.8810566663742065\n",
      "epoch 153: loss=0.8835324645042419\n",
      "epoch 154: loss=0.8800419569015503\n",
      "epoch 155: loss=0.883682131767273\n",
      "epoch 156: loss=0.8805956840515137\n",
      "epoch 157: loss=0.8790867328643799\n",
      "epoch 158: loss=0.8789628744125366\n",
      "epoch 159: loss=0.8807772994041443\n",
      "epoch 160: loss=0.8809391856193542\n",
      "epoch 161: loss=0.8829143643379211\n",
      "epoch 162: loss=0.8810791373252869\n",
      "epoch 163: loss=0.8801589608192444\n",
      "epoch 164: loss=0.880218505859375\n",
      "epoch 165: loss=0.8813298940658569\n",
      "epoch 166: loss=0.879402756690979\n",
      "epoch 167: loss=0.8819915652275085\n",
      "epoch 168: loss=0.8778617978096008\n",
      "epoch 169: loss=0.8799625635147095\n",
      "epoch 170: loss=0.880530595779419\n",
      "epoch 171: loss=0.8775339126586914\n",
      "epoch 172: loss=0.8797547817230225\n",
      "epoch 173: loss=0.8786213994026184\n",
      "epoch 174: loss=0.8785882592201233\n",
      "epoch 175: loss=0.877788782119751\n",
      "epoch 176: loss=0.8795483112335205\n",
      "epoch 177: loss=0.8793764114379883\n",
      "epoch 178: loss=0.8785207271575928\n",
      "epoch 179: loss=0.8784377574920654\n",
      "epoch 180: loss=0.8783687949180603\n",
      "epoch 181: loss=0.8788776993751526\n",
      "epoch 182: loss=0.877092182636261\n",
      "epoch 183: loss=0.8781129121780396\n",
      "epoch 184: loss=0.8769305348396301\n",
      "epoch 185: loss=0.8778933882713318\n",
      "epoch 186: loss=0.8799650073051453\n",
      "epoch 187: loss=0.8794665336608887\n",
      "epoch 188: loss=0.8784504532814026\n",
      "epoch 189: loss=0.8787508606910706\n",
      "epoch 190: loss=0.8770633339881897\n",
      "epoch 191: loss=0.8762660622596741\n",
      "epoch 192: loss=0.879176676273346\n",
      "epoch 193: loss=0.8772047162055969\n",
      "epoch 194: loss=0.8762099742889404\n",
      "epoch 195: loss=0.8783625364303589\n",
      "epoch 196: loss=0.8750324845314026\n",
      "epoch 197: loss=0.8778529763221741\n",
      "epoch 198: loss=0.8745945692062378\n",
      "epoch 199: loss=0.8753652572631836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3caf3749b14b54aec54236ae8dc7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 120584.5\n",
      "Epoch 10, Loss: 26435.419921875\n",
      "Epoch 20, Loss: 21100.25\n",
      "Epoch 30, Loss: 19275.33203125\n",
      "Epoch 40, Loss: 18552.37890625\n",
      "Epoch 50, Loss: 18220.66796875\n",
      "Epoch 60, Loss: 18061.18359375\n",
      "Epoch 70, Loss: 17996.193359375\n",
      "Epoch 80, Loss: 17961.31640625\n",
      "Epoch 90, Loss: 17941.62890625\n",
      "Epoch 100, Loss: 17927.234375\n",
      "Epoch 110, Loss: 17916.40234375\n",
      "Epoch 120, Loss: 17907.6015625\n",
      "Epoch 130, Loss: 17900.228515625\n",
      "Epoch 140, Loss: 17893.931640625\n",
      "Epoch 150, Loss: 17888.515625\n",
      "Epoch 160, Loss: 17883.822265625\n",
      "Epoch 170, Loss: 17879.80078125\n",
      "Epoch 180, Loss: 17879.7578125\n",
      "Epoch 190, Loss: 17873.142578125\n",
      "training patch with 267352 edges\n",
      "epoch 0: loss=6.690918922424316\n",
      "epoch 1: loss=6.398488521575928\n",
      "epoch 2: loss=6.108060359954834\n",
      "epoch 3: loss=5.793529033660889\n",
      "epoch 4: loss=5.342134475708008\n",
      "epoch 5: loss=4.777660369873047\n",
      "epoch 6: loss=4.268801689147949\n",
      "epoch 7: loss=3.826158285140991\n",
      "epoch 8: loss=3.5656440258026123\n",
      "epoch 9: loss=3.4550089836120605\n",
      "epoch 10: loss=3.321542739868164\n",
      "epoch 11: loss=3.113635540008545\n",
      "epoch 12: loss=2.8786556720733643\n",
      "epoch 13: loss=2.6299314498901367\n",
      "epoch 14: loss=2.428834915161133\n",
      "epoch 15: loss=2.2350804805755615\n",
      "epoch 16: loss=2.073887825012207\n",
      "epoch 17: loss=1.9511170387268066\n",
      "epoch 18: loss=1.8697469234466553\n",
      "epoch 19: loss=1.7871425151824951\n",
      "epoch 20: loss=1.7171744108200073\n",
      "epoch 21: loss=1.6518977880477905\n",
      "epoch 22: loss=1.5800304412841797\n",
      "epoch 23: loss=1.501703143119812\n",
      "epoch 24: loss=1.4189287424087524\n",
      "epoch 25: loss=1.3448020219802856\n",
      "epoch 26: loss=1.287558913230896\n",
      "epoch 27: loss=1.2327609062194824\n",
      "epoch 28: loss=1.1808793544769287\n",
      "epoch 29: loss=1.1321223974227905\n",
      "epoch 30: loss=1.0928196907043457\n",
      "epoch 31: loss=1.0579615831375122\n",
      "epoch 32: loss=1.0329893827438354\n",
      "epoch 33: loss=1.0185662508010864\n",
      "epoch 34: loss=1.009325385093689\n",
      "epoch 35: loss=1.0047191381454468\n",
      "epoch 36: loss=0.9951933026313782\n",
      "epoch 37: loss=0.9838119745254517\n",
      "epoch 38: loss=0.9703059196472168\n",
      "epoch 39: loss=0.9564865827560425\n",
      "epoch 40: loss=0.9453759789466858\n",
      "epoch 41: loss=0.9390093088150024\n",
      "epoch 42: loss=0.9341771602630615\n",
      "epoch 43: loss=0.931125819683075\n",
      "epoch 44: loss=0.9292258620262146\n",
      "epoch 45: loss=0.9247971773147583\n",
      "epoch 46: loss=0.9244795441627502\n",
      "epoch 47: loss=0.9223989248275757\n",
      "epoch 48: loss=0.9185115694999695\n",
      "epoch 49: loss=0.915245771408081\n",
      "epoch 50: loss=0.9120374321937561\n",
      "epoch 51: loss=0.9104580283164978\n",
      "epoch 52: loss=0.9068226218223572\n",
      "epoch 53: loss=0.9055589437484741\n",
      "epoch 54: loss=0.9043636918067932\n",
      "epoch 55: loss=0.9044186472892761\n",
      "epoch 56: loss=0.9039908051490784\n",
      "epoch 57: loss=0.9020950198173523\n",
      "epoch 58: loss=0.9000892043113708\n",
      "epoch 59: loss=0.8977941870689392\n",
      "epoch 60: loss=0.8985291719436646\n",
      "epoch 61: loss=0.8968161344528198\n",
      "epoch 62: loss=0.8968783020973206\n",
      "epoch 63: loss=0.8946611285209656\n",
      "epoch 64: loss=0.8953242897987366\n",
      "epoch 65: loss=0.8941657543182373\n",
      "epoch 66: loss=0.893682062625885\n",
      "epoch 67: loss=0.8922558426856995\n",
      "epoch 68: loss=0.8910995125770569\n",
      "epoch 69: loss=0.8900628685951233\n",
      "epoch 70: loss=0.8898755311965942\n",
      "epoch 71: loss=0.8892883062362671\n",
      "epoch 72: loss=0.8884549140930176\n",
      "epoch 73: loss=0.8893271684646606\n",
      "epoch 74: loss=0.8883770704269409\n",
      "epoch 75: loss=0.8862430453300476\n",
      "epoch 76: loss=0.8870896697044373\n",
      "epoch 77: loss=0.8850318789482117\n",
      "epoch 78: loss=0.8844743371009827\n",
      "epoch 79: loss=0.8851097226142883\n",
      "epoch 80: loss=0.884999692440033\n",
      "epoch 81: loss=0.8841529488563538\n",
      "epoch 82: loss=0.8821505904197693\n",
      "epoch 83: loss=0.8833045959472656\n",
      "epoch 84: loss=0.8814170956611633\n",
      "epoch 85: loss=0.881929874420166\n",
      "epoch 86: loss=0.8809508681297302\n",
      "epoch 87: loss=0.8806458115577698\n",
      "epoch 88: loss=0.8798334002494812\n",
      "epoch 89: loss=0.8788521885871887\n",
      "epoch 90: loss=0.8787465691566467\n",
      "epoch 91: loss=0.8772574067115784\n",
      "epoch 92: loss=0.8769641518592834\n",
      "epoch 93: loss=0.8753799796104431\n",
      "epoch 94: loss=0.8759859800338745\n",
      "epoch 95: loss=0.8746469020843506\n",
      "epoch 96: loss=0.8733852505683899\n",
      "epoch 97: loss=0.8724499344825745\n",
      "epoch 98: loss=0.8711642026901245\n",
      "epoch 99: loss=0.8702150583267212\n",
      "epoch 100: loss=0.8677361607551575\n",
      "epoch 101: loss=0.867606520652771\n",
      "epoch 102: loss=0.8659722208976746\n",
      "epoch 103: loss=0.864459753036499\n",
      "epoch 104: loss=0.8643250465393066\n",
      "epoch 105: loss=0.8642223477363586\n",
      "epoch 106: loss=0.8631237149238586\n",
      "epoch 107: loss=0.8598777055740356\n",
      "epoch 108: loss=0.8611117005348206\n",
      "epoch 109: loss=0.8599051833152771\n",
      "epoch 110: loss=0.860837996006012\n",
      "epoch 111: loss=0.8599227070808411\n",
      "epoch 112: loss=0.8579801917076111\n",
      "epoch 113: loss=0.8569030165672302\n",
      "epoch 114: loss=0.8570277094841003\n",
      "epoch 115: loss=0.8552830815315247\n",
      "epoch 116: loss=0.8551440238952637\n",
      "epoch 117: loss=0.8536813855171204\n",
      "epoch 118: loss=0.8531800508499146\n",
      "epoch 119: loss=0.8518903851509094\n",
      "epoch 120: loss=0.8500719666481018\n",
      "epoch 121: loss=0.8490808010101318\n",
      "epoch 122: loss=0.8502085208892822\n",
      "epoch 123: loss=0.8505085110664368\n",
      "epoch 124: loss=0.8478251099586487\n",
      "epoch 125: loss=0.8464139699935913\n",
      "epoch 126: loss=0.8470263481140137\n",
      "epoch 127: loss=0.8456926941871643\n",
      "epoch 128: loss=0.8445628881454468\n",
      "epoch 129: loss=0.8450251817703247\n",
      "epoch 130: loss=0.8442724943161011\n",
      "epoch 131: loss=0.8433234095573425\n",
      "epoch 132: loss=0.8426219820976257\n",
      "epoch 133: loss=0.8417448401451111\n",
      "epoch 134: loss=0.8410133123397827\n",
      "epoch 135: loss=0.8404076099395752\n",
      "epoch 136: loss=0.8392433524131775\n",
      "epoch 137: loss=0.8389843702316284\n",
      "epoch 138: loss=0.8388347625732422\n",
      "epoch 139: loss=0.8377004265785217\n",
      "epoch 140: loss=0.8367006182670593\n",
      "epoch 141: loss=0.8357248902320862\n",
      "epoch 142: loss=0.8351470232009888\n",
      "epoch 143: loss=0.835952877998352\n",
      "epoch 144: loss=0.83427894115448\n",
      "epoch 145: loss=0.8332448601722717\n",
      "epoch 146: loss=0.8336557149887085\n",
      "epoch 147: loss=0.8331530094146729\n",
      "epoch 148: loss=0.8308112621307373\n",
      "epoch 149: loss=0.8320151567459106\n",
      "epoch 150: loss=0.8298865556716919\n",
      "epoch 151: loss=0.8311967253684998\n",
      "epoch 152: loss=0.8296883702278137\n",
      "epoch 153: loss=0.8290524482727051\n",
      "epoch 154: loss=0.8281756043434143\n",
      "epoch 155: loss=0.8283412456512451\n",
      "epoch 156: loss=0.8278563022613525\n",
      "epoch 157: loss=0.8268319964408875\n",
      "epoch 158: loss=0.8259876370429993\n",
      "epoch 159: loss=0.8261365294456482\n",
      "epoch 160: loss=0.8264185190200806\n",
      "epoch 161: loss=0.8252397775650024\n",
      "epoch 162: loss=0.8262964487075806\n",
      "epoch 163: loss=0.8243119716644287\n",
      "epoch 164: loss=0.824654757976532\n",
      "epoch 165: loss=0.8230221271514893\n",
      "epoch 166: loss=0.8242536783218384\n",
      "epoch 167: loss=0.82369464635849\n",
      "epoch 168: loss=0.8228231072425842\n",
      "epoch 169: loss=0.8239194750785828\n",
      "epoch 170: loss=0.8211986422538757\n",
      "epoch 171: loss=0.8222956657409668\n",
      "epoch 172: loss=0.8205476999282837\n",
      "epoch 173: loss=0.8191387057304382\n",
      "epoch 174: loss=0.820687472820282\n",
      "epoch 175: loss=0.8206021785736084\n",
      "epoch 176: loss=0.8205931186676025\n",
      "epoch 177: loss=0.8189210891723633\n",
      "epoch 178: loss=0.8186632394790649\n",
      "epoch 179: loss=0.8201457262039185\n",
      "epoch 180: loss=0.8181777596473694\n",
      "epoch 181: loss=0.8175162076950073\n",
      "epoch 182: loss=0.8159071207046509\n",
      "epoch 183: loss=0.8173257112503052\n",
      "epoch 184: loss=0.8158936500549316\n",
      "epoch 185: loss=0.8159301280975342\n",
      "epoch 186: loss=0.8163956999778748\n",
      "epoch 187: loss=0.8143388628959656\n",
      "epoch 188: loss=0.8133198022842407\n",
      "epoch 189: loss=0.814528226852417\n",
      "epoch 190: loss=0.8154839277267456\n",
      "epoch 191: loss=0.8143588900566101\n",
      "epoch 192: loss=0.8134100437164307\n",
      "epoch 193: loss=0.8147146701812744\n",
      "epoch 194: loss=0.8141657114028931\n",
      "epoch 195: loss=0.8124951124191284\n",
      "epoch 196: loss=0.8135743737220764\n",
      "epoch 197: loss=0.8124845027923584\n",
      "epoch 198: loss=0.8130421042442322\n",
      "epoch 199: loss=0.811775803565979\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=6.6796064376831055\n",
      "epoch 1: loss=6.410269737243652\n",
      "epoch 2: loss=6.1006951332092285\n",
      "epoch 3: loss=5.6037678718566895\n",
      "epoch 4: loss=5.12875509262085\n",
      "epoch 5: loss=4.522677898406982\n",
      "epoch 6: loss=3.9988198280334473\n",
      "epoch 7: loss=3.6959712505340576\n",
      "epoch 8: loss=3.555266857147217\n",
      "epoch 9: loss=3.400158166885376\n",
      "epoch 10: loss=3.190760612487793\n",
      "epoch 11: loss=2.918874502182007\n",
      "epoch 12: loss=2.6442720890045166\n",
      "epoch 13: loss=2.39150333404541\n",
      "epoch 14: loss=2.175262451171875\n",
      "epoch 15: loss=2.028073310852051\n",
      "epoch 16: loss=1.9212809801101685\n",
      "epoch 17: loss=1.836759090423584\n",
      "epoch 18: loss=1.7785618305206299\n",
      "epoch 19: loss=1.7100927829742432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss=1.6375874280929565\n",
      "epoch 21: loss=1.5811530351638794\n",
      "epoch 22: loss=1.5212831497192383\n",
      "epoch 23: loss=1.4602549076080322\n",
      "epoch 24: loss=1.381783127784729\n",
      "epoch 25: loss=1.3144627809524536\n",
      "epoch 26: loss=1.2466051578521729\n",
      "epoch 27: loss=1.185461401939392\n",
      "epoch 28: loss=1.1392847299575806\n",
      "epoch 29: loss=1.092180848121643\n",
      "epoch 30: loss=1.069297432899475\n",
      "epoch 31: loss=1.0599151849746704\n",
      "epoch 32: loss=1.0449397563934326\n",
      "epoch 33: loss=1.0319135189056396\n",
      "epoch 34: loss=1.0176362991333008\n",
      "epoch 35: loss=1.0021162033081055\n",
      "epoch 36: loss=0.9910366535186768\n",
      "epoch 37: loss=0.9824203848838806\n",
      "epoch 38: loss=0.9717646837234497\n",
      "epoch 39: loss=0.9592546224594116\n",
      "epoch 40: loss=0.9533299207687378\n",
      "epoch 41: loss=0.9483082890510559\n",
      "epoch 42: loss=0.9403266310691833\n",
      "epoch 43: loss=0.9333138465881348\n",
      "epoch 44: loss=0.9284899234771729\n",
      "epoch 45: loss=0.9272041320800781\n",
      "epoch 46: loss=0.9271692633628845\n",
      "epoch 47: loss=0.9235110282897949\n",
      "epoch 48: loss=0.9204846620559692\n",
      "epoch 49: loss=0.9171157479286194\n",
      "epoch 50: loss=0.9142836332321167\n",
      "epoch 51: loss=0.9109746217727661\n",
      "epoch 52: loss=0.9099295735359192\n",
      "epoch 53: loss=0.9094586372375488\n",
      "epoch 54: loss=0.9074900150299072\n",
      "epoch 55: loss=0.9067289233207703\n",
      "epoch 56: loss=0.9043823480606079\n",
      "epoch 57: loss=0.9017415642738342\n",
      "epoch 58: loss=0.9020276665687561\n",
      "epoch 59: loss=0.8987262845039368\n",
      "epoch 60: loss=0.8981603980064392\n",
      "epoch 61: loss=0.8972666263580322\n",
      "epoch 62: loss=0.8972562551498413\n",
      "epoch 63: loss=0.8966530561447144\n",
      "epoch 64: loss=0.8952040672302246\n",
      "epoch 65: loss=0.8945923447608948\n",
      "epoch 66: loss=0.8929130434989929\n",
      "epoch 67: loss=0.8921098113059998\n",
      "epoch 68: loss=0.8915573358535767\n",
      "epoch 69: loss=0.8907710313796997\n",
      "epoch 70: loss=0.89008629322052\n",
      "epoch 71: loss=0.889381468296051\n",
      "epoch 72: loss=0.8895371556282043\n",
      "epoch 73: loss=0.8882713913917542\n",
      "epoch 74: loss=0.8867952823638916\n",
      "epoch 75: loss=0.8862467408180237\n",
      "epoch 76: loss=0.8867844939231873\n",
      "epoch 77: loss=0.8867257833480835\n",
      "epoch 78: loss=0.8865154981613159\n",
      "epoch 79: loss=0.8860153555870056\n",
      "epoch 80: loss=0.8832468390464783\n",
      "epoch 81: loss=0.8819594383239746\n",
      "epoch 82: loss=0.8819294571876526\n",
      "epoch 83: loss=0.8823931217193604\n",
      "epoch 84: loss=0.8824234008789062\n",
      "epoch 85: loss=0.8803173303604126\n",
      "epoch 86: loss=0.8787376284599304\n",
      "epoch 87: loss=0.8787501454353333\n",
      "epoch 88: loss=0.878997266292572\n",
      "epoch 89: loss=0.8786980509757996\n",
      "epoch 90: loss=0.8788117170333862\n",
      "epoch 91: loss=0.8766557574272156\n",
      "epoch 92: loss=0.8758466243743896\n",
      "epoch 93: loss=0.8750355839729309\n",
      "epoch 94: loss=0.8738422989845276\n",
      "epoch 95: loss=0.8729660511016846\n",
      "epoch 96: loss=0.8721522092819214\n",
      "epoch 97: loss=0.8718165755271912\n",
      "epoch 98: loss=0.8708451986312866\n",
      "epoch 99: loss=0.869899332523346\n",
      "epoch 100: loss=0.8697182536125183\n",
      "epoch 101: loss=0.8690600991249084\n",
      "epoch 102: loss=0.8687856793403625\n",
      "epoch 103: loss=0.8662583231925964\n",
      "epoch 104: loss=0.8656975030899048\n",
      "epoch 105: loss=0.8650730848312378\n",
      "epoch 106: loss=0.8652471899986267\n",
      "epoch 107: loss=0.8638414740562439\n",
      "epoch 108: loss=0.8647235631942749\n",
      "epoch 109: loss=0.8631294369697571\n",
      "epoch 110: loss=0.8637168407440186\n",
      "epoch 111: loss=0.8595200181007385\n",
      "epoch 112: loss=0.8612884879112244\n",
      "epoch 113: loss=0.8617365956306458\n",
      "epoch 114: loss=0.8581726551055908\n",
      "epoch 115: loss=0.8569425344467163\n",
      "epoch 116: loss=0.8573669195175171\n",
      "epoch 117: loss=0.8568117618560791\n",
      "epoch 118: loss=0.8548119068145752\n",
      "epoch 119: loss=0.8546664118766785\n",
      "epoch 120: loss=0.8541318774223328\n",
      "epoch 121: loss=0.8516381978988647\n",
      "epoch 122: loss=0.85084068775177\n",
      "epoch 123: loss=0.8506177067756653\n",
      "epoch 124: loss=0.8494131565093994\n",
      "epoch 125: loss=0.8485281467437744\n",
      "epoch 126: loss=0.8462287187576294\n",
      "epoch 127: loss=0.8465328812599182\n",
      "epoch 128: loss=0.8456717133522034\n",
      "epoch 129: loss=0.8437696695327759\n",
      "epoch 130: loss=0.8443762063980103\n",
      "epoch 131: loss=0.8446750044822693\n",
      "epoch 132: loss=0.8429604172706604\n",
      "epoch 133: loss=0.8433946371078491\n",
      "epoch 134: loss=0.8432369232177734\n",
      "epoch 135: loss=0.840408205986023\n",
      "epoch 136: loss=0.8406485915184021\n",
      "epoch 137: loss=0.8408927321434021\n",
      "epoch 138: loss=0.842323899269104\n",
      "epoch 139: loss=0.8399385809898376\n",
      "epoch 140: loss=0.8395212888717651\n",
      "epoch 141: loss=0.8395422697067261\n",
      "epoch 142: loss=0.8402228355407715\n",
      "epoch 143: loss=0.8380729556083679\n",
      "epoch 144: loss=0.838525652885437\n",
      "epoch 145: loss=0.8366546034812927\n",
      "epoch 146: loss=0.8370553255081177\n",
      "epoch 147: loss=0.8372556567192078\n",
      "epoch 148: loss=0.8371862769126892\n",
      "epoch 149: loss=0.8345536589622498\n",
      "epoch 150: loss=0.8334935903549194\n",
      "epoch 151: loss=0.8336793184280396\n",
      "epoch 152: loss=0.8350847959518433\n",
      "epoch 153: loss=0.833233118057251\n",
      "epoch 154: loss=0.8328526020050049\n",
      "epoch 155: loss=0.8328065276145935\n",
      "epoch 156: loss=0.8316034078598022\n",
      "epoch 157: loss=0.83220374584198\n",
      "epoch 158: loss=0.8328630924224854\n",
      "epoch 159: loss=0.8321150541305542\n",
      "epoch 160: loss=0.8300651907920837\n",
      "epoch 161: loss=0.8293854594230652\n",
      "epoch 162: loss=0.8293655514717102\n",
      "epoch 163: loss=0.8295844793319702\n",
      "epoch 164: loss=0.8293711543083191\n",
      "epoch 165: loss=0.8305004239082336\n",
      "epoch 166: loss=0.8274099230766296\n",
      "epoch 167: loss=0.8272346258163452\n",
      "epoch 168: loss=0.8278858065605164\n",
      "epoch 169: loss=0.8262550234794617\n",
      "epoch 170: loss=0.8279168009757996\n",
      "epoch 171: loss=0.8267439603805542\n",
      "epoch 172: loss=0.8262724876403809\n",
      "epoch 173: loss=0.8255594968795776\n",
      "epoch 174: loss=0.8282775282859802\n",
      "epoch 175: loss=0.8254595994949341\n",
      "epoch 176: loss=0.8255403637886047\n",
      "epoch 177: loss=0.8265395760536194\n",
      "epoch 178: loss=0.8237848281860352\n",
      "epoch 179: loss=0.8241774439811707\n",
      "epoch 180: loss=0.8244950175285339\n",
      "epoch 181: loss=0.824045717716217\n",
      "epoch 182: loss=0.8237219452857971\n",
      "epoch 183: loss=0.8243230581283569\n",
      "epoch 184: loss=0.8226572871208191\n",
      "epoch 185: loss=0.8243852853775024\n",
      "epoch 186: loss=0.8219602108001709\n",
      "epoch 187: loss=0.8249727487564087\n",
      "epoch 188: loss=0.8246528506278992\n",
      "epoch 189: loss=0.8221839070320129\n",
      "epoch 190: loss=0.8214772939682007\n",
      "epoch 191: loss=0.8223479986190796\n",
      "epoch 192: loss=0.8219698071479797\n",
      "epoch 193: loss=0.8216846585273743\n",
      "epoch 194: loss=0.8231144547462463\n",
      "epoch 195: loss=0.8192330598831177\n",
      "epoch 196: loss=0.8217347264289856\n",
      "epoch 197: loss=0.8196708559989929\n",
      "epoch 198: loss=0.8219308853149414\n",
      "epoch 199: loss=0.8224489092826843\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=6.639604568481445\n",
      "epoch 1: loss=6.472743034362793\n",
      "epoch 2: loss=6.0878705978393555\n",
      "epoch 3: loss=5.712613582611084\n",
      "epoch 4: loss=5.204320907592773\n",
      "epoch 5: loss=4.689243316650391\n",
      "epoch 6: loss=4.082561016082764\n",
      "epoch 7: loss=3.7085647583007812\n",
      "epoch 8: loss=3.5806140899658203\n",
      "epoch 9: loss=3.4291975498199463\n",
      "epoch 10: loss=3.163151502609253\n",
      "epoch 11: loss=2.8852343559265137\n",
      "epoch 12: loss=2.5534608364105225\n",
      "epoch 13: loss=2.317337989807129\n",
      "epoch 14: loss=2.0955371856689453\n",
      "epoch 15: loss=1.9724748134613037\n",
      "epoch 16: loss=1.848681092262268\n",
      "epoch 17: loss=1.790223479270935\n",
      "epoch 18: loss=1.7079720497131348\n",
      "epoch 19: loss=1.6421860456466675\n",
      "epoch 20: loss=1.5558514595031738\n",
      "epoch 21: loss=1.5045270919799805\n",
      "epoch 22: loss=1.4383541345596313\n",
      "epoch 23: loss=1.375869631767273\n",
      "epoch 24: loss=1.293513298034668\n",
      "epoch 25: loss=1.2287503480911255\n",
      "epoch 26: loss=1.1744205951690674\n",
      "epoch 27: loss=1.1296461820602417\n",
      "epoch 28: loss=1.1029096841812134\n",
      "epoch 29: loss=1.0890575647354126\n",
      "epoch 30: loss=1.0712764263153076\n",
      "epoch 31: loss=1.055124282836914\n",
      "epoch 32: loss=1.0489948987960815\n",
      "epoch 33: loss=1.038521409034729\n",
      "epoch 34: loss=1.0280944108963013\n",
      "epoch 35: loss=1.0161200761795044\n",
      "epoch 36: loss=1.0076391696929932\n",
      "epoch 37: loss=1.003806710243225\n",
      "epoch 38: loss=0.997371256351471\n",
      "epoch 39: loss=0.9909902215003967\n",
      "epoch 40: loss=0.9865188002586365\n",
      "epoch 41: loss=0.9844900965690613\n",
      "epoch 42: loss=0.9809246063232422\n",
      "epoch 43: loss=0.982460081577301\n",
      "epoch 44: loss=0.9731680750846863\n",
      "epoch 45: loss=0.9772135019302368\n",
      "epoch 46: loss=0.9691647291183472\n",
      "epoch 47: loss=0.9698018431663513\n",
      "epoch 48: loss=0.9663444757461548\n",
      "epoch 49: loss=0.9644904732704163\n",
      "epoch 50: loss=0.9597806930541992\n",
      "epoch 51: loss=0.9575806260108948\n",
      "epoch 52: loss=0.9582203030586243\n",
      "epoch 53: loss=0.9591331481933594\n",
      "epoch 54: loss=0.9565503001213074\n",
      "epoch 55: loss=0.9546937942504883\n",
      "epoch 56: loss=0.9543517231941223\n",
      "epoch 57: loss=0.9567411541938782\n",
      "epoch 58: loss=0.9553180932998657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59: loss=0.9487964510917664\n",
      "epoch 60: loss=0.9458799958229065\n",
      "epoch 61: loss=0.9498593807220459\n",
      "epoch 62: loss=0.9499717950820923\n",
      "epoch 63: loss=0.9486620426177979\n",
      "epoch 64: loss=0.9475606083869934\n",
      "epoch 65: loss=0.9459503293037415\n",
      "epoch 66: loss=0.9443671107292175\n",
      "epoch 67: loss=0.944318413734436\n",
      "epoch 68: loss=0.9420345425605774\n",
      "epoch 69: loss=0.9414994120597839\n",
      "epoch 70: loss=0.9421549439430237\n",
      "epoch 71: loss=0.9420377612113953\n",
      "epoch 72: loss=0.9399363994598389\n",
      "epoch 73: loss=0.9399306774139404\n",
      "epoch 74: loss=0.9383929371833801\n",
      "epoch 75: loss=0.93724125623703\n",
      "epoch 76: loss=0.9362534880638123\n",
      "epoch 77: loss=0.9368366003036499\n",
      "epoch 78: loss=0.9358459115028381\n",
      "epoch 79: loss=0.9350661039352417\n",
      "epoch 80: loss=0.9338799118995667\n",
      "epoch 81: loss=0.9354702234268188\n",
      "epoch 82: loss=0.9320932626724243\n",
      "epoch 83: loss=0.9352357387542725\n",
      "epoch 84: loss=0.9302027821540833\n",
      "epoch 85: loss=0.9293634295463562\n",
      "epoch 86: loss=0.9314132332801819\n",
      "epoch 87: loss=0.9301398396492004\n",
      "epoch 88: loss=0.9294903874397278\n",
      "epoch 89: loss=0.9270641207695007\n",
      "epoch 90: loss=0.9293830990791321\n",
      "epoch 91: loss=0.9241470098495483\n",
      "epoch 92: loss=0.9271050095558167\n",
      "epoch 93: loss=0.9283078908920288\n",
      "epoch 94: loss=0.9233537316322327\n",
      "epoch 95: loss=0.9239566326141357\n",
      "epoch 96: loss=0.9247723817825317\n",
      "epoch 97: loss=0.9238173365592957\n",
      "epoch 98: loss=0.9234955906867981\n",
      "epoch 99: loss=0.9242916107177734\n",
      "epoch 100: loss=0.9200629591941833\n",
      "epoch 101: loss=0.923886239528656\n",
      "epoch 102: loss=0.9216397404670715\n",
      "epoch 103: loss=0.919277548789978\n",
      "epoch 104: loss=0.9242762923240662\n",
      "epoch 105: loss=0.9182900786399841\n",
      "epoch 106: loss=0.9192846417427063\n",
      "epoch 107: loss=0.9177420735359192\n",
      "epoch 108: loss=0.9207703471183777\n",
      "epoch 109: loss=0.9160365462303162\n",
      "epoch 110: loss=0.91319340467453\n",
      "epoch 111: loss=0.9173670411109924\n",
      "epoch 112: loss=0.9173662662506104\n",
      "epoch 113: loss=0.9145780801773071\n",
      "epoch 114: loss=0.9165189266204834\n",
      "epoch 115: loss=0.9143590331077576\n",
      "epoch 116: loss=0.914818286895752\n",
      "epoch 117: loss=0.9129350185394287\n",
      "epoch 118: loss=0.9122310280799866\n",
      "epoch 119: loss=0.9095483422279358\n",
      "epoch 120: loss=0.9123308658599854\n",
      "epoch 121: loss=0.913571834564209\n",
      "epoch 122: loss=0.9112097024917603\n",
      "epoch 123: loss=0.9095021486282349\n",
      "epoch 124: loss=0.9113866686820984\n",
      "epoch 125: loss=0.9089109897613525\n",
      "epoch 126: loss=0.9087631702423096\n",
      "epoch 127: loss=0.9090850949287415\n",
      "epoch 128: loss=0.9096569418907166\n",
      "epoch 129: loss=0.9103529453277588\n",
      "epoch 130: loss=0.9060836434364319\n",
      "epoch 131: loss=0.9064384698867798\n",
      "epoch 132: loss=0.9072644710540771\n",
      "epoch 133: loss=0.9055333137512207\n",
      "epoch 134: loss=0.9038726091384888\n",
      "epoch 135: loss=0.9046264886856079\n",
      "epoch 136: loss=0.9078169465065002\n",
      "epoch 137: loss=0.9039259552955627\n",
      "epoch 138: loss=0.9026187062263489\n",
      "epoch 139: loss=0.902187168598175\n",
      "epoch 140: loss=0.9018810987472534\n",
      "epoch 141: loss=0.9035582542419434\n",
      "epoch 142: loss=0.9004621505737305\n",
      "epoch 143: loss=0.8996990323066711\n",
      "epoch 144: loss=0.9026454091072083\n",
      "epoch 145: loss=0.8986214995384216\n",
      "epoch 146: loss=0.9004160761833191\n",
      "epoch 147: loss=0.8988347053527832\n",
      "epoch 148: loss=0.8974202871322632\n",
      "epoch 149: loss=0.8942537903785706\n",
      "epoch 150: loss=0.8976026773452759\n",
      "epoch 151: loss=0.8974277377128601\n",
      "epoch 152: loss=0.8942707180976868\n",
      "epoch 153: loss=0.8930230736732483\n",
      "epoch 154: loss=0.8952943682670593\n",
      "epoch 155: loss=0.893268883228302\n",
      "epoch 156: loss=0.8946813344955444\n",
      "epoch 157: loss=0.893174409866333\n",
      "epoch 158: loss=0.8910122513771057\n",
      "epoch 159: loss=0.8899486660957336\n",
      "epoch 160: loss=0.8916394114494324\n",
      "epoch 161: loss=0.8897597789764404\n",
      "epoch 162: loss=0.8850946426391602\n",
      "epoch 163: loss=0.8887410163879395\n",
      "epoch 164: loss=0.8887577056884766\n",
      "epoch 165: loss=0.8874874711036682\n",
      "epoch 166: loss=0.8845359683036804\n",
      "epoch 167: loss=0.8849985003471375\n",
      "epoch 168: loss=0.8815220594406128\n",
      "epoch 169: loss=0.8825295567512512\n",
      "epoch 170: loss=0.8825750350952148\n",
      "epoch 171: loss=0.8826636075973511\n",
      "epoch 172: loss=0.8805112242698669\n",
      "epoch 173: loss=0.878156840801239\n",
      "epoch 174: loss=0.8848382234573364\n",
      "epoch 175: loss=0.8828445672988892\n",
      "epoch 176: loss=0.878402829170227\n",
      "epoch 177: loss=0.8792359232902527\n",
      "epoch 178: loss=0.8801283836364746\n",
      "epoch 179: loss=0.8809788227081299\n",
      "epoch 180: loss=0.8779565691947937\n",
      "epoch 181: loss=0.8778277039527893\n",
      "epoch 182: loss=0.8738921880722046\n",
      "epoch 183: loss=0.8772943615913391\n",
      "epoch 184: loss=0.8779270052909851\n",
      "epoch 185: loss=0.8783804178237915\n",
      "epoch 186: loss=0.8742740750312805\n",
      "epoch 187: loss=0.8770150542259216\n",
      "epoch 188: loss=0.8739675283432007\n",
      "epoch 189: loss=0.8758735060691833\n",
      "epoch 190: loss=0.8693531155586243\n",
      "epoch 191: loss=0.8775714635848999\n",
      "epoch 192: loss=0.8770754337310791\n",
      "epoch 193: loss=0.8754957318305969\n",
      "epoch 194: loss=0.8737698793411255\n",
      "epoch 195: loss=0.8722494840621948\n",
      "epoch 196: loss=0.8760107159614563\n",
      "epoch 197: loss=0.8706891536712646\n",
      "epoch 198: loss=0.8707159757614136\n",
      "epoch 199: loss=0.8712445497512817\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=7.089200973510742\n",
      "epoch 1: loss=7.07453727722168\n",
      "epoch 2: loss=7.054301738739014\n",
      "epoch 3: loss=5.817129611968994\n",
      "epoch 4: loss=6.104164123535156\n",
      "epoch 5: loss=5.513067245483398\n",
      "epoch 6: loss=5.164440155029297\n",
      "epoch 7: loss=5.1429877281188965\n",
      "epoch 8: loss=4.593995571136475\n",
      "epoch 9: loss=4.665222644805908\n",
      "epoch 10: loss=3.521587371826172\n",
      "epoch 11: loss=3.5753238201141357\n",
      "epoch 12: loss=3.3314108848571777\n",
      "epoch 13: loss=3.411712884902954\n",
      "epoch 14: loss=3.479036331176758\n",
      "epoch 15: loss=3.1091880798339844\n",
      "epoch 16: loss=2.7555253505706787\n",
      "epoch 17: loss=2.9012911319732666\n",
      "epoch 18: loss=2.4248239994049072\n",
      "epoch 19: loss=2.305208444595337\n",
      "epoch 20: loss=2.220111608505249\n",
      "epoch 21: loss=1.9685536623001099\n",
      "epoch 22: loss=1.6674736738204956\n",
      "epoch 23: loss=1.7919609546661377\n",
      "epoch 24: loss=1.8075519800186157\n",
      "epoch 25: loss=1.5366125106811523\n",
      "epoch 26: loss=1.6913257837295532\n",
      "epoch 27: loss=1.6517895460128784\n",
      "epoch 28: loss=1.555559515953064\n",
      "epoch 29: loss=1.642930030822754\n",
      "epoch 30: loss=1.6497784852981567\n",
      "epoch 31: loss=1.558126449584961\n",
      "epoch 32: loss=1.5527472496032715\n",
      "epoch 33: loss=1.5562142133712769\n",
      "epoch 34: loss=1.6817315816879272\n",
      "epoch 35: loss=1.4622777700424194\n",
      "epoch 36: loss=1.5023715496063232\n",
      "epoch 37: loss=1.5411369800567627\n",
      "epoch 38: loss=1.504211664199829\n",
      "epoch 39: loss=1.4710133075714111\n",
      "epoch 40: loss=1.4664475917816162\n",
      "epoch 41: loss=1.4560030698776245\n",
      "epoch 42: loss=1.4494582414627075\n",
      "epoch 43: loss=1.5649417638778687\n",
      "epoch 44: loss=1.392812967300415\n",
      "epoch 45: loss=1.4923208951950073\n",
      "epoch 46: loss=1.4976533651351929\n",
      "epoch 47: loss=1.4412490129470825\n",
      "epoch 48: loss=1.6616888046264648\n",
      "epoch 49: loss=1.5400474071502686\n",
      "epoch 50: loss=1.6120063066482544\n",
      "epoch 51: loss=1.3881239891052246\n",
      "epoch 52: loss=1.4805798530578613\n",
      "epoch 53: loss=1.4588818550109863\n",
      "epoch 54: loss=1.5283210277557373\n",
      "epoch 55: loss=1.5686259269714355\n",
      "epoch 56: loss=1.5774939060211182\n",
      "epoch 57: loss=1.4017413854599\n",
      "epoch 58: loss=1.5021806955337524\n",
      "epoch 59: loss=1.3925970792770386\n",
      "epoch 60: loss=1.5286988019943237\n",
      "epoch 61: loss=1.442400574684143\n",
      "epoch 62: loss=1.4723855257034302\n",
      "epoch 63: loss=1.5368753671646118\n",
      "epoch 64: loss=1.4597288370132446\n",
      "epoch 65: loss=1.4135037660598755\n",
      "epoch 66: loss=1.51197350025177\n",
      "epoch 67: loss=1.452128529548645\n",
      "epoch 68: loss=1.4776465892791748\n",
      "epoch 69: loss=1.4585572481155396\n",
      "epoch 70: loss=1.486831784248352\n",
      "epoch 71: loss=1.4684927463531494\n",
      "epoch 72: loss=1.4029909372329712\n",
      "epoch 73: loss=1.4360309839248657\n",
      "epoch 74: loss=1.396965742111206\n",
      "epoch 75: loss=1.4179924726486206\n",
      "epoch 76: loss=1.4319968223571777\n",
      "epoch 77: loss=1.4780173301696777\n",
      "epoch 78: loss=1.5717135667800903\n",
      "epoch 79: loss=1.5153495073318481\n",
      "epoch 80: loss=1.3650152683258057\n",
      "epoch 81: loss=1.5186299085617065\n",
      "epoch 82: loss=1.5215295553207397\n",
      "epoch 83: loss=1.4138801097869873\n",
      "epoch 84: loss=1.4432318210601807\n",
      "epoch 85: loss=1.4793235063552856\n",
      "epoch 86: loss=1.5008829832077026\n",
      "epoch 87: loss=1.492478609085083\n",
      "epoch 88: loss=1.3978581428527832\n",
      "epoch 89: loss=1.464568853378296\n",
      "epoch 90: loss=1.380999207496643\n",
      "epoch 91: loss=1.4676265716552734\n",
      "epoch 92: loss=1.4735114574432373\n",
      "epoch 93: loss=1.46891188621521\n",
      "epoch 94: loss=1.4007989168167114\n",
      "epoch 95: loss=1.4845402240753174\n",
      "epoch 96: loss=1.4591952562332153\n",
      "epoch 97: loss=1.5903956890106201\n",
      "epoch 98: loss=1.413439154624939\n",
      "epoch 99: loss=1.3788342475891113\n",
      "epoch 100: loss=1.4877216815948486\n",
      "epoch 101: loss=1.4419145584106445\n",
      "epoch 102: loss=1.4990637302398682\n",
      "epoch 103: loss=1.484034538269043\n",
      "epoch 104: loss=1.511526346206665\n",
      "epoch 105: loss=1.4411120414733887\n",
      "epoch 106: loss=1.435262680053711\n",
      "epoch 107: loss=1.5551221370697021\n",
      "epoch 108: loss=1.3935606479644775\n",
      "epoch 109: loss=1.401324987411499\n",
      "epoch 110: loss=1.4832712411880493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 111: loss=1.4484035968780518\n",
      "epoch 112: loss=1.4566128253936768\n",
      "epoch 113: loss=1.3462237119674683\n",
      "epoch 114: loss=1.513184905052185\n",
      "epoch 115: loss=1.5490446090698242\n",
      "epoch 116: loss=1.4603984355926514\n",
      "epoch 117: loss=1.4899165630340576\n",
      "epoch 118: loss=1.513071060180664\n",
      "epoch 119: loss=1.454363465309143\n",
      "epoch 120: loss=1.4597452878952026\n",
      "epoch 121: loss=1.5425221920013428\n",
      "epoch 122: loss=1.445067048072815\n",
      "epoch 123: loss=1.4107251167297363\n",
      "epoch 124: loss=1.5509867668151855\n",
      "epoch 125: loss=1.3896225690841675\n",
      "epoch 126: loss=1.5487898588180542\n",
      "epoch 127: loss=1.4797546863555908\n",
      "epoch 128: loss=1.3741686344146729\n",
      "epoch 129: loss=1.4361566305160522\n",
      "epoch 130: loss=1.4318886995315552\n",
      "epoch 131: loss=1.4630099534988403\n",
      "epoch 132: loss=1.4516650438308716\n",
      "epoch 133: loss=1.4550188779830933\n",
      "epoch 134: loss=1.4560935497283936\n",
      "epoch 135: loss=1.4934228658676147\n",
      "epoch 136: loss=1.4532530307769775\n",
      "epoch 137: loss=1.4064819812774658\n",
      "epoch 138: loss=1.3962376117706299\n",
      "epoch 139: loss=1.473374605178833\n",
      "epoch 140: loss=1.44247305393219\n",
      "epoch 141: loss=1.4369752407073975\n",
      "epoch 142: loss=1.54816472530365\n",
      "epoch 143: loss=1.4531447887420654\n",
      "epoch 144: loss=1.4828895330429077\n",
      "epoch 145: loss=1.403212547302246\n",
      "epoch 146: loss=1.4263392686843872\n",
      "epoch 147: loss=1.4626110792160034\n",
      "epoch 148: loss=1.3995758295059204\n",
      "epoch 149: loss=1.4414396286010742\n",
      "epoch 150: loss=1.5104048252105713\n",
      "epoch 151: loss=1.4623417854309082\n",
      "epoch 152: loss=1.4520642757415771\n",
      "epoch 153: loss=1.3887760639190674\n",
      "epoch 154: loss=1.4895355701446533\n",
      "epoch 155: loss=1.4482816457748413\n",
      "epoch 156: loss=1.337563395500183\n",
      "epoch 157: loss=1.4502595663070679\n",
      "epoch 158: loss=1.4066309928894043\n",
      "epoch 159: loss=1.471922755241394\n",
      "epoch 160: loss=1.436607837677002\n",
      "epoch 161: loss=1.4884042739868164\n",
      "epoch 162: loss=1.3529865741729736\n",
      "epoch 163: loss=1.451348066329956\n",
      "epoch 164: loss=1.4609938859939575\n",
      "epoch 165: loss=1.4974030256271362\n",
      "epoch 166: loss=1.444716453552246\n",
      "epoch 167: loss=1.52247154712677\n",
      "epoch 168: loss=1.4610211849212646\n",
      "epoch 169: loss=1.4018739461898804\n",
      "epoch 170: loss=1.4674588441848755\n",
      "epoch 171: loss=1.4637670516967773\n",
      "epoch 172: loss=1.4491374492645264\n",
      "epoch 173: loss=1.4601675271987915\n",
      "epoch 174: loss=1.4567785263061523\n",
      "epoch 175: loss=1.4572535753250122\n",
      "epoch 176: loss=1.4410890340805054\n",
      "epoch 177: loss=1.377456784248352\n",
      "epoch 178: loss=1.4240922927856445\n",
      "epoch 179: loss=1.4772515296936035\n",
      "epoch 180: loss=1.4627453088760376\n",
      "epoch 181: loss=1.4306155443191528\n",
      "epoch 182: loss=1.454645037651062\n",
      "epoch 183: loss=1.5242931842803955\n",
      "epoch 184: loss=1.4719035625457764\n",
      "epoch 185: loss=1.5104249715805054\n",
      "epoch 186: loss=1.4282712936401367\n",
      "epoch 187: loss=1.4311234951019287\n",
      "epoch 188: loss=1.4730242490768433\n",
      "epoch 189: loss=1.4449666738510132\n",
      "epoch 190: loss=1.459532380104065\n",
      "epoch 191: loss=1.4536480903625488\n",
      "epoch 192: loss=1.3996907472610474\n",
      "epoch 193: loss=1.4935485124588013\n",
      "epoch 194: loss=1.4542654752731323\n",
      "epoch 195: loss=1.4883520603179932\n",
      "epoch 196: loss=1.4609876871109009\n",
      "epoch 197: loss=1.4581656455993652\n",
      "epoch 198: loss=1.4431508779525757\n",
      "epoch 199: loss=1.4246314764022827\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=6.7393670082092285\n",
      "epoch 1: loss=6.5016045570373535\n",
      "epoch 2: loss=6.162996768951416\n",
      "epoch 3: loss=5.760314464569092\n",
      "epoch 4: loss=5.1574296951293945\n",
      "epoch 5: loss=4.521403789520264\n",
      "epoch 6: loss=3.974946975708008\n",
      "epoch 7: loss=3.5897603034973145\n",
      "epoch 8: loss=3.522404909133911\n",
      "epoch 9: loss=3.424590587615967\n",
      "epoch 10: loss=3.265556812286377\n",
      "epoch 11: loss=3.0624608993530273\n",
      "epoch 12: loss=2.785630464553833\n",
      "epoch 13: loss=2.5362491607666016\n",
      "epoch 14: loss=2.3402976989746094\n",
      "epoch 15: loss=2.175767660140991\n",
      "epoch 16: loss=2.0582430362701416\n",
      "epoch 17: loss=1.9787976741790771\n",
      "epoch 18: loss=1.902706265449524\n",
      "epoch 19: loss=1.8441554307937622\n",
      "epoch 20: loss=1.7919374704360962\n",
      "epoch 21: loss=1.726394772529602\n",
      "epoch 22: loss=1.6567168235778809\n",
      "epoch 23: loss=1.5922794342041016\n",
      "epoch 24: loss=1.5198156833648682\n",
      "epoch 25: loss=1.4595900774002075\n",
      "epoch 26: loss=1.395728349685669\n",
      "epoch 27: loss=1.3289098739624023\n",
      "epoch 28: loss=1.2673126459121704\n",
      "epoch 29: loss=1.2020578384399414\n",
      "epoch 30: loss=1.1457737684249878\n",
      "epoch 31: loss=1.1064716577529907\n",
      "epoch 32: loss=1.0710583925247192\n",
      "epoch 33: loss=1.0512906312942505\n",
      "epoch 34: loss=1.0318411588668823\n",
      "epoch 35: loss=1.0272983312606812\n",
      "epoch 36: loss=1.0073208808898926\n",
      "epoch 37: loss=0.9895840287208557\n",
      "epoch 38: loss=0.9699281454086304\n",
      "epoch 39: loss=0.9512913823127747\n",
      "epoch 40: loss=0.935397207736969\n",
      "epoch 41: loss=0.9239518046379089\n",
      "epoch 42: loss=0.9216379523277283\n",
      "epoch 43: loss=0.9184322357177734\n",
      "epoch 44: loss=0.9143106341362\n",
      "epoch 45: loss=0.909209668636322\n",
      "epoch 46: loss=0.9062489867210388\n",
      "epoch 47: loss=0.8998972773551941\n",
      "epoch 48: loss=0.8929915428161621\n",
      "epoch 49: loss=0.8871352672576904\n",
      "epoch 50: loss=0.8820062279701233\n",
      "epoch 51: loss=0.8808649778366089\n",
      "epoch 52: loss=0.8781960606575012\n",
      "epoch 53: loss=0.8757171630859375\n",
      "epoch 54: loss=0.8752381801605225\n",
      "epoch 55: loss=0.8734533786773682\n",
      "epoch 56: loss=0.8707702159881592\n",
      "epoch 57: loss=0.8685453534126282\n",
      "epoch 58: loss=0.8665630221366882\n",
      "epoch 59: loss=0.864941418170929\n",
      "epoch 60: loss=0.8637738227844238\n",
      "epoch 61: loss=0.8629284501075745\n",
      "epoch 62: loss=0.8608684539794922\n",
      "epoch 63: loss=0.8606613874435425\n",
      "epoch 64: loss=0.8581341505050659\n",
      "epoch 65: loss=0.8576866388320923\n",
      "epoch 66: loss=0.8562505841255188\n",
      "epoch 67: loss=0.8559019565582275\n",
      "epoch 68: loss=0.8579551577568054\n",
      "epoch 69: loss=0.8554948568344116\n",
      "epoch 70: loss=0.8537314534187317\n",
      "epoch 71: loss=0.8534187078475952\n",
      "epoch 72: loss=0.852700412273407\n",
      "epoch 73: loss=0.8516838550567627\n",
      "epoch 74: loss=0.8516818881034851\n",
      "epoch 75: loss=0.8517568111419678\n",
      "epoch 76: loss=0.8502731323242188\n",
      "epoch 77: loss=0.8498451113700867\n",
      "epoch 78: loss=0.8494464159011841\n",
      "epoch 79: loss=0.8477677702903748\n",
      "epoch 80: loss=0.8466943502426147\n",
      "epoch 81: loss=0.8466712236404419\n",
      "epoch 82: loss=0.8466327786445618\n",
      "epoch 83: loss=0.8464908003807068\n",
      "epoch 84: loss=0.8451710343360901\n",
      "epoch 85: loss=0.8452673554420471\n",
      "epoch 86: loss=0.8448613882064819\n",
      "epoch 87: loss=0.8437151312828064\n",
      "epoch 88: loss=0.8431912660598755\n",
      "epoch 89: loss=0.8414972424507141\n",
      "epoch 90: loss=0.8420912027359009\n",
      "epoch 91: loss=0.8419482111930847\n",
      "epoch 92: loss=0.8421797156333923\n",
      "epoch 93: loss=0.8405891060829163\n",
      "epoch 94: loss=0.841758668422699\n",
      "epoch 95: loss=0.839805543422699\n",
      "epoch 96: loss=0.8387409448623657\n",
      "epoch 97: loss=0.8401798605918884\n",
      "epoch 98: loss=0.8395715951919556\n",
      "epoch 99: loss=0.8386796712875366\n",
      "epoch 100: loss=0.8391703367233276\n",
      "epoch 101: loss=0.8379444479942322\n",
      "epoch 102: loss=0.8373432159423828\n",
      "epoch 103: loss=0.8377163410186768\n",
      "epoch 104: loss=0.837609052658081\n",
      "epoch 105: loss=0.8370484113693237\n",
      "epoch 106: loss=0.8357527256011963\n",
      "epoch 107: loss=0.8358965516090393\n",
      "epoch 108: loss=0.8350701332092285\n",
      "epoch 109: loss=0.8351203799247742\n",
      "epoch 110: loss=0.8355816006660461\n",
      "epoch 111: loss=0.8343427777290344\n",
      "epoch 112: loss=0.8348889350891113\n",
      "epoch 113: loss=0.8340452909469604\n",
      "epoch 114: loss=0.8338178992271423\n",
      "epoch 115: loss=0.8330029845237732\n",
      "epoch 116: loss=0.8348746299743652\n",
      "epoch 117: loss=0.8344571590423584\n",
      "epoch 118: loss=0.8322175145149231\n",
      "epoch 119: loss=0.8327200412750244\n",
      "epoch 120: loss=0.8324699997901917\n",
      "epoch 121: loss=0.8326230645179749\n",
      "epoch 122: loss=0.8327767848968506\n",
      "epoch 123: loss=0.8324633240699768\n",
      "epoch 124: loss=0.8319489359855652\n",
      "epoch 125: loss=0.8319701552391052\n",
      "epoch 126: loss=0.8316799998283386\n",
      "epoch 127: loss=0.8308740258216858\n",
      "epoch 128: loss=0.8307749629020691\n",
      "epoch 129: loss=0.8309739828109741\n",
      "epoch 130: loss=0.8318813443183899\n",
      "epoch 131: loss=0.8303866386413574\n",
      "epoch 132: loss=0.8301627039909363\n",
      "epoch 133: loss=0.8298053741455078\n",
      "epoch 134: loss=0.8290224671363831\n",
      "epoch 135: loss=0.8288424015045166\n",
      "epoch 136: loss=0.8285492658615112\n",
      "epoch 137: loss=0.8292009234428406\n",
      "epoch 138: loss=0.8291411995887756\n",
      "epoch 139: loss=0.8289375901222229\n",
      "epoch 140: loss=0.829146146774292\n",
      "epoch 141: loss=0.8288504481315613\n",
      "epoch 142: loss=0.8282888531684875\n",
      "epoch 143: loss=0.8287661671638489\n",
      "epoch 144: loss=0.8291718363761902\n",
      "epoch 145: loss=0.8259913921356201\n",
      "epoch 146: loss=0.8255209922790527\n",
      "epoch 147: loss=0.828002393245697\n",
      "epoch 148: loss=0.8265092372894287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 149: loss=0.8279973268508911\n",
      "epoch 150: loss=0.8255994319915771\n",
      "epoch 151: loss=0.8254586458206177\n",
      "epoch 152: loss=0.8250732421875\n",
      "epoch 153: loss=0.8271328210830688\n",
      "epoch 154: loss=0.8249356150627136\n",
      "epoch 155: loss=0.8247010707855225\n",
      "epoch 156: loss=0.8250728249549866\n",
      "epoch 157: loss=0.8243879675865173\n",
      "epoch 158: loss=0.8255578279495239\n",
      "epoch 159: loss=0.8243386149406433\n",
      "epoch 160: loss=0.8251947164535522\n",
      "epoch 161: loss=0.8241406083106995\n",
      "epoch 162: loss=0.8238189220428467\n",
      "epoch 163: loss=0.8236696720123291\n",
      "epoch 164: loss=0.824478030204773\n",
      "epoch 165: loss=0.8230920433998108\n",
      "epoch 166: loss=0.8245419263839722\n",
      "epoch 167: loss=0.8252276182174683\n",
      "epoch 168: loss=0.8222423791885376\n",
      "epoch 169: loss=0.8236293792724609\n",
      "epoch 170: loss=0.8226020336151123\n",
      "epoch 171: loss=0.8231065273284912\n",
      "epoch 172: loss=0.8239754438400269\n",
      "epoch 173: loss=0.8237294554710388\n",
      "epoch 174: loss=0.8222900032997131\n",
      "epoch 175: loss=0.8218558430671692\n",
      "epoch 176: loss=0.8222993016242981\n",
      "epoch 177: loss=0.8205651640892029\n",
      "epoch 178: loss=0.819557785987854\n",
      "epoch 179: loss=0.8208792805671692\n",
      "epoch 180: loss=0.8223208785057068\n",
      "epoch 181: loss=0.8211233615875244\n",
      "epoch 182: loss=0.8201335072517395\n",
      "epoch 183: loss=0.8214316964149475\n",
      "epoch 184: loss=0.820907711982727\n",
      "epoch 185: loss=0.8207501769065857\n",
      "epoch 186: loss=0.8207598924636841\n",
      "epoch 187: loss=0.8181830644607544\n",
      "epoch 188: loss=0.8189141750335693\n",
      "epoch 189: loss=0.8177434206008911\n",
      "epoch 190: loss=0.8182874321937561\n",
      "epoch 191: loss=0.8168601393699646\n",
      "epoch 192: loss=0.816635251045227\n",
      "epoch 193: loss=0.8162803649902344\n",
      "epoch 194: loss=0.816423773765564\n",
      "epoch 195: loss=0.8154380917549133\n",
      "epoch 196: loss=0.8166645765304565\n",
      "epoch 197: loss=0.8149741888046265\n",
      "epoch 198: loss=0.8149940371513367\n",
      "epoch 199: loss=0.8130739331245422\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=6.690537452697754\n",
      "epoch 1: loss=6.373047351837158\n",
      "epoch 2: loss=6.007043361663818\n",
      "epoch 3: loss=5.555896759033203\n",
      "epoch 4: loss=5.015811443328857\n",
      "epoch 5: loss=4.318469524383545\n",
      "epoch 6: loss=3.788526773452759\n",
      "epoch 7: loss=3.5102059841156006\n",
      "epoch 8: loss=3.3813836574554443\n",
      "epoch 9: loss=3.273123025894165\n",
      "epoch 10: loss=3.0506601333618164\n",
      "epoch 11: loss=2.7645657062530518\n",
      "epoch 12: loss=2.4952938556671143\n",
      "epoch 13: loss=2.280818462371826\n",
      "epoch 14: loss=2.0625882148742676\n",
      "epoch 15: loss=1.929456353187561\n",
      "epoch 16: loss=1.8300398588180542\n",
      "epoch 17: loss=1.7610557079315186\n",
      "epoch 18: loss=1.709824800491333\n",
      "epoch 19: loss=1.6292613744735718\n",
      "epoch 20: loss=1.5599477291107178\n",
      "epoch 21: loss=1.472758173942566\n",
      "epoch 22: loss=1.4059211015701294\n",
      "epoch 23: loss=1.3195984363555908\n",
      "epoch 24: loss=1.266600489616394\n",
      "epoch 25: loss=1.206709384918213\n",
      "epoch 26: loss=1.1459053754806519\n",
      "epoch 27: loss=1.0985380411148071\n",
      "epoch 28: loss=1.0553417205810547\n",
      "epoch 29: loss=1.030898094177246\n",
      "epoch 30: loss=1.0135631561279297\n",
      "epoch 31: loss=0.9992260932922363\n",
      "epoch 32: loss=0.9940617084503174\n",
      "epoch 33: loss=0.9864990711212158\n",
      "epoch 34: loss=0.9810172915458679\n",
      "epoch 35: loss=0.9680460691452026\n",
      "epoch 36: loss=0.9541934132575989\n",
      "epoch 37: loss=0.9389599561691284\n",
      "epoch 38: loss=0.9261173605918884\n",
      "epoch 39: loss=0.9182938933372498\n",
      "epoch 40: loss=0.915525496006012\n",
      "epoch 41: loss=0.9119556546211243\n",
      "epoch 42: loss=0.9096508026123047\n",
      "epoch 43: loss=0.9056885838508606\n",
      "epoch 44: loss=0.9011130332946777\n",
      "epoch 45: loss=0.8996902108192444\n",
      "epoch 46: loss=0.8978340029716492\n",
      "epoch 47: loss=0.8938801288604736\n",
      "epoch 48: loss=0.8914714455604553\n",
      "epoch 49: loss=0.8875622749328613\n",
      "epoch 50: loss=0.8874123096466064\n",
      "epoch 51: loss=0.8862863183021545\n",
      "epoch 52: loss=0.884128987789154\n",
      "epoch 53: loss=0.8821421265602112\n",
      "epoch 54: loss=0.8819944858551025\n",
      "epoch 55: loss=0.879265546798706\n",
      "epoch 56: loss=0.8791486024856567\n",
      "epoch 57: loss=0.8792104125022888\n",
      "epoch 58: loss=0.877054750919342\n",
      "epoch 59: loss=0.8764981031417847\n",
      "epoch 60: loss=0.8767412900924683\n",
      "epoch 61: loss=0.8743961453437805\n",
      "epoch 62: loss=0.873939573764801\n",
      "epoch 63: loss=0.8731628060340881\n",
      "epoch 64: loss=0.8719860911369324\n",
      "epoch 65: loss=0.8727085590362549\n",
      "epoch 66: loss=0.871959388256073\n",
      "epoch 67: loss=0.8721898198127747\n",
      "epoch 68: loss=0.8717084527015686\n",
      "epoch 69: loss=0.8689732551574707\n",
      "epoch 70: loss=0.8700650334358215\n",
      "epoch 71: loss=0.8684528470039368\n",
      "epoch 72: loss=0.8684058785438538\n",
      "epoch 73: loss=0.8674731850624084\n",
      "epoch 74: loss=0.8671575784683228\n",
      "epoch 75: loss=0.8669337034225464\n",
      "epoch 76: loss=0.8670819401741028\n",
      "epoch 77: loss=0.8673030734062195\n",
      "epoch 78: loss=0.8680543899536133\n",
      "epoch 79: loss=0.8654250502586365\n",
      "epoch 80: loss=0.8679490685462952\n",
      "epoch 81: loss=0.8663837909698486\n",
      "epoch 82: loss=0.8664209842681885\n",
      "epoch 83: loss=0.8660058975219727\n",
      "epoch 84: loss=0.864555299282074\n",
      "epoch 85: loss=0.8645087480545044\n",
      "epoch 86: loss=0.8647966384887695\n",
      "epoch 87: loss=0.8641468286514282\n",
      "epoch 88: loss=0.8626424670219421\n",
      "epoch 89: loss=0.8621891736984253\n",
      "epoch 90: loss=0.8623208403587341\n",
      "epoch 91: loss=0.8629384636878967\n",
      "epoch 92: loss=0.8614522218704224\n",
      "epoch 93: loss=0.8622448444366455\n",
      "epoch 94: loss=0.8623749613761902\n",
      "epoch 95: loss=0.8612344861030579\n",
      "epoch 96: loss=0.8621012568473816\n",
      "epoch 97: loss=0.8607282042503357\n",
      "epoch 98: loss=0.8607144951820374\n",
      "epoch 99: loss=0.8607339859008789\n",
      "epoch 100: loss=0.8605796694755554\n",
      "epoch 101: loss=0.8598940968513489\n",
      "epoch 102: loss=0.8602164387702942\n",
      "epoch 103: loss=0.8596778512001038\n",
      "epoch 104: loss=0.8610489368438721\n",
      "epoch 105: loss=0.8595078587532043\n",
      "epoch 106: loss=0.8597984313964844\n",
      "epoch 107: loss=0.858467698097229\n",
      "epoch 108: loss=0.8588611483573914\n",
      "epoch 109: loss=0.8572126626968384\n",
      "epoch 110: loss=0.8575230240821838\n",
      "epoch 111: loss=0.8570342659950256\n",
      "epoch 112: loss=0.8583594560623169\n",
      "epoch 113: loss=0.8556371331214905\n",
      "epoch 114: loss=0.8562455177307129\n",
      "epoch 115: loss=0.8575850129127502\n",
      "epoch 116: loss=0.855508029460907\n",
      "epoch 117: loss=0.8554248213768005\n",
      "epoch 118: loss=0.8572839498519897\n",
      "epoch 119: loss=0.8546277284622192\n",
      "epoch 120: loss=0.8552561402320862\n",
      "epoch 121: loss=0.8552526831626892\n",
      "epoch 122: loss=0.8559353947639465\n",
      "epoch 123: loss=0.8535503149032593\n",
      "epoch 124: loss=0.8545270562171936\n",
      "epoch 125: loss=0.8527688384056091\n",
      "epoch 126: loss=0.8545566201210022\n",
      "epoch 127: loss=0.853122353553772\n",
      "epoch 128: loss=0.8531569242477417\n",
      "epoch 129: loss=0.8535643815994263\n",
      "epoch 130: loss=0.8516069054603577\n",
      "epoch 131: loss=0.851853609085083\n",
      "epoch 132: loss=0.8521522283554077\n",
      "epoch 133: loss=0.8520474433898926\n",
      "epoch 134: loss=0.8504735827445984\n",
      "epoch 135: loss=0.8513475060462952\n",
      "epoch 136: loss=0.8487250208854675\n",
      "epoch 137: loss=0.8509424328804016\n",
      "epoch 138: loss=0.8493794202804565\n",
      "epoch 139: loss=0.8497211933135986\n",
      "epoch 140: loss=0.8484951853752136\n",
      "epoch 141: loss=0.8486031293869019\n",
      "epoch 142: loss=0.8493553400039673\n",
      "epoch 143: loss=0.846625030040741\n",
      "epoch 144: loss=0.8468123078346252\n",
      "epoch 145: loss=0.8478098511695862\n",
      "epoch 146: loss=0.8478460907936096\n",
      "epoch 147: loss=0.8480435609817505\n",
      "epoch 148: loss=0.8479152321815491\n",
      "epoch 149: loss=0.8455109000205994\n",
      "epoch 150: loss=0.8459142446517944\n",
      "epoch 151: loss=0.8466293811798096\n",
      "epoch 152: loss=0.8446399569511414\n",
      "epoch 153: loss=0.8461402654647827\n",
      "epoch 154: loss=0.8452180624008179\n",
      "epoch 155: loss=0.8428552150726318\n",
      "epoch 156: loss=0.8441280126571655\n",
      "epoch 157: loss=0.8434637188911438\n",
      "epoch 158: loss=0.8413466215133667\n",
      "epoch 159: loss=0.8435662388801575\n",
      "epoch 160: loss=0.8423296213150024\n",
      "epoch 161: loss=0.8434262871742249\n",
      "epoch 162: loss=0.8416461944580078\n",
      "epoch 163: loss=0.842121422290802\n",
      "epoch 164: loss=0.8398938179016113\n",
      "epoch 165: loss=0.8403444886207581\n",
      "epoch 166: loss=0.8409044146537781\n",
      "epoch 167: loss=0.8395865559577942\n",
      "epoch 168: loss=0.8404464721679688\n",
      "epoch 169: loss=0.8394085168838501\n",
      "epoch 170: loss=0.8384945392608643\n",
      "epoch 171: loss=0.840185821056366\n",
      "epoch 172: loss=0.8370885848999023\n",
      "epoch 173: loss=0.8374703526496887\n",
      "epoch 174: loss=0.8369669914245605\n",
      "epoch 175: loss=0.8360824584960938\n",
      "epoch 176: loss=0.8364535570144653\n",
      "epoch 177: loss=0.8350046277046204\n",
      "epoch 178: loss=0.8348174095153809\n",
      "epoch 179: loss=0.8352218270301819\n",
      "epoch 180: loss=0.8336689472198486\n",
      "epoch 181: loss=0.8336238265037537\n",
      "epoch 182: loss=0.8324940204620361\n",
      "epoch 183: loss=0.8323527574539185\n",
      "epoch 184: loss=0.8317117691040039\n",
      "epoch 185: loss=0.832240641117096\n",
      "epoch 186: loss=0.8317450881004333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 187: loss=0.8306207060813904\n",
      "epoch 188: loss=0.8311156630516052\n",
      "epoch 189: loss=0.8287643194198608\n",
      "epoch 190: loss=0.8305450081825256\n",
      "epoch 191: loss=0.8292291164398193\n",
      "epoch 192: loss=0.8279118537902832\n",
      "epoch 193: loss=0.8286811709403992\n",
      "epoch 194: loss=0.8275631070137024\n",
      "epoch 195: loss=0.826087236404419\n",
      "epoch 196: loss=0.8272078037261963\n",
      "epoch 197: loss=0.8273416757583618\n",
      "epoch 198: loss=0.8254770040512085\n",
      "epoch 199: loss=0.8264085054397583\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=6.687656879425049\n",
      "epoch 1: loss=6.500640869140625\n",
      "epoch 2: loss=6.173816680908203\n",
      "epoch 3: loss=5.823277473449707\n",
      "epoch 4: loss=5.376910209655762\n",
      "epoch 5: loss=4.927420139312744\n",
      "epoch 6: loss=4.3306403160095215\n",
      "epoch 7: loss=3.887451410293579\n",
      "epoch 8: loss=3.644544839859009\n",
      "epoch 9: loss=3.5341949462890625\n",
      "epoch 10: loss=3.452160596847534\n",
      "epoch 11: loss=3.2546916007995605\n",
      "epoch 12: loss=3.0208282470703125\n",
      "epoch 13: loss=2.7488768100738525\n",
      "epoch 14: loss=2.4991772174835205\n",
      "epoch 15: loss=2.28230619430542\n",
      "epoch 16: loss=2.141024351119995\n",
      "epoch 17: loss=2.030648708343506\n",
      "epoch 18: loss=1.9580984115600586\n",
      "epoch 19: loss=1.88141930103302\n",
      "epoch 20: loss=1.833593487739563\n",
      "epoch 21: loss=1.7632654905319214\n",
      "epoch 22: loss=1.6860991716384888\n",
      "epoch 23: loss=1.5876760482788086\n",
      "epoch 24: loss=1.5027939081192017\n",
      "epoch 25: loss=1.4333748817443848\n",
      "epoch 26: loss=1.3643614053726196\n",
      "epoch 27: loss=1.2954068183898926\n",
      "epoch 28: loss=1.2221558094024658\n",
      "epoch 29: loss=1.1607047319412231\n",
      "epoch 30: loss=1.1155915260314941\n",
      "epoch 31: loss=1.08018958568573\n",
      "epoch 32: loss=1.0484461784362793\n",
      "epoch 33: loss=1.0325071811676025\n",
      "epoch 34: loss=1.0266749858856201\n",
      "epoch 35: loss=1.0179599523544312\n",
      "epoch 36: loss=1.0008646249771118\n",
      "epoch 37: loss=0.9804006218910217\n",
      "epoch 38: loss=0.9634500741958618\n",
      "epoch 39: loss=0.9462243318557739\n",
      "epoch 40: loss=0.9381986260414124\n",
      "epoch 41: loss=0.9320365190505981\n",
      "epoch 42: loss=0.9273172616958618\n",
      "epoch 43: loss=0.9219691157341003\n",
      "epoch 44: loss=0.9230130314826965\n",
      "epoch 45: loss=0.9136542677879333\n",
      "epoch 46: loss=0.9116368293762207\n",
      "epoch 47: loss=0.9015913009643555\n",
      "epoch 48: loss=0.9015669226646423\n",
      "epoch 49: loss=0.9016717672348022\n",
      "epoch 50: loss=0.8976739645004272\n",
      "epoch 51: loss=0.894923746585846\n",
      "epoch 52: loss=0.8953223824501038\n",
      "epoch 53: loss=0.8924168348312378\n",
      "epoch 54: loss=0.8897157311439514\n",
      "epoch 55: loss=0.8897072672843933\n",
      "epoch 56: loss=0.8883274793624878\n",
      "epoch 57: loss=0.8872359991073608\n",
      "epoch 58: loss=0.8846961855888367\n",
      "epoch 59: loss=0.8867841958999634\n",
      "epoch 60: loss=0.8845803737640381\n",
      "epoch 61: loss=0.8834033608436584\n",
      "epoch 62: loss=0.8839239478111267\n",
      "epoch 63: loss=0.8841713070869446\n",
      "epoch 64: loss=0.8811259865760803\n",
      "epoch 65: loss=0.880947470664978\n",
      "epoch 66: loss=0.8834046125411987\n",
      "epoch 67: loss=0.8796929717063904\n",
      "epoch 68: loss=0.8813233375549316\n",
      "epoch 69: loss=0.8810459971427917\n",
      "epoch 70: loss=0.8784403800964355\n",
      "epoch 71: loss=0.8784419894218445\n",
      "epoch 72: loss=0.8796324729919434\n",
      "epoch 73: loss=0.8789284825325012\n",
      "epoch 74: loss=0.8757585883140564\n",
      "epoch 75: loss=0.8771275877952576\n",
      "epoch 76: loss=0.8776998519897461\n",
      "epoch 77: loss=0.876711905002594\n",
      "epoch 78: loss=0.8764276504516602\n",
      "epoch 79: loss=0.8769118189811707\n",
      "epoch 80: loss=0.8781867027282715\n",
      "epoch 81: loss=0.8742712140083313\n",
      "epoch 82: loss=0.8749376535415649\n",
      "epoch 83: loss=0.8762184381484985\n",
      "epoch 84: loss=0.8753294348716736\n",
      "epoch 85: loss=0.875219464302063\n",
      "epoch 86: loss=0.8746258616447449\n",
      "epoch 87: loss=0.8746306896209717\n",
      "epoch 88: loss=0.8731039762496948\n",
      "epoch 89: loss=0.8730065822601318\n",
      "epoch 90: loss=0.8742702007293701\n",
      "epoch 91: loss=0.8740074038505554\n",
      "epoch 92: loss=0.8742125630378723\n",
      "epoch 93: loss=0.8720117807388306\n",
      "epoch 94: loss=0.8727338314056396\n",
      "epoch 95: loss=0.8740814924240112\n",
      "epoch 96: loss=0.8734932541847229\n",
      "epoch 97: loss=0.8735035061836243\n",
      "epoch 98: loss=0.8732854723930359\n",
      "epoch 99: loss=0.8721708655357361\n",
      "epoch 100: loss=0.872092068195343\n",
      "epoch 101: loss=0.8710159063339233\n",
      "epoch 102: loss=0.8709148168563843\n",
      "epoch 103: loss=0.8692020773887634\n",
      "epoch 104: loss=0.8711859583854675\n",
      "epoch 105: loss=0.8716948628425598\n",
      "epoch 106: loss=0.8726189136505127\n",
      "epoch 107: loss=0.8711944818496704\n",
      "epoch 108: loss=0.8715574145317078\n",
      "epoch 109: loss=0.8695127367973328\n",
      "epoch 110: loss=0.8701194524765015\n",
      "epoch 111: loss=0.8702327609062195\n",
      "epoch 112: loss=0.8693594336509705\n",
      "epoch 113: loss=0.8679969310760498\n",
      "epoch 114: loss=0.8695376515388489\n",
      "epoch 115: loss=0.8676626086235046\n",
      "epoch 116: loss=0.8687256574630737\n",
      "epoch 117: loss=0.8676963448524475\n",
      "epoch 118: loss=0.867911696434021\n",
      "epoch 119: loss=0.8695235252380371\n",
      "epoch 120: loss=0.8682575225830078\n",
      "epoch 121: loss=0.8681227564811707\n",
      "epoch 122: loss=0.8686931133270264\n",
      "epoch 123: loss=0.8672214150428772\n",
      "epoch 124: loss=0.8687678575515747\n",
      "epoch 125: loss=0.8682432174682617\n",
      "epoch 126: loss=0.8679524064064026\n",
      "epoch 127: loss=0.8691667318344116\n",
      "epoch 128: loss=0.8686361908912659\n",
      "epoch 129: loss=0.8669825196266174\n",
      "epoch 130: loss=0.8676190376281738\n",
      "epoch 131: loss=0.8686017394065857\n",
      "epoch 132: loss=0.8690149188041687\n",
      "epoch 133: loss=0.8652054667472839\n",
      "epoch 134: loss=0.8674150705337524\n",
      "epoch 135: loss=0.8665980100631714\n",
      "epoch 136: loss=0.8655803203582764\n",
      "epoch 137: loss=0.8644853830337524\n",
      "epoch 138: loss=0.8650535941123962\n",
      "epoch 139: loss=0.8661704063415527\n",
      "epoch 140: loss=0.8648871183395386\n",
      "epoch 141: loss=0.8662376403808594\n",
      "epoch 142: loss=0.8643837571144104\n",
      "epoch 143: loss=0.8643990159034729\n",
      "epoch 144: loss=0.8638161420822144\n",
      "epoch 145: loss=0.8637224435806274\n",
      "epoch 146: loss=0.8639255166053772\n",
      "epoch 147: loss=0.8623314499855042\n",
      "epoch 148: loss=0.8636634349822998\n",
      "epoch 149: loss=0.8625500202178955\n",
      "epoch 150: loss=0.8645311594009399\n",
      "epoch 151: loss=0.8621953725814819\n",
      "epoch 152: loss=0.8625291585922241\n",
      "epoch 153: loss=0.8624830842018127\n",
      "epoch 154: loss=0.8630303144454956\n",
      "epoch 155: loss=0.8622982501983643\n",
      "epoch 156: loss=0.8611527681350708\n",
      "epoch 157: loss=0.8626997470855713\n",
      "epoch 158: loss=0.8618305325508118\n",
      "epoch 159: loss=0.8625918030738831\n",
      "epoch 160: loss=0.8610320091247559\n",
      "epoch 161: loss=0.8605331182479858\n",
      "epoch 162: loss=0.8606153726577759\n",
      "epoch 163: loss=0.8580397367477417\n",
      "epoch 164: loss=0.8602906465530396\n",
      "epoch 165: loss=0.8603867888450623\n",
      "epoch 166: loss=0.8576552867889404\n",
      "epoch 167: loss=0.85731440782547\n",
      "epoch 168: loss=0.858463704586029\n",
      "epoch 169: loss=0.8599191308021545\n",
      "epoch 170: loss=0.8578242659568787\n",
      "epoch 171: loss=0.855362594127655\n",
      "epoch 172: loss=0.8568748831748962\n",
      "epoch 173: loss=0.8582016825675964\n",
      "epoch 174: loss=0.8566492795944214\n",
      "epoch 175: loss=0.8545300364494324\n",
      "epoch 176: loss=0.8558517098426819\n",
      "epoch 177: loss=0.8548935055732727\n",
      "epoch 178: loss=0.8554071187973022\n",
      "epoch 179: loss=0.8543838858604431\n",
      "epoch 180: loss=0.8550792336463928\n",
      "epoch 181: loss=0.8550277352333069\n",
      "epoch 182: loss=0.8526632785797119\n",
      "epoch 183: loss=0.8524104952812195\n",
      "epoch 184: loss=0.8524206876754761\n",
      "epoch 185: loss=0.853071928024292\n",
      "epoch 186: loss=0.8557290434837341\n",
      "epoch 187: loss=0.852677583694458\n",
      "epoch 188: loss=0.8522987365722656\n",
      "epoch 189: loss=0.853216290473938\n",
      "epoch 190: loss=0.8534769415855408\n",
      "epoch 191: loss=0.8512645363807678\n",
      "epoch 192: loss=0.8495469689369202\n",
      "epoch 193: loss=0.8495098948478699\n",
      "epoch 194: loss=0.8493407368659973\n",
      "epoch 195: loss=0.8491482734680176\n",
      "epoch 196: loss=0.8511289954185486\n",
      "epoch 197: loss=0.8495939373970032\n",
      "epoch 198: loss=0.8499781489372253\n",
      "epoch 199: loss=0.8499836325645447\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=6.774307727813721\n",
      "epoch 1: loss=6.489864826202393\n",
      "epoch 2: loss=6.196009635925293\n",
      "epoch 3: loss=5.786098480224609\n",
      "epoch 4: loss=5.408690452575684\n",
      "epoch 5: loss=4.901329040527344\n",
      "epoch 6: loss=4.328083515167236\n",
      "epoch 7: loss=3.8703713417053223\n",
      "epoch 8: loss=3.6416213512420654\n",
      "epoch 9: loss=3.563800811767578\n",
      "epoch 10: loss=3.4151668548583984\n",
      "epoch 11: loss=3.1972038745880127\n",
      "epoch 12: loss=2.9240779876708984\n",
      "epoch 13: loss=2.6700680255889893\n",
      "epoch 14: loss=2.455289840698242\n",
      "epoch 15: loss=2.2422542572021484\n",
      "epoch 16: loss=2.1109066009521484\n",
      "epoch 17: loss=1.990621566772461\n",
      "epoch 18: loss=1.9145612716674805\n",
      "epoch 19: loss=1.847961664199829\n",
      "epoch 20: loss=1.7784278392791748\n",
      "epoch 21: loss=1.703523874282837\n",
      "epoch 22: loss=1.6393795013427734\n",
      "epoch 23: loss=1.5572588443756104\n",
      "epoch 24: loss=1.481436014175415\n",
      "epoch 25: loss=1.4064815044403076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26: loss=1.3411344289779663\n",
      "epoch 27: loss=1.278935194015503\n",
      "epoch 28: loss=1.2150371074676514\n",
      "epoch 29: loss=1.16145658493042\n",
      "epoch 30: loss=1.1189638376235962\n",
      "epoch 31: loss=1.0864362716674805\n",
      "epoch 32: loss=1.0660392045974731\n",
      "epoch 33: loss=1.0563774108886719\n",
      "epoch 34: loss=1.0439419746398926\n",
      "epoch 35: loss=1.0385711193084717\n",
      "epoch 36: loss=1.0283253192901611\n",
      "epoch 37: loss=1.0169458389282227\n",
      "epoch 38: loss=1.002246618270874\n",
      "epoch 39: loss=0.9876729249954224\n",
      "epoch 40: loss=0.9733384847640991\n",
      "epoch 41: loss=0.9628728032112122\n",
      "epoch 42: loss=0.9583007097244263\n",
      "epoch 43: loss=0.9534910917282104\n",
      "epoch 44: loss=0.9472737908363342\n",
      "epoch 45: loss=0.9465760588645935\n",
      "epoch 46: loss=0.9438990354537964\n",
      "epoch 47: loss=0.941016435623169\n",
      "epoch 48: loss=0.937634289264679\n",
      "epoch 49: loss=0.9358274936676025\n",
      "epoch 50: loss=0.9329615235328674\n",
      "epoch 51: loss=0.9285566210746765\n",
      "epoch 52: loss=0.9263482093811035\n",
      "epoch 53: loss=0.9235900044441223\n",
      "epoch 54: loss=0.9238736629486084\n",
      "epoch 55: loss=0.9218199849128723\n",
      "epoch 56: loss=0.9216966032981873\n",
      "epoch 57: loss=0.9204362630844116\n",
      "epoch 58: loss=0.9186011552810669\n",
      "epoch 59: loss=0.9170281887054443\n",
      "epoch 60: loss=0.9158548712730408\n",
      "epoch 61: loss=0.9152160286903381\n",
      "epoch 62: loss=0.9137606620788574\n",
      "epoch 63: loss=0.9127283096313477\n",
      "epoch 64: loss=0.9137002229690552\n",
      "epoch 65: loss=0.9117677807807922\n",
      "epoch 66: loss=0.9107239246368408\n",
      "epoch 67: loss=0.911300778388977\n",
      "epoch 68: loss=0.9100086092948914\n",
      "epoch 69: loss=0.9094843864440918\n",
      "epoch 70: loss=0.9097293615341187\n",
      "epoch 71: loss=0.9087305068969727\n",
      "epoch 72: loss=0.9080067873001099\n",
      "epoch 73: loss=0.9076418876647949\n",
      "epoch 74: loss=0.9071202278137207\n",
      "epoch 75: loss=0.9043295979499817\n",
      "epoch 76: loss=0.9047517776489258\n",
      "epoch 77: loss=0.906766951084137\n",
      "epoch 78: loss=0.9051822423934937\n",
      "epoch 79: loss=0.9057866930961609\n",
      "epoch 80: loss=0.9043091535568237\n",
      "epoch 81: loss=0.9041783213615417\n",
      "epoch 82: loss=0.9061166644096375\n",
      "epoch 83: loss=0.9013171792030334\n",
      "epoch 84: loss=0.9011611938476562\n",
      "epoch 85: loss=0.9074686765670776\n",
      "epoch 86: loss=0.9023306369781494\n",
      "epoch 87: loss=0.9030081629753113\n",
      "epoch 88: loss=0.9024397134780884\n",
      "epoch 89: loss=0.9008521437644958\n",
      "epoch 90: loss=0.9034204483032227\n",
      "epoch 91: loss=0.9020246267318726\n",
      "epoch 92: loss=0.9006231427192688\n",
      "epoch 93: loss=0.9010926485061646\n",
      "epoch 94: loss=0.9011011719703674\n",
      "epoch 95: loss=0.899501621723175\n",
      "epoch 96: loss=0.8995410203933716\n",
      "epoch 97: loss=0.8979330062866211\n",
      "epoch 98: loss=0.8999971151351929\n",
      "epoch 99: loss=0.8989900350570679\n",
      "epoch 100: loss=0.8961076140403748\n",
      "epoch 101: loss=0.8992419242858887\n",
      "epoch 102: loss=0.8983970880508423\n",
      "epoch 103: loss=0.8971372842788696\n",
      "epoch 104: loss=0.8983258605003357\n",
      "epoch 105: loss=0.8979176878929138\n",
      "epoch 106: loss=0.8949534893035889\n",
      "epoch 107: loss=0.8938250541687012\n",
      "epoch 108: loss=0.8975273966789246\n",
      "epoch 109: loss=0.8959487676620483\n",
      "epoch 110: loss=0.8964412808418274\n",
      "epoch 111: loss=0.8953803181648254\n",
      "epoch 112: loss=0.8954949975013733\n",
      "epoch 113: loss=0.8963320851325989\n",
      "epoch 114: loss=0.8962565064430237\n",
      "epoch 115: loss=0.8952089548110962\n",
      "epoch 116: loss=0.8925096392631531\n",
      "epoch 117: loss=0.8927441835403442\n",
      "epoch 118: loss=0.8929263949394226\n",
      "epoch 119: loss=0.8941746950149536\n",
      "epoch 120: loss=0.8943758010864258\n",
      "epoch 121: loss=0.8933281302452087\n",
      "epoch 122: loss=0.8917118310928345\n",
      "epoch 123: loss=0.8941033482551575\n",
      "epoch 124: loss=0.8925473690032959\n",
      "epoch 125: loss=0.8924158811569214\n",
      "epoch 126: loss=0.8925619721412659\n",
      "epoch 127: loss=0.8926061987876892\n",
      "epoch 128: loss=0.8916222453117371\n",
      "epoch 129: loss=0.8902043104171753\n",
      "epoch 130: loss=0.8894572854042053\n",
      "epoch 131: loss=0.8896206617355347\n",
      "epoch 132: loss=0.8906024694442749\n",
      "epoch 133: loss=0.8882586359977722\n",
      "epoch 134: loss=0.8882254958152771\n",
      "epoch 135: loss=0.8894630074501038\n",
      "epoch 136: loss=0.889497697353363\n",
      "epoch 137: loss=0.8898905515670776\n",
      "epoch 138: loss=0.8894900679588318\n",
      "epoch 139: loss=0.8889302611351013\n",
      "epoch 140: loss=0.8877915143966675\n",
      "epoch 141: loss=0.8888717293739319\n",
      "epoch 142: loss=0.8871719241142273\n",
      "epoch 143: loss=0.8890075087547302\n",
      "epoch 144: loss=0.8881244659423828\n",
      "epoch 145: loss=0.8867195844650269\n",
      "epoch 146: loss=0.8897020220756531\n",
      "epoch 147: loss=0.8888845443725586\n",
      "epoch 148: loss=0.887273907661438\n",
      "epoch 149: loss=0.8867700695991516\n",
      "epoch 150: loss=0.8858903050422668\n",
      "epoch 151: loss=0.8875929713249207\n",
      "epoch 152: loss=0.886911928653717\n",
      "epoch 153: loss=0.8856081962585449\n",
      "epoch 154: loss=0.8859202861785889\n",
      "epoch 155: loss=0.8861741423606873\n",
      "epoch 156: loss=0.8850579261779785\n",
      "epoch 157: loss=0.884564220905304\n",
      "epoch 158: loss=0.8868677616119385\n",
      "epoch 159: loss=0.8854051232337952\n",
      "epoch 160: loss=0.8872986435890198\n",
      "epoch 161: loss=0.8839457035064697\n",
      "epoch 162: loss=0.8855822682380676\n",
      "epoch 163: loss=0.8824622631072998\n",
      "epoch 164: loss=0.8835304379463196\n",
      "epoch 165: loss=0.8847859501838684\n",
      "epoch 166: loss=0.883622407913208\n",
      "epoch 167: loss=0.885248601436615\n",
      "epoch 168: loss=0.8844349980354309\n",
      "epoch 169: loss=0.8823465704917908\n",
      "epoch 170: loss=0.8827285766601562\n",
      "epoch 171: loss=0.8824854493141174\n",
      "epoch 172: loss=0.8816294074058533\n",
      "epoch 173: loss=0.8817237019538879\n",
      "epoch 174: loss=0.8824494481086731\n",
      "epoch 175: loss=0.8828890323638916\n",
      "epoch 176: loss=0.8807668089866638\n",
      "epoch 177: loss=0.8805152177810669\n",
      "epoch 178: loss=0.8798861503601074\n",
      "epoch 179: loss=0.8794081807136536\n",
      "epoch 180: loss=0.8796656727790833\n",
      "epoch 181: loss=0.8821320533752441\n",
      "epoch 182: loss=0.8778802156448364\n",
      "epoch 183: loss=0.8799729943275452\n",
      "epoch 184: loss=0.8806469440460205\n",
      "epoch 185: loss=0.8775414824485779\n",
      "epoch 186: loss=0.8790383338928223\n",
      "epoch 187: loss=0.878887414932251\n",
      "epoch 188: loss=0.8787109851837158\n",
      "epoch 189: loss=0.8786803483963013\n",
      "epoch 190: loss=0.8782428503036499\n",
      "epoch 191: loss=0.8780807852745056\n",
      "epoch 192: loss=0.876552939414978\n",
      "epoch 193: loss=0.8757785558700562\n",
      "epoch 194: loss=0.878399670124054\n",
      "epoch 195: loss=0.8765561580657959\n",
      "epoch 196: loss=0.8754973411560059\n",
      "epoch 197: loss=0.8756745457649231\n",
      "epoch 198: loss=0.8752384185791016\n",
      "epoch 199: loss=0.8732892274856567\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=6.694543838500977\n",
      "epoch 1: loss=6.351161479949951\n",
      "epoch 2: loss=6.022234916687012\n",
      "epoch 3: loss=5.604487895965576\n",
      "epoch 4: loss=4.920081615447998\n",
      "epoch 5: loss=4.270844459533691\n",
      "epoch 6: loss=3.7818667888641357\n",
      "epoch 7: loss=3.5516152381896973\n",
      "epoch 8: loss=3.4954681396484375\n",
      "epoch 9: loss=3.367802143096924\n",
      "epoch 10: loss=3.1794402599334717\n",
      "epoch 11: loss=2.9755022525787354\n",
      "epoch 12: loss=2.738492727279663\n",
      "epoch 13: loss=2.490013360977173\n",
      "epoch 14: loss=2.3085248470306396\n",
      "epoch 15: loss=2.1563353538513184\n",
      "epoch 16: loss=2.0394287109375\n",
      "epoch 17: loss=1.9337400197982788\n",
      "epoch 18: loss=1.8504178524017334\n",
      "epoch 19: loss=1.7786509990692139\n",
      "epoch 20: loss=1.7112244367599487\n",
      "epoch 21: loss=1.6666746139526367\n",
      "epoch 22: loss=1.5974129438400269\n",
      "epoch 23: loss=1.508135199546814\n",
      "epoch 24: loss=1.4399735927581787\n",
      "epoch 25: loss=1.361395001411438\n",
      "epoch 26: loss=1.2944694757461548\n",
      "epoch 27: loss=1.2382997274398804\n",
      "epoch 28: loss=1.1840643882751465\n",
      "epoch 29: loss=1.1355726718902588\n",
      "epoch 30: loss=1.096197485923767\n",
      "epoch 31: loss=1.0617790222167969\n",
      "epoch 32: loss=1.0414083003997803\n",
      "epoch 33: loss=1.0231132507324219\n",
      "epoch 34: loss=1.0174157619476318\n",
      "epoch 35: loss=1.0028373003005981\n",
      "epoch 36: loss=0.9929032325744629\n",
      "epoch 37: loss=0.9804733991622925\n",
      "epoch 38: loss=0.9670491814613342\n",
      "epoch 39: loss=0.9550457000732422\n",
      "epoch 40: loss=0.9495335817337036\n",
      "epoch 41: loss=0.9381508827209473\n",
      "epoch 42: loss=0.9351555109024048\n",
      "epoch 43: loss=0.9321473836898804\n",
      "epoch 44: loss=0.9262362718582153\n",
      "epoch 45: loss=0.9263134002685547\n",
      "epoch 46: loss=0.921522855758667\n",
      "epoch 47: loss=0.920624315738678\n",
      "epoch 48: loss=0.9140735268592834\n",
      "epoch 49: loss=0.9112387299537659\n",
      "epoch 50: loss=0.9096168279647827\n",
      "epoch 51: loss=0.9076217412948608\n",
      "epoch 52: loss=0.9077872037887573\n",
      "epoch 53: loss=0.9040457010269165\n",
      "epoch 54: loss=0.9043612480163574\n",
      "epoch 55: loss=0.9021636247634888\n",
      "epoch 56: loss=0.9029041528701782\n",
      "epoch 57: loss=0.9008144736289978\n",
      "epoch 58: loss=0.8999612927436829\n",
      "epoch 59: loss=0.8966293931007385\n",
      "epoch 60: loss=0.8967226147651672\n",
      "epoch 61: loss=0.8941546678543091\n",
      "epoch 62: loss=0.8957855105400085\n",
      "epoch 63: loss=0.8947002291679382\n",
      "epoch 64: loss=0.8962680101394653\n",
      "epoch 65: loss=0.8927525877952576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66: loss=0.8933179378509521\n",
      "epoch 67: loss=0.8919470906257629\n",
      "epoch 68: loss=0.8904905915260315\n",
      "epoch 69: loss=0.8914863467216492\n",
      "epoch 70: loss=0.889940619468689\n",
      "epoch 71: loss=0.8914219737052917\n",
      "epoch 72: loss=0.8910629749298096\n",
      "epoch 73: loss=0.8882167935371399\n",
      "epoch 74: loss=0.8889081478118896\n",
      "epoch 75: loss=0.8891040682792664\n",
      "epoch 76: loss=0.8857093453407288\n",
      "epoch 77: loss=0.8882849216461182\n",
      "epoch 78: loss=0.8892030715942383\n",
      "epoch 79: loss=0.8861438632011414\n",
      "epoch 80: loss=0.8874372839927673\n",
      "epoch 81: loss=0.8869413137435913\n",
      "epoch 82: loss=0.8860516548156738\n",
      "epoch 83: loss=0.8850821852684021\n",
      "epoch 84: loss=0.8864266276359558\n",
      "epoch 85: loss=0.886035680770874\n",
      "epoch 86: loss=0.8847435712814331\n",
      "epoch 87: loss=0.884731650352478\n",
      "epoch 88: loss=0.8855785727500916\n",
      "epoch 89: loss=0.8845897316932678\n",
      "epoch 90: loss=0.88332200050354\n",
      "epoch 91: loss=0.8824588656425476\n",
      "epoch 92: loss=0.8844916820526123\n",
      "epoch 93: loss=0.8852806687355042\n",
      "epoch 94: loss=0.8830767869949341\n",
      "epoch 95: loss=0.8832626342773438\n",
      "epoch 96: loss=0.8824073672294617\n",
      "epoch 97: loss=0.8840302228927612\n",
      "epoch 98: loss=0.8827094435691833\n",
      "epoch 99: loss=0.8811353445053101\n",
      "epoch 100: loss=0.8837993144989014\n",
      "epoch 101: loss=0.8812456130981445\n",
      "epoch 102: loss=0.8817058205604553\n",
      "epoch 103: loss=0.8796859383583069\n",
      "epoch 104: loss=0.8816446661949158\n",
      "epoch 105: loss=0.881037175655365\n",
      "epoch 106: loss=0.8809372782707214\n",
      "epoch 107: loss=0.8804607391357422\n",
      "epoch 108: loss=0.8795833587646484\n",
      "epoch 109: loss=0.8797584176063538\n",
      "epoch 110: loss=0.8787500858306885\n",
      "epoch 111: loss=0.8779078722000122\n",
      "epoch 112: loss=0.8789941072463989\n",
      "epoch 113: loss=0.8797114491462708\n",
      "epoch 114: loss=0.878407895565033\n",
      "epoch 115: loss=0.8789801597595215\n",
      "epoch 116: loss=0.8779159784317017\n",
      "epoch 117: loss=0.8753432035446167\n",
      "epoch 118: loss=0.8746576309204102\n",
      "epoch 119: loss=0.8780041337013245\n",
      "epoch 120: loss=0.8757871389389038\n",
      "epoch 121: loss=0.8751383423805237\n",
      "epoch 122: loss=0.8752062916755676\n",
      "epoch 123: loss=0.873279869556427\n",
      "epoch 124: loss=0.8751919865608215\n",
      "epoch 125: loss=0.8755490779876709\n",
      "epoch 126: loss=0.876984179019928\n",
      "epoch 127: loss=0.873338520526886\n",
      "epoch 128: loss=0.8733236789703369\n",
      "epoch 129: loss=0.8729948401451111\n",
      "epoch 130: loss=0.8743678331375122\n",
      "epoch 131: loss=0.8739712238311768\n",
      "epoch 132: loss=0.8728377819061279\n",
      "epoch 133: loss=0.8729020357131958\n",
      "epoch 134: loss=0.873877227306366\n",
      "epoch 135: loss=0.8725335597991943\n",
      "epoch 136: loss=0.872218668460846\n",
      "epoch 137: loss=0.8713086843490601\n",
      "epoch 138: loss=0.871471643447876\n",
      "epoch 139: loss=0.8707503080368042\n",
      "epoch 140: loss=0.8704245686531067\n",
      "epoch 141: loss=0.8688368201255798\n",
      "epoch 142: loss=0.8720499873161316\n",
      "epoch 143: loss=0.8694449067115784\n",
      "epoch 144: loss=0.8693662881851196\n",
      "epoch 145: loss=0.8694391846656799\n",
      "epoch 146: loss=0.8688669204711914\n",
      "epoch 147: loss=0.8683022856712341\n",
      "epoch 148: loss=0.8669443726539612\n",
      "epoch 149: loss=0.8664743304252625\n",
      "epoch 150: loss=0.8676530718803406\n",
      "epoch 151: loss=0.8660122156143188\n",
      "epoch 152: loss=0.8670281171798706\n",
      "epoch 153: loss=0.8648583292961121\n",
      "epoch 154: loss=0.8670413494110107\n",
      "epoch 155: loss=0.8635783195495605\n",
      "epoch 156: loss=0.8648167252540588\n",
      "epoch 157: loss=0.8637840151786804\n",
      "epoch 158: loss=0.863157331943512\n",
      "epoch 159: loss=0.8669734597206116\n",
      "epoch 160: loss=0.862915575504303\n",
      "epoch 161: loss=0.8631951212882996\n",
      "epoch 162: loss=0.8618946671485901\n",
      "epoch 163: loss=0.8618574738502502\n",
      "epoch 164: loss=0.862034022808075\n",
      "epoch 165: loss=0.8623747229576111\n",
      "epoch 166: loss=0.8615127801895142\n",
      "epoch 167: loss=0.8629141449928284\n",
      "epoch 168: loss=0.8587958812713623\n",
      "epoch 169: loss=0.8618525862693787\n",
      "epoch 170: loss=0.8603706359863281\n",
      "epoch 171: loss=0.859686553478241\n",
      "epoch 172: loss=0.8601264953613281\n",
      "epoch 173: loss=0.8590596318244934\n",
      "epoch 174: loss=0.8580996990203857\n",
      "epoch 175: loss=0.8584782481193542\n",
      "epoch 176: loss=0.8592393398284912\n",
      "epoch 177: loss=0.8569327592849731\n",
      "epoch 178: loss=0.8584114909172058\n",
      "epoch 179: loss=0.8573375940322876\n",
      "epoch 180: loss=0.8585625886917114\n",
      "epoch 181: loss=0.8572343587875366\n",
      "epoch 182: loss=0.8553752303123474\n",
      "epoch 183: loss=0.8581181168556213\n",
      "epoch 184: loss=0.8570576310157776\n",
      "epoch 185: loss=0.8556005954742432\n",
      "epoch 186: loss=0.8527487516403198\n",
      "epoch 187: loss=0.854038655757904\n",
      "epoch 188: loss=0.8549027442932129\n",
      "epoch 189: loss=0.8552871346473694\n",
      "epoch 190: loss=0.8550928831100464\n",
      "epoch 191: loss=0.8529653549194336\n",
      "epoch 192: loss=0.8522440195083618\n",
      "epoch 193: loss=0.8542080521583557\n",
      "epoch 194: loss=0.8519898056983948\n",
      "epoch 195: loss=0.8531760573387146\n",
      "epoch 196: loss=0.8526103496551514\n",
      "epoch 197: loss=0.8500310182571411\n",
      "epoch 198: loss=0.8520031571388245\n",
      "epoch 199: loss=0.8487886190414429\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=6.746382713317871\n",
      "epoch 1: loss=6.3606767654418945\n",
      "epoch 2: loss=6.022436618804932\n",
      "epoch 3: loss=5.508463382720947\n",
      "epoch 4: loss=5.034012794494629\n",
      "epoch 5: loss=4.370657444000244\n",
      "epoch 6: loss=3.875523805618286\n",
      "epoch 7: loss=3.6056740283966064\n",
      "epoch 8: loss=3.5302553176879883\n",
      "epoch 9: loss=3.3158628940582275\n",
      "epoch 10: loss=2.9924745559692383\n",
      "epoch 11: loss=2.672027587890625\n",
      "epoch 12: loss=2.409050941467285\n",
      "epoch 13: loss=2.171193838119507\n",
      "epoch 14: loss=2.027899742126465\n",
      "epoch 15: loss=1.9129769802093506\n",
      "epoch 16: loss=1.849355697631836\n",
      "epoch 17: loss=1.7772350311279297\n",
      "epoch 18: loss=1.6859722137451172\n",
      "epoch 19: loss=1.6177836656570435\n",
      "epoch 20: loss=1.5314680337905884\n",
      "epoch 21: loss=1.4584670066833496\n",
      "epoch 22: loss=1.3894401788711548\n",
      "epoch 23: loss=1.3243281841278076\n",
      "epoch 24: loss=1.2511314153671265\n",
      "epoch 25: loss=1.1805261373519897\n",
      "epoch 26: loss=1.1241788864135742\n",
      "epoch 27: loss=1.085114598274231\n",
      "epoch 28: loss=1.0543503761291504\n",
      "epoch 29: loss=1.038127064704895\n",
      "epoch 30: loss=1.0283563137054443\n",
      "epoch 31: loss=1.0241011381149292\n",
      "epoch 32: loss=1.0128421783447266\n",
      "epoch 33: loss=0.9984539151191711\n",
      "epoch 34: loss=0.9853646159172058\n",
      "epoch 35: loss=0.9735091328620911\n",
      "epoch 36: loss=0.9593658447265625\n",
      "epoch 37: loss=0.949351966381073\n",
      "epoch 38: loss=0.9432454109191895\n",
      "epoch 39: loss=0.9361661672592163\n",
      "epoch 40: loss=0.9317353367805481\n",
      "epoch 41: loss=0.9316239953041077\n",
      "epoch 42: loss=0.9284631013870239\n",
      "epoch 43: loss=0.9263595342636108\n",
      "epoch 44: loss=0.9222755432128906\n",
      "epoch 45: loss=0.9197050929069519\n",
      "epoch 46: loss=0.9138898849487305\n",
      "epoch 47: loss=0.914318859577179\n",
      "epoch 48: loss=0.9096676111221313\n",
      "epoch 49: loss=0.9073854088783264\n",
      "epoch 50: loss=0.906105637550354\n",
      "epoch 51: loss=0.9091112613677979\n",
      "epoch 52: loss=0.9079732298851013\n",
      "epoch 53: loss=0.9042466282844543\n",
      "epoch 54: loss=0.9022802710533142\n",
      "epoch 55: loss=0.9011102914810181\n",
      "epoch 56: loss=0.8989869356155396\n",
      "epoch 57: loss=0.8972971439361572\n",
      "epoch 58: loss=0.8972026705741882\n",
      "epoch 59: loss=0.8957266211509705\n",
      "epoch 60: loss=0.898980975151062\n",
      "epoch 61: loss=0.8977483510971069\n",
      "epoch 62: loss=0.893998384475708\n",
      "epoch 63: loss=0.8946835398674011\n",
      "epoch 64: loss=0.8958249688148499\n",
      "epoch 65: loss=0.8942984342575073\n",
      "epoch 66: loss=0.8904719352722168\n",
      "epoch 67: loss=0.8912468552589417\n",
      "epoch 68: loss=0.8898302912712097\n",
      "epoch 69: loss=0.8920531868934631\n",
      "epoch 70: loss=0.8909136056900024\n",
      "epoch 71: loss=0.8904401063919067\n",
      "epoch 72: loss=0.8912231922149658\n",
      "epoch 73: loss=0.889224648475647\n",
      "epoch 74: loss=0.8884757161140442\n",
      "epoch 75: loss=0.8899073600769043\n",
      "epoch 76: loss=0.8877344131469727\n",
      "epoch 77: loss=0.889224648475647\n",
      "epoch 78: loss=0.8897237777709961\n",
      "epoch 79: loss=0.8908920884132385\n",
      "epoch 80: loss=0.8867518901824951\n",
      "epoch 81: loss=0.8888450264930725\n",
      "epoch 82: loss=0.8859104514122009\n",
      "epoch 83: loss=0.8862528204917908\n",
      "epoch 84: loss=0.8884719014167786\n",
      "epoch 85: loss=0.8867947459220886\n",
      "epoch 86: loss=0.8856867551803589\n",
      "epoch 87: loss=0.8855107426643372\n",
      "epoch 88: loss=0.8842558264732361\n",
      "epoch 89: loss=0.8851375579833984\n",
      "epoch 90: loss=0.8831604719161987\n",
      "epoch 91: loss=0.8817853331565857\n",
      "epoch 92: loss=0.8835750222206116\n",
      "epoch 93: loss=0.8852550983428955\n",
      "epoch 94: loss=0.8834075331687927\n",
      "epoch 95: loss=0.8839161992073059\n",
      "epoch 96: loss=0.8809224367141724\n",
      "epoch 97: loss=0.881213903427124\n",
      "epoch 98: loss=0.8837423920631409\n",
      "epoch 99: loss=0.8806287050247192\n",
      "epoch 100: loss=0.8826178908348083\n",
      "epoch 101: loss=0.8844174742698669\n",
      "epoch 102: loss=0.8799341917037964\n",
      "epoch 103: loss=0.8810423612594604\n",
      "epoch 104: loss=0.8833086490631104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 105: loss=0.8833718299865723\n",
      "epoch 106: loss=0.8803322911262512\n",
      "epoch 107: loss=0.8791599869728088\n",
      "epoch 108: loss=0.8805680274963379\n",
      "epoch 109: loss=0.8816434741020203\n",
      "epoch 110: loss=0.8782850503921509\n",
      "epoch 111: loss=0.8789931535720825\n",
      "epoch 112: loss=0.8816898465156555\n",
      "epoch 113: loss=0.8798015117645264\n",
      "epoch 114: loss=0.8797463774681091\n",
      "epoch 115: loss=0.8788540959358215\n",
      "epoch 116: loss=0.8765179514884949\n",
      "epoch 117: loss=0.8775442838668823\n",
      "epoch 118: loss=0.8779743909835815\n",
      "epoch 119: loss=0.8790807723999023\n",
      "epoch 120: loss=0.8791230916976929\n",
      "epoch 121: loss=0.8780624866485596\n",
      "epoch 122: loss=0.8766433000564575\n",
      "epoch 123: loss=0.8758574724197388\n",
      "epoch 124: loss=0.8773213028907776\n",
      "epoch 125: loss=0.8777865171432495\n",
      "epoch 126: loss=0.8760054111480713\n",
      "epoch 127: loss=0.8737836480140686\n",
      "epoch 128: loss=0.8774591088294983\n",
      "epoch 129: loss=0.8758773803710938\n",
      "epoch 130: loss=0.8748421669006348\n",
      "epoch 131: loss=0.8760688304901123\n",
      "epoch 132: loss=0.8761125802993774\n",
      "epoch 133: loss=0.8741523027420044\n",
      "epoch 134: loss=0.8740110993385315\n",
      "epoch 135: loss=0.8750882744789124\n",
      "epoch 136: loss=0.8738625049591064\n",
      "epoch 137: loss=0.8772333860397339\n",
      "epoch 138: loss=0.8728826642036438\n",
      "epoch 139: loss=0.8755772113800049\n",
      "epoch 140: loss=0.871074914932251\n",
      "epoch 141: loss=0.8722572326660156\n",
      "epoch 142: loss=0.8727181553840637\n",
      "epoch 143: loss=0.8738754391670227\n",
      "epoch 144: loss=0.8698554635047913\n",
      "epoch 145: loss=0.871245265007019\n",
      "epoch 146: loss=0.8722659945487976\n",
      "epoch 147: loss=0.8726256489753723\n",
      "epoch 148: loss=0.8709266185760498\n",
      "epoch 149: loss=0.8700863718986511\n",
      "epoch 150: loss=0.872230052947998\n",
      "epoch 151: loss=0.869035005569458\n",
      "epoch 152: loss=0.8699183464050293\n",
      "epoch 153: loss=0.8713904023170471\n",
      "epoch 154: loss=0.8678938746452332\n",
      "epoch 155: loss=0.8696965575218201\n",
      "epoch 156: loss=0.871146559715271\n",
      "epoch 157: loss=0.8704383969306946\n",
      "epoch 158: loss=0.8704341053962708\n",
      "epoch 159: loss=0.8688256740570068\n",
      "epoch 160: loss=0.8696418404579163\n",
      "epoch 161: loss=0.8678427338600159\n",
      "epoch 162: loss=0.8685597777366638\n",
      "epoch 163: loss=0.8694524765014648\n",
      "epoch 164: loss=0.8684040904045105\n",
      "epoch 165: loss=0.8701746463775635\n",
      "epoch 166: loss=0.8654634952545166\n",
      "epoch 167: loss=0.868993878364563\n",
      "epoch 168: loss=0.8688702583312988\n",
      "epoch 169: loss=0.8664514422416687\n",
      "epoch 170: loss=0.8659208416938782\n",
      "epoch 171: loss=0.8696932792663574\n",
      "epoch 172: loss=0.8667451739311218\n",
      "epoch 173: loss=0.8669945597648621\n",
      "epoch 174: loss=0.8686294555664062\n",
      "epoch 175: loss=0.8664222955703735\n",
      "epoch 176: loss=0.8648903369903564\n",
      "epoch 177: loss=0.8637078404426575\n",
      "epoch 178: loss=0.8701218962669373\n",
      "epoch 179: loss=0.8637979626655579\n",
      "epoch 180: loss=0.8649563789367676\n",
      "epoch 181: loss=0.8634175658226013\n",
      "epoch 182: loss=0.8624392747879028\n",
      "epoch 183: loss=0.8630313873291016\n",
      "epoch 184: loss=0.8598796725273132\n",
      "epoch 185: loss=0.8576385974884033\n",
      "epoch 186: loss=0.8605577349662781\n",
      "epoch 187: loss=0.8602288365364075\n",
      "epoch 188: loss=0.8581923842430115\n",
      "epoch 189: loss=0.8562119007110596\n",
      "epoch 190: loss=0.8558407425880432\n",
      "epoch 191: loss=0.8557896018028259\n",
      "epoch 192: loss=0.8544489145278931\n",
      "epoch 193: loss=0.8539792895317078\n",
      "epoch 194: loss=0.8546113967895508\n",
      "epoch 195: loss=0.8534765839576721\n",
      "epoch 196: loss=0.851249098777771\n",
      "epoch 197: loss=0.8526074886322021\n",
      "epoch 198: loss=0.8519290089607239\n",
      "epoch 199: loss=0.8516800403594971\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=6.685365676879883\n",
      "epoch 1: loss=6.491275787353516\n",
      "epoch 2: loss=6.196911334991455\n",
      "epoch 3: loss=5.777307987213135\n",
      "epoch 4: loss=5.3206610679626465\n",
      "epoch 5: loss=4.756131172180176\n",
      "epoch 6: loss=4.1640625\n",
      "epoch 7: loss=3.833984375\n",
      "epoch 8: loss=3.6587798595428467\n",
      "epoch 9: loss=3.5508458614349365\n",
      "epoch 10: loss=3.3307535648345947\n",
      "epoch 11: loss=3.097508668899536\n",
      "epoch 12: loss=2.8119566440582275\n",
      "epoch 13: loss=2.5391921997070312\n",
      "epoch 14: loss=2.284518003463745\n",
      "epoch 15: loss=2.1665077209472656\n",
      "epoch 16: loss=2.040343761444092\n",
      "epoch 17: loss=1.936515212059021\n",
      "epoch 18: loss=1.8778653144836426\n",
      "epoch 19: loss=1.8014063835144043\n",
      "epoch 20: loss=1.7217222452163696\n",
      "epoch 21: loss=1.6372811794281006\n",
      "epoch 22: loss=1.5662716627120972\n",
      "epoch 23: loss=1.504921793937683\n",
      "epoch 24: loss=1.4331880807876587\n",
      "epoch 25: loss=1.3609117269515991\n",
      "epoch 26: loss=1.2950481176376343\n",
      "epoch 27: loss=1.2266900539398193\n",
      "epoch 28: loss=1.1634784936904907\n",
      "epoch 29: loss=1.1169358491897583\n",
      "epoch 30: loss=1.0744655132293701\n",
      "epoch 31: loss=1.0619876384735107\n",
      "epoch 32: loss=1.0460376739501953\n",
      "epoch 33: loss=1.0423598289489746\n",
      "epoch 34: loss=1.0302934646606445\n",
      "epoch 35: loss=1.0185004472732544\n",
      "epoch 36: loss=1.00528085231781\n",
      "epoch 37: loss=0.9889199733734131\n",
      "epoch 38: loss=0.9715527296066284\n",
      "epoch 39: loss=0.962201714515686\n",
      "epoch 40: loss=0.9603332877159119\n",
      "epoch 41: loss=0.9550738334655762\n",
      "epoch 42: loss=0.952965259552002\n",
      "epoch 43: loss=0.9483542442321777\n",
      "epoch 44: loss=0.9458113312721252\n",
      "epoch 45: loss=0.9400721788406372\n",
      "epoch 46: loss=0.9324648976325989\n",
      "epoch 47: loss=0.9316681027412415\n",
      "epoch 48: loss=0.9287514686584473\n",
      "epoch 49: loss=0.9297345876693726\n",
      "epoch 50: loss=0.925754964351654\n",
      "epoch 51: loss=0.924828052520752\n",
      "epoch 52: loss=0.9228885173797607\n",
      "epoch 53: loss=0.9220346212387085\n",
      "epoch 54: loss=0.9190458655357361\n",
      "epoch 55: loss=0.9208542108535767\n",
      "epoch 56: loss=0.921125054359436\n",
      "epoch 57: loss=0.9173903465270996\n",
      "epoch 58: loss=0.9153584241867065\n",
      "epoch 59: loss=0.9159926772117615\n",
      "epoch 60: loss=0.9145424962043762\n",
      "epoch 61: loss=0.9146863222122192\n",
      "epoch 62: loss=0.913479745388031\n",
      "epoch 63: loss=0.9108442664146423\n",
      "epoch 64: loss=0.9109305739402771\n",
      "epoch 65: loss=0.9107642769813538\n",
      "epoch 66: loss=0.9097164273262024\n",
      "epoch 67: loss=0.9092101454734802\n",
      "epoch 68: loss=0.9089004993438721\n",
      "epoch 69: loss=0.9078842997550964\n",
      "epoch 70: loss=0.9082352519035339\n",
      "epoch 71: loss=0.9094886183738708\n",
      "epoch 72: loss=0.9067776203155518\n",
      "epoch 73: loss=0.9057097434997559\n",
      "epoch 74: loss=0.9074605107307434\n",
      "epoch 75: loss=0.9057174921035767\n",
      "epoch 76: loss=0.9072037935256958\n",
      "epoch 77: loss=0.9045528769493103\n",
      "epoch 78: loss=0.9029121398925781\n",
      "epoch 79: loss=0.9044503569602966\n",
      "epoch 80: loss=0.9041686058044434\n",
      "epoch 81: loss=0.9025923609733582\n",
      "epoch 82: loss=0.9035028219223022\n",
      "epoch 83: loss=0.9027590751647949\n",
      "epoch 84: loss=0.9023808240890503\n",
      "epoch 85: loss=0.9015873670578003\n",
      "epoch 86: loss=0.9006553888320923\n",
      "epoch 87: loss=0.9044463634490967\n",
      "epoch 88: loss=0.9011901617050171\n",
      "epoch 89: loss=0.9005995988845825\n",
      "epoch 90: loss=0.9009169340133667\n",
      "epoch 91: loss=0.9013900756835938\n",
      "epoch 92: loss=0.9000918865203857\n",
      "epoch 93: loss=0.8987831473350525\n",
      "epoch 94: loss=0.8981119394302368\n",
      "epoch 95: loss=0.9002688527107239\n",
      "epoch 96: loss=0.8985040187835693\n",
      "epoch 97: loss=0.8995070457458496\n",
      "epoch 98: loss=0.8964505195617676\n",
      "epoch 99: loss=0.8970393538475037\n",
      "epoch 100: loss=0.8989608287811279\n",
      "epoch 101: loss=0.896801769733429\n",
      "epoch 102: loss=0.8967909216880798\n",
      "epoch 103: loss=0.8955805897712708\n",
      "epoch 104: loss=0.8967679142951965\n",
      "epoch 105: loss=0.8955947756767273\n",
      "epoch 106: loss=0.8948484659194946\n",
      "epoch 107: loss=0.8952543139457703\n",
      "epoch 108: loss=0.8939487934112549\n",
      "epoch 109: loss=0.8959194421768188\n",
      "epoch 110: loss=0.8950074911117554\n",
      "epoch 111: loss=0.8931863903999329\n",
      "epoch 112: loss=0.8957173228263855\n",
      "epoch 113: loss=0.8949218988418579\n",
      "epoch 114: loss=0.893881618976593\n",
      "epoch 115: loss=0.8940272331237793\n",
      "epoch 116: loss=0.8939604163169861\n",
      "epoch 117: loss=0.8931412100791931\n",
      "epoch 118: loss=0.8933022618293762\n",
      "epoch 119: loss=0.891729474067688\n",
      "epoch 120: loss=0.8910278081893921\n",
      "epoch 121: loss=0.8915032744407654\n",
      "epoch 122: loss=0.8891370296478271\n",
      "epoch 123: loss=0.8891230821609497\n",
      "epoch 124: loss=0.8919643759727478\n",
      "epoch 125: loss=0.8898553252220154\n",
      "epoch 126: loss=0.8891043066978455\n",
      "epoch 127: loss=0.8875749111175537\n",
      "epoch 128: loss=0.8891714811325073\n",
      "epoch 129: loss=0.8885328769683838\n",
      "epoch 130: loss=0.8885580897331238\n",
      "epoch 131: loss=0.8905840516090393\n",
      "epoch 132: loss=0.8882608413696289\n",
      "epoch 133: loss=0.8875404000282288\n",
      "epoch 134: loss=0.8847888112068176\n",
      "epoch 135: loss=0.8866792917251587\n",
      "epoch 136: loss=0.8860016465187073\n",
      "epoch 137: loss=0.8843635320663452\n",
      "epoch 138: loss=0.8879429697990417\n",
      "epoch 139: loss=0.8850404024124146\n",
      "epoch 140: loss=0.8849114775657654\n",
      "epoch 141: loss=0.8834614753723145\n",
      "epoch 142: loss=0.8826577663421631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 143: loss=0.8829189538955688\n",
      "epoch 144: loss=0.8810334801673889\n",
      "epoch 145: loss=0.8818398714065552\n",
      "epoch 146: loss=0.882530152797699\n",
      "epoch 147: loss=0.8817722797393799\n",
      "epoch 148: loss=0.8773316740989685\n",
      "epoch 149: loss=0.8797298669815063\n",
      "epoch 150: loss=0.8789230585098267\n",
      "epoch 151: loss=0.8788057565689087\n",
      "epoch 152: loss=0.8781022429466248\n",
      "epoch 153: loss=0.8776859045028687\n",
      "epoch 154: loss=0.8766315579414368\n",
      "epoch 155: loss=0.8756393194198608\n",
      "epoch 156: loss=0.8747490644454956\n",
      "epoch 157: loss=0.8747283816337585\n",
      "epoch 158: loss=0.8745712637901306\n",
      "epoch 159: loss=0.8774346113204956\n",
      "epoch 160: loss=0.8725038766860962\n",
      "epoch 161: loss=0.8741857409477234\n",
      "epoch 162: loss=0.8720966577529907\n",
      "epoch 163: loss=0.8726977109909058\n",
      "epoch 164: loss=0.8734301328659058\n",
      "epoch 165: loss=0.8722098469734192\n",
      "epoch 166: loss=0.874077320098877\n",
      "epoch 167: loss=0.8737041354179382\n",
      "epoch 168: loss=0.8721021413803101\n",
      "epoch 169: loss=0.8722752928733826\n",
      "epoch 170: loss=0.8711358308792114\n",
      "epoch 171: loss=0.8712616562843323\n",
      "epoch 172: loss=0.8714713454246521\n",
      "epoch 173: loss=0.8696844577789307\n",
      "epoch 174: loss=0.8665515184402466\n",
      "epoch 175: loss=0.8685321807861328\n",
      "epoch 176: loss=0.8680573105812073\n",
      "epoch 177: loss=0.8688052892684937\n",
      "epoch 178: loss=0.8693715929985046\n",
      "epoch 179: loss=0.8683559894561768\n",
      "epoch 180: loss=0.8681581616401672\n",
      "epoch 181: loss=0.8667274117469788\n",
      "epoch 182: loss=0.867577075958252\n",
      "epoch 183: loss=0.8652253746986389\n",
      "epoch 184: loss=0.8656354546546936\n",
      "epoch 185: loss=0.866377592086792\n",
      "epoch 186: loss=0.8653385043144226\n",
      "epoch 187: loss=0.8648866415023804\n",
      "epoch 188: loss=0.866287887096405\n",
      "epoch 189: loss=0.8654515147209167\n",
      "epoch 190: loss=0.8611885905265808\n",
      "epoch 191: loss=0.8645575046539307\n",
      "epoch 192: loss=0.8636252284049988\n",
      "epoch 193: loss=0.8630521297454834\n",
      "epoch 194: loss=0.8611024618148804\n",
      "epoch 195: loss=0.862373948097229\n",
      "epoch 196: loss=0.8621114492416382\n",
      "epoch 197: loss=0.8629415035247803\n",
      "epoch 198: loss=0.8631945252418518\n",
      "epoch 199: loss=0.8614104390144348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd4204445e74e929a0858962d94fb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 122094.546875\n",
      "Epoch 10, Loss: 30421.064453125\n",
      "Epoch 20, Loss: 21275.19140625\n",
      "Epoch 30, Loss: 17770.794921875\n",
      "Epoch 40, Loss: 16679.994140625\n",
      "Epoch 50, Loss: 16292.15234375\n",
      "Epoch 60, Loss: 16111.99609375\n",
      "Epoch 70, Loss: 16038.6845703125\n",
      "Epoch 80, Loss: 16006.6015625\n",
      "Epoch 90, Loss: 15989.8115234375\n",
      "Epoch 100, Loss: 15979.7060546875\n",
      "Epoch 110, Loss: 15972.40234375\n",
      "Epoch 120, Loss: 15966.47265625\n",
      "Epoch 130, Loss: 15961.4755859375\n",
      "Epoch 140, Loss: 15957.14453125\n",
      "Epoch 150, Loss: 15953.3330078125\n",
      "Epoch 160, Loss: 15949.9619140625\n",
      "Epoch 170, Loss: 15946.9658203125\n",
      "Epoch 180, Loss: 15944.3037109375\n",
      "Epoch 190, Loss: 15941.9326171875\n",
      "training patch with 267352 edges\n",
      "epoch 0: loss=9.861708641052246\n",
      "epoch 1: loss=9.447575569152832\n",
      "epoch 2: loss=8.98830509185791\n",
      "epoch 3: loss=8.387341499328613\n",
      "epoch 4: loss=7.526914596557617\n",
      "epoch 5: loss=6.656010150909424\n",
      "epoch 6: loss=5.981040000915527\n",
      "epoch 7: loss=5.764354228973389\n",
      "epoch 8: loss=5.5858540534973145\n",
      "epoch 9: loss=5.390693187713623\n",
      "epoch 10: loss=4.914121150970459\n",
      "epoch 11: loss=4.425450325012207\n",
      "epoch 12: loss=3.9331014156341553\n",
      "epoch 13: loss=3.54618763923645\n",
      "epoch 14: loss=3.2388787269592285\n",
      "epoch 15: loss=2.9792380332946777\n",
      "epoch 16: loss=2.7816829681396484\n",
      "epoch 17: loss=2.6118931770324707\n",
      "epoch 18: loss=2.481353759765625\n",
      "epoch 19: loss=2.3279078006744385\n",
      "epoch 20: loss=2.1939783096313477\n",
      "epoch 21: loss=2.0305655002593994\n",
      "epoch 22: loss=1.8748924732208252\n",
      "epoch 23: loss=1.7392321825027466\n",
      "epoch 24: loss=1.5956387519836426\n",
      "epoch 25: loss=1.4660792350769043\n",
      "epoch 26: loss=1.3476142883300781\n",
      "epoch 27: loss=1.2546113729476929\n",
      "epoch 28: loss=1.1861404180526733\n",
      "epoch 29: loss=1.1437057256698608\n",
      "epoch 30: loss=1.119930624961853\n",
      "epoch 31: loss=1.101637840270996\n",
      "epoch 32: loss=1.0738533735275269\n",
      "epoch 33: loss=1.0438015460968018\n",
      "epoch 34: loss=1.020616888999939\n",
      "epoch 35: loss=0.9974607229232788\n",
      "epoch 36: loss=0.9821676015853882\n",
      "epoch 37: loss=0.9724092483520508\n",
      "epoch 38: loss=0.9659981727600098\n",
      "epoch 39: loss=0.9570526480674744\n",
      "epoch 40: loss=0.9482951760292053\n",
      "epoch 41: loss=0.9441398978233337\n",
      "epoch 42: loss=0.9422391057014465\n",
      "epoch 43: loss=0.9376130104064941\n",
      "epoch 44: loss=0.9347171783447266\n",
      "epoch 45: loss=0.927853524684906\n",
      "epoch 46: loss=0.9230172634124756\n",
      "epoch 47: loss=0.919825553894043\n",
      "epoch 48: loss=0.9169787168502808\n",
      "epoch 49: loss=0.916767418384552\n",
      "epoch 50: loss=0.9168120622634888\n",
      "epoch 51: loss=0.9155707955360413\n",
      "epoch 52: loss=0.9103586077690125\n",
      "epoch 53: loss=0.9069762825965881\n",
      "epoch 54: loss=0.9062864184379578\n",
      "epoch 55: loss=0.9057013988494873\n",
      "epoch 56: loss=0.9058651924133301\n",
      "epoch 57: loss=0.9059187173843384\n",
      "epoch 58: loss=0.9036974906921387\n",
      "epoch 59: loss=0.9016348123550415\n",
      "epoch 60: loss=0.9004453420639038\n",
      "epoch 61: loss=0.8986746072769165\n",
      "epoch 62: loss=0.8972593545913696\n",
      "epoch 63: loss=0.8978980779647827\n",
      "epoch 64: loss=0.896136999130249\n",
      "epoch 65: loss=0.8967456221580505\n",
      "epoch 66: loss=0.8947121500968933\n",
      "epoch 67: loss=0.8943518400192261\n",
      "epoch 68: loss=0.8927838206291199\n",
      "epoch 69: loss=0.8918132185935974\n",
      "epoch 70: loss=0.8917075991630554\n",
      "epoch 71: loss=0.8921641111373901\n",
      "epoch 72: loss=0.8915226459503174\n",
      "epoch 73: loss=0.8909029960632324\n",
      "epoch 74: loss=0.8891168236732483\n",
      "epoch 75: loss=0.8894321918487549\n",
      "epoch 76: loss=0.8884164094924927\n",
      "epoch 77: loss=0.8885961771011353\n",
      "epoch 78: loss=0.886481523513794\n",
      "epoch 79: loss=0.886896014213562\n",
      "epoch 80: loss=0.8856695890426636\n",
      "epoch 81: loss=0.885761022567749\n",
      "epoch 82: loss=0.8849050998687744\n",
      "epoch 83: loss=0.8839207887649536\n",
      "epoch 84: loss=0.8815596699714661\n",
      "epoch 85: loss=0.881367027759552\n",
      "epoch 86: loss=0.8807864189147949\n",
      "epoch 87: loss=0.8801410794258118\n",
      "epoch 88: loss=0.8790668249130249\n",
      "epoch 89: loss=0.877323567867279\n",
      "epoch 90: loss=0.8770869374275208\n",
      "epoch 91: loss=0.8758222460746765\n",
      "epoch 92: loss=0.8756857514381409\n",
      "epoch 93: loss=0.8723506331443787\n",
      "epoch 94: loss=0.8710927963256836\n",
      "epoch 95: loss=0.8714585900306702\n",
      "epoch 96: loss=0.8707390427589417\n",
      "epoch 97: loss=0.8679118156433105\n",
      "epoch 98: loss=0.8673686385154724\n",
      "epoch 99: loss=0.8652527928352356\n",
      "epoch 100: loss=0.8645358085632324\n",
      "epoch 101: loss=0.8642318248748779\n",
      "epoch 102: loss=0.863336980342865\n",
      "epoch 103: loss=0.8625509142875671\n",
      "epoch 104: loss=0.8603932857513428\n",
      "epoch 105: loss=0.8608183860778809\n",
      "epoch 106: loss=0.8582075834274292\n",
      "epoch 107: loss=0.8583138585090637\n",
      "epoch 108: loss=0.8584758043289185\n",
      "epoch 109: loss=0.8574488162994385\n",
      "epoch 110: loss=0.8569640517234802\n",
      "epoch 111: loss=0.856906533241272\n",
      "epoch 112: loss=0.8551631569862366\n",
      "epoch 113: loss=0.8535027503967285\n",
      "epoch 114: loss=0.8529192209243774\n",
      "epoch 115: loss=0.8522104620933533\n",
      "epoch 116: loss=0.8517385721206665\n",
      "epoch 117: loss=0.8514580726623535\n",
      "epoch 118: loss=0.8501611948013306\n",
      "epoch 119: loss=0.8498615026473999\n",
      "epoch 120: loss=0.8489577174186707\n",
      "epoch 121: loss=0.848751425743103\n",
      "epoch 122: loss=0.8485153913497925\n",
      "epoch 123: loss=0.8467885255813599\n",
      "epoch 124: loss=0.8454515933990479\n",
      "epoch 125: loss=0.847037672996521\n",
      "epoch 126: loss=0.8457037210464478\n",
      "epoch 127: loss=0.8434805274009705\n",
      "epoch 128: loss=0.8447092771530151\n",
      "epoch 129: loss=0.8434131145477295\n",
      "epoch 130: loss=0.8422962427139282\n",
      "epoch 131: loss=0.8435603380203247\n",
      "epoch 132: loss=0.8415591716766357\n",
      "epoch 133: loss=0.839870810508728\n",
      "epoch 134: loss=0.8415517807006836\n",
      "epoch 135: loss=0.839506983757019\n",
      "epoch 136: loss=0.8385655283927917\n",
      "epoch 137: loss=0.8384538292884827\n",
      "epoch 138: loss=0.8374702334403992\n",
      "epoch 139: loss=0.836878776550293\n",
      "epoch 140: loss=0.8373292088508606\n",
      "epoch 141: loss=0.8362486958503723\n",
      "epoch 142: loss=0.8360602855682373\n",
      "epoch 143: loss=0.8351036310195923\n",
      "epoch 144: loss=0.8345547914505005\n",
      "epoch 145: loss=0.8337785601615906\n",
      "epoch 146: loss=0.8340185880661011\n",
      "epoch 147: loss=0.8327785730361938\n",
      "epoch 148: loss=0.8340151906013489\n",
      "epoch 149: loss=0.8318851590156555\n",
      "epoch 150: loss=0.8320598602294922\n",
      "epoch 151: loss=0.8309121131896973\n",
      "epoch 152: loss=0.8306897282600403\n",
      "epoch 153: loss=0.8289738893508911\n",
      "epoch 154: loss=0.829386830329895\n",
      "epoch 155: loss=0.8296043872833252\n",
      "epoch 156: loss=0.8287323117256165\n",
      "epoch 157: loss=0.8284534811973572\n",
      "epoch 158: loss=0.82797771692276\n",
      "epoch 159: loss=0.8267412185668945\n",
      "epoch 160: loss=0.8260042667388916\n",
      "epoch 161: loss=0.8258306384086609\n",
      "epoch 162: loss=0.8248966336250305\n",
      "epoch 163: loss=0.8227057456970215\n",
      "epoch 164: loss=0.8234326243400574\n",
      "epoch 165: loss=0.8233587145805359\n",
      "epoch 166: loss=0.8235394954681396\n",
      "epoch 167: loss=0.8231737613677979\n",
      "epoch 168: loss=0.8226832151412964\n",
      "epoch 169: loss=0.8214929699897766\n",
      "epoch 170: loss=0.8221145272254944\n",
      "epoch 171: loss=0.8209353685379028\n",
      "epoch 172: loss=0.820218026638031\n",
      "epoch 173: loss=0.8190752267837524\n",
      "epoch 174: loss=0.82036954164505\n",
      "epoch 175: loss=0.8210610747337341\n",
      "epoch 176: loss=0.8191215991973877\n",
      "epoch 177: loss=0.8181207180023193\n",
      "epoch 178: loss=0.8169432878494263\n",
      "epoch 179: loss=0.8172358870506287\n",
      "epoch 180: loss=0.8159303665161133\n",
      "epoch 181: loss=0.8151513338088989\n",
      "epoch 182: loss=0.8169916868209839\n",
      "epoch 183: loss=0.8156808018684387\n",
      "epoch 184: loss=0.8147017955780029\n",
      "epoch 185: loss=0.8150122165679932\n",
      "epoch 186: loss=0.8153337836265564\n",
      "epoch 187: loss=0.8133135437965393\n",
      "epoch 188: loss=0.8134059906005859\n",
      "epoch 189: loss=0.8147032856941223\n",
      "epoch 190: loss=0.8128800988197327\n",
      "epoch 191: loss=0.813102126121521\n",
      "epoch 192: loss=0.8128698468208313\n",
      "epoch 193: loss=0.8113700747489929\n",
      "epoch 194: loss=0.8106891512870789\n",
      "epoch 195: loss=0.8119015693664551\n",
      "epoch 196: loss=0.8103041648864746\n",
      "epoch 197: loss=0.8102536201477051\n",
      "epoch 198: loss=0.811012864112854\n",
      "epoch 199: loss=0.8111206293106079\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=9.811635971069336\n",
      "epoch 1: loss=9.530110359191895\n",
      "epoch 2: loss=9.031689643859863\n",
      "epoch 3: loss=8.406106948852539\n",
      "epoch 4: loss=7.560401439666748\n",
      "epoch 5: loss=6.574584007263184\n",
      "epoch 6: loss=5.8310747146606445\n",
      "epoch 7: loss=5.5276689529418945\n",
      "epoch 8: loss=5.506298542022705\n",
      "epoch 9: loss=5.17886209487915\n",
      "epoch 10: loss=4.720123291015625\n",
      "epoch 11: loss=4.13443660736084\n",
      "epoch 12: loss=3.6474087238311768\n",
      "epoch 13: loss=3.2569401264190674\n",
      "epoch 14: loss=2.986572742462158\n",
      "epoch 15: loss=2.8038063049316406\n",
      "epoch 16: loss=2.6610889434814453\n",
      "epoch 17: loss=2.518197536468506\n",
      "epoch 18: loss=2.3936753273010254\n",
      "epoch 19: loss=2.2521893978118896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss=2.1176083087921143\n",
      "epoch 21: loss=2.010505437850952\n",
      "epoch 22: loss=1.9156913757324219\n",
      "epoch 23: loss=1.7821160554885864\n",
      "epoch 24: loss=1.6490778923034668\n",
      "epoch 25: loss=1.5237982273101807\n",
      "epoch 26: loss=1.4171841144561768\n",
      "epoch 27: loss=1.3281255960464478\n",
      "epoch 28: loss=1.2616883516311646\n",
      "epoch 29: loss=1.226016879081726\n",
      "epoch 30: loss=1.195623755455017\n",
      "epoch 31: loss=1.167357325553894\n",
      "epoch 32: loss=1.1217021942138672\n",
      "epoch 33: loss=1.087080717086792\n",
      "epoch 34: loss=1.065459132194519\n",
      "epoch 35: loss=1.0542895793914795\n",
      "epoch 36: loss=1.042366862297058\n",
      "epoch 37: loss=1.0324546098709106\n",
      "epoch 38: loss=1.0229558944702148\n",
      "epoch 39: loss=1.00981605052948\n",
      "epoch 40: loss=0.9914471507072449\n",
      "epoch 41: loss=0.973731279373169\n",
      "epoch 42: loss=0.9634819030761719\n",
      "epoch 43: loss=0.9587000608444214\n",
      "epoch 44: loss=0.9559217095375061\n",
      "epoch 45: loss=0.9567914009094238\n",
      "epoch 46: loss=0.9518857598304749\n",
      "epoch 47: loss=0.943977415561676\n",
      "epoch 48: loss=0.9359396696090698\n",
      "epoch 49: loss=0.9295119047164917\n",
      "epoch 50: loss=0.9261962175369263\n",
      "epoch 51: loss=0.9256053566932678\n",
      "epoch 52: loss=0.9277358055114746\n",
      "epoch 53: loss=0.9242910146713257\n",
      "epoch 54: loss=0.9200137853622437\n",
      "epoch 55: loss=0.9174522757530212\n",
      "epoch 56: loss=0.9153619408607483\n",
      "epoch 57: loss=0.9134792685508728\n",
      "epoch 58: loss=0.9127673506736755\n",
      "epoch 59: loss=0.9114367961883545\n",
      "epoch 60: loss=0.9093707203865051\n",
      "epoch 61: loss=0.9090178608894348\n",
      "epoch 62: loss=0.9076598286628723\n",
      "epoch 63: loss=0.9061319828033447\n",
      "epoch 64: loss=0.9031556248664856\n",
      "epoch 65: loss=0.9042637348175049\n",
      "epoch 66: loss=0.9038305282592773\n",
      "epoch 67: loss=0.9027642011642456\n",
      "epoch 68: loss=0.9011685848236084\n",
      "epoch 69: loss=0.9014720916748047\n",
      "epoch 70: loss=0.8999487161636353\n",
      "epoch 71: loss=0.9017229080200195\n",
      "epoch 72: loss=0.8973327279090881\n",
      "epoch 73: loss=0.8972780704498291\n",
      "epoch 74: loss=0.8969249129295349\n",
      "epoch 75: loss=0.8969287872314453\n",
      "epoch 76: loss=0.8967297077178955\n",
      "epoch 77: loss=0.8991535305976868\n",
      "epoch 78: loss=0.8967840075492859\n",
      "epoch 79: loss=0.8958899974822998\n",
      "epoch 80: loss=0.895378828048706\n",
      "epoch 81: loss=0.8960835337638855\n",
      "epoch 82: loss=0.8945958614349365\n",
      "epoch 83: loss=0.8936136364936829\n",
      "epoch 84: loss=0.8928807377815247\n",
      "epoch 85: loss=0.8935391306877136\n",
      "epoch 86: loss=0.8943001627922058\n",
      "epoch 87: loss=0.8940553665161133\n",
      "epoch 88: loss=0.8915114998817444\n",
      "epoch 89: loss=0.89119952917099\n",
      "epoch 90: loss=0.8940833210945129\n",
      "epoch 91: loss=0.8923220634460449\n",
      "epoch 92: loss=0.8909792900085449\n",
      "epoch 93: loss=0.8924600481987\n",
      "epoch 94: loss=0.8925615549087524\n",
      "epoch 95: loss=0.8903824687004089\n",
      "epoch 96: loss=0.8923254609107971\n",
      "epoch 97: loss=0.8919544219970703\n",
      "epoch 98: loss=0.8918927907943726\n",
      "epoch 99: loss=0.8903896808624268\n",
      "epoch 100: loss=0.8919349908828735\n",
      "epoch 101: loss=0.890770673751831\n",
      "epoch 102: loss=0.8894396424293518\n",
      "epoch 103: loss=0.8895015120506287\n",
      "epoch 104: loss=0.8908570408821106\n",
      "epoch 105: loss=0.8883940577507019\n",
      "epoch 106: loss=0.8898137807846069\n",
      "epoch 107: loss=0.8886991739273071\n",
      "epoch 108: loss=0.8894505500793457\n",
      "epoch 109: loss=0.8882516026496887\n",
      "epoch 110: loss=0.8869103193283081\n",
      "epoch 111: loss=0.887807309627533\n",
      "epoch 112: loss=0.88765949010849\n",
      "epoch 113: loss=0.8857951760292053\n",
      "epoch 114: loss=0.8883876204490662\n",
      "epoch 115: loss=0.8859174251556396\n",
      "epoch 116: loss=0.8872685432434082\n",
      "epoch 117: loss=0.8865503668785095\n",
      "epoch 118: loss=0.8871835470199585\n",
      "epoch 119: loss=0.8875173330307007\n",
      "epoch 120: loss=0.8844800591468811\n",
      "epoch 121: loss=0.8851401805877686\n",
      "epoch 122: loss=0.8840402960777283\n",
      "epoch 123: loss=0.8839057683944702\n",
      "epoch 124: loss=0.8832642436027527\n",
      "epoch 125: loss=0.8845611214637756\n",
      "epoch 126: loss=0.8822839856147766\n",
      "epoch 127: loss=0.8818777799606323\n",
      "epoch 128: loss=0.8835543990135193\n",
      "epoch 129: loss=0.8810109496116638\n",
      "epoch 130: loss=0.8831356167793274\n",
      "epoch 131: loss=0.8821622133255005\n",
      "epoch 132: loss=0.8798745274543762\n",
      "epoch 133: loss=0.8792073130607605\n",
      "epoch 134: loss=0.8787981867790222\n",
      "epoch 135: loss=0.8792324066162109\n",
      "epoch 136: loss=0.8782258629798889\n",
      "epoch 137: loss=0.8757169246673584\n",
      "epoch 138: loss=0.8765899538993835\n",
      "epoch 139: loss=0.8735641837120056\n",
      "epoch 140: loss=0.8737507462501526\n",
      "epoch 141: loss=0.8738163709640503\n",
      "epoch 142: loss=0.8729866147041321\n",
      "epoch 143: loss=0.8728270530700684\n",
      "epoch 144: loss=0.8706110119819641\n",
      "epoch 145: loss=0.8688182234764099\n",
      "epoch 146: loss=0.8675998449325562\n",
      "epoch 147: loss=0.8679429292678833\n",
      "epoch 148: loss=0.8673855662345886\n",
      "epoch 149: loss=0.8674992322921753\n",
      "epoch 150: loss=0.8650767803192139\n",
      "epoch 151: loss=0.863676130771637\n",
      "epoch 152: loss=0.8634196519851685\n",
      "epoch 153: loss=0.8630035519599915\n",
      "epoch 154: loss=0.8608346581459045\n",
      "epoch 155: loss=0.8600878119468689\n",
      "epoch 156: loss=0.859488308429718\n",
      "epoch 157: loss=0.8565170764923096\n",
      "epoch 158: loss=0.8544948101043701\n",
      "epoch 159: loss=0.8532861471176147\n",
      "epoch 160: loss=0.851377010345459\n",
      "epoch 161: loss=0.8500398397445679\n",
      "epoch 162: loss=0.8493101596832275\n",
      "epoch 163: loss=0.8475498557090759\n",
      "epoch 164: loss=0.8458589315414429\n",
      "epoch 165: loss=0.8446195125579834\n",
      "epoch 166: loss=0.8463690876960754\n",
      "epoch 167: loss=0.8435022830963135\n",
      "epoch 168: loss=0.8419511318206787\n",
      "epoch 169: loss=0.842166006565094\n",
      "epoch 170: loss=0.8405706882476807\n",
      "epoch 171: loss=0.8408513069152832\n",
      "epoch 172: loss=0.8403843641281128\n",
      "epoch 173: loss=0.8397033214569092\n",
      "epoch 174: loss=0.8388592600822449\n",
      "epoch 175: loss=0.8383137583732605\n",
      "epoch 176: loss=0.8379284739494324\n",
      "epoch 177: loss=0.838805079460144\n",
      "epoch 178: loss=0.8362890481948853\n",
      "epoch 179: loss=0.8369011282920837\n",
      "epoch 180: loss=0.8350040912628174\n",
      "epoch 181: loss=0.8348472714424133\n",
      "epoch 182: loss=0.8361323475837708\n",
      "epoch 183: loss=0.835023045539856\n",
      "epoch 184: loss=0.8341127634048462\n",
      "epoch 185: loss=0.8325883746147156\n",
      "epoch 186: loss=0.8352470993995667\n",
      "epoch 187: loss=0.8339184522628784\n",
      "epoch 188: loss=0.8331027626991272\n",
      "epoch 189: loss=0.8327374458312988\n",
      "epoch 190: loss=0.8326739072799683\n",
      "epoch 191: loss=0.8325622081756592\n",
      "epoch 192: loss=0.8342391848564148\n",
      "epoch 193: loss=0.8309447765350342\n",
      "epoch 194: loss=0.8343763947486877\n",
      "epoch 195: loss=0.8303964734077454\n",
      "epoch 196: loss=0.832359790802002\n",
      "epoch 197: loss=0.8302643299102783\n",
      "epoch 198: loss=0.8311049342155457\n",
      "epoch 199: loss=0.8307409882545471\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=9.819507598876953\n",
      "epoch 1: loss=9.513671875\n",
      "epoch 2: loss=9.138997077941895\n",
      "epoch 3: loss=8.691130638122559\n",
      "epoch 4: loss=8.025224685668945\n",
      "epoch 5: loss=7.311130523681641\n",
      "epoch 6: loss=6.504281997680664\n",
      "epoch 7: loss=5.9108500480651855\n",
      "epoch 8: loss=5.644112586975098\n",
      "epoch 9: loss=5.473823547363281\n",
      "epoch 10: loss=5.0994110107421875\n",
      "epoch 11: loss=4.5233588218688965\n",
      "epoch 12: loss=3.954503297805786\n",
      "epoch 13: loss=3.4485788345336914\n",
      "epoch 14: loss=3.0944032669067383\n",
      "epoch 15: loss=2.824723482131958\n",
      "epoch 16: loss=2.6353538036346436\n",
      "epoch 17: loss=2.4471914768218994\n",
      "epoch 18: loss=2.3038458824157715\n",
      "epoch 19: loss=2.1871628761291504\n",
      "epoch 20: loss=2.0714542865753174\n",
      "epoch 21: loss=1.9718339443206787\n",
      "epoch 22: loss=1.8303638696670532\n",
      "epoch 23: loss=1.7101835012435913\n",
      "epoch 24: loss=1.5800312757492065\n",
      "epoch 25: loss=1.4667699337005615\n",
      "epoch 26: loss=1.37201726436615\n",
      "epoch 27: loss=1.3035067319869995\n",
      "epoch 28: loss=1.2464712858200073\n",
      "epoch 29: loss=1.1948457956314087\n",
      "epoch 30: loss=1.1649682521820068\n",
      "epoch 31: loss=1.1486315727233887\n",
      "epoch 32: loss=1.1349148750305176\n",
      "epoch 33: loss=1.1240949630737305\n",
      "epoch 34: loss=1.1006015539169312\n",
      "epoch 35: loss=1.0825321674346924\n",
      "epoch 36: loss=1.059473991394043\n",
      "epoch 37: loss=1.0423933267593384\n",
      "epoch 38: loss=1.032809853553772\n",
      "epoch 39: loss=1.027174472808838\n",
      "epoch 40: loss=1.0294907093048096\n",
      "epoch 41: loss=1.022793173789978\n",
      "epoch 42: loss=1.0198523998260498\n",
      "epoch 43: loss=1.011143445968628\n",
      "epoch 44: loss=1.0056109428405762\n",
      "epoch 45: loss=0.9973930716514587\n",
      "epoch 46: loss=0.9937968850135803\n",
      "epoch 47: loss=0.9884795546531677\n",
      "epoch 48: loss=0.990007221698761\n",
      "epoch 49: loss=0.991321325302124\n",
      "epoch 50: loss=0.9888214468955994\n",
      "epoch 51: loss=0.9783979654312134\n",
      "epoch 52: loss=0.9778782725334167\n",
      "epoch 53: loss=0.9794706106185913\n",
      "epoch 54: loss=0.9731587171554565\n",
      "epoch 55: loss=0.9736723303794861\n",
      "epoch 56: loss=0.9740719199180603\n",
      "epoch 57: loss=0.9744904637336731\n",
      "epoch 58: loss=0.9715827107429504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59: loss=0.9712032675743103\n",
      "epoch 60: loss=0.9651692509651184\n",
      "epoch 61: loss=0.9676569700241089\n",
      "epoch 62: loss=0.9681259393692017\n",
      "epoch 63: loss=0.9646897315979004\n",
      "epoch 64: loss=0.9639387130737305\n",
      "epoch 65: loss=0.9635433554649353\n",
      "epoch 66: loss=0.965216875076294\n",
      "epoch 67: loss=0.9621334075927734\n",
      "epoch 68: loss=0.9610235691070557\n",
      "epoch 69: loss=0.960983395576477\n",
      "epoch 70: loss=0.9608465433120728\n",
      "epoch 71: loss=0.9579620361328125\n",
      "epoch 72: loss=0.9575244784355164\n",
      "epoch 73: loss=0.9620955586433411\n",
      "epoch 74: loss=0.9570315480232239\n",
      "epoch 75: loss=0.9564828276634216\n",
      "epoch 76: loss=0.9551547765731812\n",
      "epoch 77: loss=0.9538652896881104\n",
      "epoch 78: loss=0.9527599811553955\n",
      "epoch 79: loss=0.9537219405174255\n",
      "epoch 80: loss=0.9556775689125061\n",
      "epoch 81: loss=0.9504110813140869\n",
      "epoch 82: loss=0.9506956934928894\n",
      "epoch 83: loss=0.9522027373313904\n",
      "epoch 84: loss=0.9502979516983032\n",
      "epoch 85: loss=0.9526726007461548\n",
      "epoch 86: loss=0.9487161636352539\n",
      "epoch 87: loss=0.9457188844680786\n",
      "epoch 88: loss=0.9497833251953125\n",
      "epoch 89: loss=0.9447621703147888\n",
      "epoch 90: loss=0.9445080161094666\n",
      "epoch 91: loss=0.9420551061630249\n",
      "epoch 92: loss=0.9439608454704285\n",
      "epoch 93: loss=0.9421221017837524\n",
      "epoch 94: loss=0.9425054788589478\n",
      "epoch 95: loss=0.9397047162055969\n",
      "epoch 96: loss=0.9375146627426147\n",
      "epoch 97: loss=0.9365971088409424\n",
      "epoch 98: loss=0.9364774227142334\n",
      "epoch 99: loss=0.9363580346107483\n",
      "epoch 100: loss=0.9365449547767639\n",
      "epoch 101: loss=0.9328896403312683\n",
      "epoch 102: loss=0.9350065588951111\n",
      "epoch 103: loss=0.9301104545593262\n",
      "epoch 104: loss=0.9265779852867126\n",
      "epoch 105: loss=0.9243559837341309\n",
      "epoch 106: loss=0.9244898557662964\n",
      "epoch 107: loss=0.9217942953109741\n",
      "epoch 108: loss=0.9202826619148254\n",
      "epoch 109: loss=0.9204390645027161\n",
      "epoch 110: loss=0.9192330837249756\n",
      "epoch 111: loss=0.9186489582061768\n",
      "epoch 112: loss=0.9154320955276489\n",
      "epoch 113: loss=0.9175146818161011\n",
      "epoch 114: loss=0.9182763695716858\n",
      "epoch 115: loss=0.9132737517356873\n",
      "epoch 116: loss=0.9145408868789673\n",
      "epoch 117: loss=0.9135480523109436\n",
      "epoch 118: loss=0.9130314588546753\n",
      "epoch 119: loss=0.9095090627670288\n",
      "epoch 120: loss=0.9091870188713074\n",
      "epoch 121: loss=0.9106825590133667\n",
      "epoch 122: loss=0.90886390209198\n",
      "epoch 123: loss=0.9084179401397705\n",
      "epoch 124: loss=0.910858690738678\n",
      "epoch 125: loss=0.9069796800613403\n",
      "epoch 126: loss=0.9081681966781616\n",
      "epoch 127: loss=0.9061330556869507\n",
      "epoch 128: loss=0.9068382382392883\n",
      "epoch 129: loss=0.9055721163749695\n",
      "epoch 130: loss=0.9066036939620972\n",
      "epoch 131: loss=0.9036974310874939\n",
      "epoch 132: loss=0.9041747450828552\n",
      "epoch 133: loss=0.9043756127357483\n",
      "epoch 134: loss=0.9036607146263123\n",
      "epoch 135: loss=0.900189995765686\n",
      "epoch 136: loss=0.9006389379501343\n",
      "epoch 137: loss=0.9038519263267517\n",
      "epoch 138: loss=0.9001578092575073\n",
      "epoch 139: loss=0.9027020335197449\n",
      "epoch 140: loss=0.9003823399543762\n",
      "epoch 141: loss=0.8991116285324097\n",
      "epoch 142: loss=0.9005102515220642\n",
      "epoch 143: loss=0.8962502479553223\n",
      "epoch 144: loss=0.8992099165916443\n",
      "epoch 145: loss=0.8975821137428284\n",
      "epoch 146: loss=0.8986861109733582\n",
      "epoch 147: loss=0.8990721106529236\n",
      "epoch 148: loss=0.8963226079940796\n",
      "epoch 149: loss=0.8967490196228027\n",
      "epoch 150: loss=0.8952471613883972\n",
      "epoch 151: loss=0.8936945796012878\n",
      "epoch 152: loss=0.8958289623260498\n",
      "epoch 153: loss=0.8943764567375183\n",
      "epoch 154: loss=0.8943281769752502\n",
      "epoch 155: loss=0.8951160907745361\n",
      "epoch 156: loss=0.8943578004837036\n",
      "epoch 157: loss=0.8911049365997314\n",
      "epoch 158: loss=0.891294002532959\n",
      "epoch 159: loss=0.8922675848007202\n",
      "epoch 160: loss=0.893231213092804\n",
      "epoch 161: loss=0.891767144203186\n",
      "epoch 162: loss=0.8904767632484436\n",
      "epoch 163: loss=0.8886553049087524\n",
      "epoch 164: loss=0.8922773599624634\n",
      "epoch 165: loss=0.8897035717964172\n",
      "epoch 166: loss=0.8881403803825378\n",
      "epoch 167: loss=0.8896010518074036\n",
      "epoch 168: loss=0.888823926448822\n",
      "epoch 169: loss=0.8895823359489441\n",
      "epoch 170: loss=0.888909637928009\n",
      "epoch 171: loss=0.8896793723106384\n",
      "epoch 172: loss=0.8890780806541443\n",
      "epoch 173: loss=0.8858392238616943\n",
      "epoch 174: loss=0.8872949481010437\n",
      "epoch 175: loss=0.8859412670135498\n",
      "epoch 176: loss=0.8879358172416687\n",
      "epoch 177: loss=0.8872096538543701\n",
      "epoch 178: loss=0.8839429616928101\n",
      "epoch 179: loss=0.884863018989563\n",
      "epoch 180: loss=0.8846054077148438\n",
      "epoch 181: loss=0.8880023956298828\n",
      "epoch 182: loss=0.884299099445343\n",
      "epoch 183: loss=0.8824717402458191\n",
      "epoch 184: loss=0.8828439712524414\n",
      "epoch 185: loss=0.8844289183616638\n",
      "epoch 186: loss=0.8846783638000488\n",
      "epoch 187: loss=0.8822612166404724\n",
      "epoch 188: loss=0.8840440511703491\n",
      "epoch 189: loss=0.8840715289115906\n",
      "epoch 190: loss=0.8825417757034302\n",
      "epoch 191: loss=0.8832108378410339\n",
      "epoch 192: loss=0.8810194134712219\n",
      "epoch 193: loss=0.8808830380439758\n",
      "epoch 194: loss=0.8825478553771973\n",
      "epoch 195: loss=0.8814446926116943\n",
      "epoch 196: loss=0.8821414709091187\n",
      "epoch 197: loss=0.877363383769989\n",
      "epoch 198: loss=0.8785578012466431\n",
      "epoch 199: loss=0.8798161149024963\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=9.870534896850586\n",
      "epoch 1: loss=9.811715126037598\n",
      "epoch 2: loss=9.190282821655273\n",
      "epoch 3: loss=8.950796127319336\n",
      "epoch 4: loss=8.160799026489258\n",
      "epoch 5: loss=7.1603217124938965\n",
      "epoch 6: loss=7.281190872192383\n",
      "epoch 7: loss=6.965138912200928\n",
      "epoch 8: loss=6.184420585632324\n",
      "epoch 9: loss=6.125598907470703\n",
      "epoch 10: loss=6.221040725708008\n",
      "epoch 11: loss=5.6629638671875\n",
      "epoch 12: loss=5.218106269836426\n",
      "epoch 13: loss=5.10273551940918\n",
      "epoch 14: loss=4.422011375427246\n",
      "epoch 15: loss=4.420913219451904\n",
      "epoch 16: loss=4.3058600425720215\n",
      "epoch 17: loss=3.368889093399048\n",
      "epoch 18: loss=3.1661570072174072\n",
      "epoch 19: loss=2.89412260055542\n",
      "epoch 20: loss=2.62516450881958\n",
      "epoch 21: loss=3.0146965980529785\n",
      "epoch 22: loss=2.1862494945526123\n",
      "epoch 23: loss=2.223365545272827\n",
      "epoch 24: loss=2.042937994003296\n",
      "epoch 25: loss=2.0973126888275146\n",
      "epoch 26: loss=1.9002835750579834\n",
      "epoch 27: loss=1.963838815689087\n",
      "epoch 28: loss=1.7838553190231323\n",
      "epoch 29: loss=1.8205361366271973\n",
      "epoch 30: loss=1.8280396461486816\n",
      "epoch 31: loss=1.8572099208831787\n",
      "epoch 32: loss=1.886548638343811\n",
      "epoch 33: loss=1.7208389043807983\n",
      "epoch 34: loss=1.6721985340118408\n",
      "epoch 35: loss=1.4994440078735352\n",
      "epoch 36: loss=1.5513685941696167\n",
      "epoch 37: loss=1.5903228521347046\n",
      "epoch 38: loss=1.6156941652297974\n",
      "epoch 39: loss=1.6642638444900513\n",
      "epoch 40: loss=1.7024829387664795\n",
      "epoch 41: loss=1.6463695764541626\n",
      "epoch 42: loss=1.6242320537567139\n",
      "epoch 43: loss=1.522064447402954\n",
      "epoch 44: loss=1.624849796295166\n",
      "epoch 45: loss=1.5491340160369873\n",
      "epoch 46: loss=1.5853004455566406\n",
      "epoch 47: loss=1.5764158964157104\n",
      "epoch 48: loss=1.5384739637374878\n",
      "epoch 49: loss=1.5847793817520142\n",
      "epoch 50: loss=1.538308024406433\n",
      "epoch 51: loss=1.5934786796569824\n",
      "epoch 52: loss=1.554431676864624\n",
      "epoch 53: loss=1.4624476432800293\n",
      "epoch 54: loss=1.5920625925064087\n",
      "epoch 55: loss=1.6036293506622314\n",
      "epoch 56: loss=1.5317169427871704\n",
      "epoch 57: loss=1.552518606185913\n",
      "epoch 58: loss=1.4937800168991089\n",
      "epoch 59: loss=1.4902929067611694\n",
      "epoch 60: loss=1.588685154914856\n",
      "epoch 61: loss=1.712998628616333\n",
      "epoch 62: loss=1.5177539587020874\n",
      "epoch 63: loss=1.5544028282165527\n",
      "epoch 64: loss=1.511360764503479\n",
      "epoch 65: loss=1.4468088150024414\n",
      "epoch 66: loss=1.5370444059371948\n",
      "epoch 67: loss=1.5408616065979004\n",
      "epoch 68: loss=1.5557196140289307\n",
      "epoch 69: loss=1.438981056213379\n",
      "epoch 70: loss=1.527069330215454\n",
      "epoch 71: loss=1.5746568441390991\n",
      "epoch 72: loss=1.4441725015640259\n",
      "epoch 73: loss=1.4458674192428589\n",
      "epoch 74: loss=1.5369154214859009\n",
      "epoch 75: loss=1.5136401653289795\n",
      "epoch 76: loss=1.4945547580718994\n",
      "epoch 77: loss=1.5330548286437988\n",
      "epoch 78: loss=1.4657992124557495\n",
      "epoch 79: loss=1.4700281620025635\n",
      "epoch 80: loss=1.548758864402771\n",
      "epoch 81: loss=1.5235276222229004\n",
      "epoch 82: loss=1.4742597341537476\n",
      "epoch 83: loss=1.493431568145752\n",
      "epoch 84: loss=1.4566620588302612\n",
      "epoch 85: loss=1.6572470664978027\n",
      "epoch 86: loss=1.4314687252044678\n",
      "epoch 87: loss=1.4613018035888672\n",
      "epoch 88: loss=1.4972542524337769\n",
      "epoch 89: loss=1.51954185962677\n",
      "epoch 90: loss=1.5151883363723755\n",
      "epoch 91: loss=1.5248335599899292\n",
      "epoch 92: loss=1.5393438339233398\n",
      "epoch 93: loss=1.583072304725647\n",
      "epoch 94: loss=1.588104248046875\n",
      "epoch 95: loss=1.4916948080062866\n",
      "epoch 96: loss=1.4843072891235352\n",
      "epoch 97: loss=1.474824070930481\n",
      "epoch 98: loss=1.4952466487884521\n",
      "epoch 99: loss=1.5686686038970947\n",
      "epoch 100: loss=1.531461238861084\n",
      "epoch 101: loss=1.5125526189804077\n",
      "epoch 102: loss=1.5082002878189087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103: loss=1.6005772352218628\n",
      "epoch 104: loss=1.5195773839950562\n",
      "epoch 105: loss=1.5475599765777588\n",
      "epoch 106: loss=1.462619662284851\n",
      "epoch 107: loss=1.594177007675171\n",
      "epoch 108: loss=1.4684945344924927\n",
      "epoch 109: loss=1.4803881645202637\n",
      "epoch 110: loss=1.5105096101760864\n",
      "epoch 111: loss=1.4886226654052734\n",
      "epoch 112: loss=1.557390809059143\n",
      "epoch 113: loss=1.560232162475586\n",
      "epoch 114: loss=1.488142490386963\n",
      "epoch 115: loss=1.4776169061660767\n",
      "epoch 116: loss=1.468929409980774\n",
      "epoch 117: loss=1.5237846374511719\n",
      "epoch 118: loss=1.5425996780395508\n",
      "epoch 119: loss=1.6224311590194702\n",
      "epoch 120: loss=1.5254464149475098\n",
      "epoch 121: loss=1.6323347091674805\n",
      "epoch 122: loss=1.5127984285354614\n",
      "epoch 123: loss=1.6033695936203003\n",
      "epoch 124: loss=1.4691638946533203\n",
      "epoch 125: loss=1.4666054248809814\n",
      "epoch 126: loss=1.4856047630310059\n",
      "epoch 127: loss=1.4656240940093994\n",
      "epoch 128: loss=1.4456425905227661\n",
      "epoch 129: loss=1.6026285886764526\n",
      "epoch 130: loss=1.5482017993927002\n",
      "epoch 131: loss=1.5996088981628418\n",
      "epoch 132: loss=1.4733808040618896\n",
      "epoch 133: loss=1.5822190046310425\n",
      "epoch 134: loss=1.5054879188537598\n",
      "epoch 135: loss=1.5525360107421875\n",
      "epoch 136: loss=1.6283023357391357\n",
      "epoch 137: loss=1.5816535949707031\n",
      "epoch 138: loss=1.457271933555603\n",
      "epoch 139: loss=1.4958255290985107\n",
      "epoch 140: loss=1.5536669492721558\n",
      "epoch 141: loss=1.5590301752090454\n",
      "epoch 142: loss=1.501354694366455\n",
      "epoch 143: loss=1.4686336517333984\n",
      "epoch 144: loss=1.5115270614624023\n",
      "epoch 145: loss=1.4877725839614868\n",
      "epoch 146: loss=1.5011234283447266\n",
      "epoch 147: loss=1.462831974029541\n",
      "epoch 148: loss=1.4814876317977905\n",
      "epoch 149: loss=1.420680284500122\n",
      "epoch 150: loss=1.4773056507110596\n",
      "epoch 151: loss=1.4775288105010986\n",
      "epoch 152: loss=1.5521655082702637\n",
      "epoch 153: loss=1.5871307849884033\n",
      "epoch 154: loss=1.4773212671279907\n",
      "epoch 155: loss=1.4468724727630615\n",
      "epoch 156: loss=1.445982575416565\n",
      "epoch 157: loss=1.5314967632293701\n",
      "epoch 158: loss=1.435530662536621\n",
      "epoch 159: loss=1.5229657888412476\n",
      "epoch 160: loss=1.5681203603744507\n",
      "epoch 161: loss=1.469520926475525\n",
      "epoch 162: loss=1.4572861194610596\n",
      "epoch 163: loss=1.5710116624832153\n",
      "epoch 164: loss=1.5416697263717651\n",
      "epoch 165: loss=1.5229710340499878\n",
      "epoch 166: loss=1.5569841861724854\n",
      "epoch 167: loss=1.4725377559661865\n",
      "epoch 168: loss=1.499239444732666\n",
      "epoch 169: loss=1.4835147857666016\n",
      "epoch 170: loss=1.4893195629119873\n",
      "epoch 171: loss=1.572049856185913\n",
      "epoch 172: loss=1.4879252910614014\n",
      "epoch 173: loss=1.5282493829727173\n",
      "epoch 174: loss=1.526263952255249\n",
      "epoch 175: loss=1.441654920578003\n",
      "epoch 176: loss=1.4853928089141846\n",
      "epoch 177: loss=1.4676522016525269\n",
      "epoch 178: loss=1.4775193929672241\n",
      "epoch 179: loss=1.6005406379699707\n",
      "epoch 180: loss=1.5197237730026245\n",
      "epoch 181: loss=1.5005179643630981\n",
      "epoch 182: loss=1.4267760515213013\n",
      "epoch 183: loss=1.4652798175811768\n",
      "epoch 184: loss=1.473502516746521\n",
      "epoch 185: loss=1.4685752391815186\n",
      "epoch 186: loss=1.4880497455596924\n",
      "epoch 187: loss=1.4777772426605225\n",
      "epoch 188: loss=1.456437110900879\n",
      "epoch 189: loss=1.5009222030639648\n",
      "epoch 190: loss=1.4898500442504883\n",
      "epoch 191: loss=1.4122028350830078\n",
      "epoch 192: loss=1.5311590433120728\n",
      "epoch 193: loss=1.4987316131591797\n",
      "epoch 194: loss=1.535719633102417\n",
      "epoch 195: loss=1.4943398237228394\n",
      "epoch 196: loss=1.496209979057312\n",
      "epoch 197: loss=1.4513142108917236\n",
      "epoch 198: loss=1.4513574838638306\n",
      "epoch 199: loss=1.457434058189392\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=9.881589889526367\n",
      "epoch 1: loss=9.580378532409668\n",
      "epoch 2: loss=9.106956481933594\n",
      "epoch 3: loss=8.456716537475586\n",
      "epoch 4: loss=7.5244927406311035\n",
      "epoch 5: loss=6.641774654388428\n",
      "epoch 6: loss=5.832877159118652\n",
      "epoch 7: loss=5.4945292472839355\n",
      "epoch 8: loss=5.4581298828125\n",
      "epoch 9: loss=5.295106887817383\n",
      "epoch 10: loss=4.956655025482178\n",
      "epoch 11: loss=4.498024940490723\n",
      "epoch 12: loss=3.9655425548553467\n",
      "epoch 13: loss=3.523202419281006\n",
      "epoch 14: loss=3.2162280082702637\n",
      "epoch 15: loss=2.9896557331085205\n",
      "epoch 16: loss=2.793785333633423\n",
      "epoch 17: loss=2.6842641830444336\n",
      "epoch 18: loss=2.572047472000122\n",
      "epoch 19: loss=2.4688544273376465\n",
      "epoch 20: loss=2.353747606277466\n",
      "epoch 21: loss=2.2364869117736816\n",
      "epoch 22: loss=2.109546422958374\n",
      "epoch 23: loss=1.9917103052139282\n",
      "epoch 24: loss=1.8757619857788086\n",
      "epoch 25: loss=1.758695125579834\n",
      "epoch 26: loss=1.6283507347106934\n",
      "epoch 27: loss=1.5099537372589111\n",
      "epoch 28: loss=1.4063650369644165\n",
      "epoch 29: loss=1.3254501819610596\n",
      "epoch 30: loss=1.2627812623977661\n",
      "epoch 31: loss=1.2204170227050781\n",
      "epoch 32: loss=1.19253408908844\n",
      "epoch 33: loss=1.1617565155029297\n",
      "epoch 34: loss=1.1279691457748413\n",
      "epoch 35: loss=1.090431571006775\n",
      "epoch 36: loss=1.0595892667770386\n",
      "epoch 37: loss=1.035479187965393\n",
      "epoch 38: loss=1.0124058723449707\n",
      "epoch 39: loss=0.9956049919128418\n",
      "epoch 40: loss=0.9821807146072388\n",
      "epoch 41: loss=0.9663099050521851\n",
      "epoch 42: loss=0.9532180428504944\n",
      "epoch 43: loss=0.9409868717193604\n",
      "epoch 44: loss=0.9347989559173584\n",
      "epoch 45: loss=0.9293779730796814\n",
      "epoch 46: loss=0.9236878156661987\n",
      "epoch 47: loss=0.917418897151947\n",
      "epoch 48: loss=0.9109653830528259\n",
      "epoch 49: loss=0.9036965370178223\n",
      "epoch 50: loss=0.8984942436218262\n",
      "epoch 51: loss=0.8949853181838989\n",
      "epoch 52: loss=0.8927798271179199\n",
      "epoch 53: loss=0.8927846550941467\n",
      "epoch 54: loss=0.8885954022407532\n",
      "epoch 55: loss=0.8860912919044495\n",
      "epoch 56: loss=0.8845810294151306\n",
      "epoch 57: loss=0.8805302977561951\n",
      "epoch 58: loss=0.878807008266449\n",
      "epoch 59: loss=0.8760711550712585\n",
      "epoch 60: loss=0.8742239475250244\n",
      "epoch 61: loss=0.8747060894966125\n",
      "epoch 62: loss=0.8738527297973633\n",
      "epoch 63: loss=0.8721990585327148\n",
      "epoch 64: loss=0.869998037815094\n",
      "epoch 65: loss=0.8689500689506531\n",
      "epoch 66: loss=0.868859052658081\n",
      "epoch 67: loss=0.8672711253166199\n",
      "epoch 68: loss=0.867292046546936\n",
      "epoch 69: loss=0.8650403618812561\n",
      "epoch 70: loss=0.8636537194252014\n",
      "epoch 71: loss=0.8636040687561035\n",
      "epoch 72: loss=0.8641985654830933\n",
      "epoch 73: loss=0.862611711025238\n",
      "epoch 74: loss=0.8618988394737244\n",
      "epoch 75: loss=0.8603237271308899\n",
      "epoch 76: loss=0.8590714335441589\n",
      "epoch 77: loss=0.8595725893974304\n",
      "epoch 78: loss=0.8589001893997192\n",
      "epoch 79: loss=0.8596062064170837\n",
      "epoch 80: loss=0.8580385446548462\n",
      "epoch 81: loss=0.858128011226654\n",
      "epoch 82: loss=0.8560808897018433\n",
      "epoch 83: loss=0.8557855486869812\n",
      "epoch 84: loss=0.8554438352584839\n",
      "epoch 85: loss=0.8562853932380676\n",
      "epoch 86: loss=0.8554739356040955\n",
      "epoch 87: loss=0.8547878265380859\n",
      "epoch 88: loss=0.8533673286437988\n",
      "epoch 89: loss=0.8531578779220581\n",
      "epoch 90: loss=0.8518518209457397\n",
      "epoch 91: loss=0.8529430031776428\n",
      "epoch 92: loss=0.8527108430862427\n",
      "epoch 93: loss=0.8515493273735046\n",
      "epoch 94: loss=0.8509165644645691\n",
      "epoch 95: loss=0.8519358038902283\n",
      "epoch 96: loss=0.8499276638031006\n",
      "epoch 97: loss=0.8504059910774231\n",
      "epoch 98: loss=0.8487781286239624\n",
      "epoch 99: loss=0.8488357067108154\n",
      "epoch 100: loss=0.8474745154380798\n",
      "epoch 101: loss=0.8481236100196838\n",
      "epoch 102: loss=0.8476397395133972\n",
      "epoch 103: loss=0.8475685119628906\n",
      "epoch 104: loss=0.8464598655700684\n",
      "epoch 105: loss=0.8462650179862976\n",
      "epoch 106: loss=0.846082866191864\n",
      "epoch 107: loss=0.8463899493217468\n",
      "epoch 108: loss=0.8450721502304077\n",
      "epoch 109: loss=0.8459534645080566\n",
      "epoch 110: loss=0.8447022438049316\n",
      "epoch 111: loss=0.843929648399353\n",
      "epoch 112: loss=0.8450228571891785\n",
      "epoch 113: loss=0.8443141579627991\n",
      "epoch 114: loss=0.8437953591346741\n",
      "epoch 115: loss=0.8424744606018066\n",
      "epoch 116: loss=0.8423675894737244\n",
      "epoch 117: loss=0.8418516516685486\n",
      "epoch 118: loss=0.8419946432113647\n",
      "epoch 119: loss=0.8411162495613098\n",
      "epoch 120: loss=0.8407331109046936\n",
      "epoch 121: loss=0.8422656059265137\n",
      "epoch 122: loss=0.8406897187232971\n",
      "epoch 123: loss=0.8388784527778625\n",
      "epoch 124: loss=0.8401203751564026\n",
      "epoch 125: loss=0.8390102386474609\n",
      "epoch 126: loss=0.8396283388137817\n",
      "epoch 127: loss=0.8376020193099976\n",
      "epoch 128: loss=0.8391692638397217\n",
      "epoch 129: loss=0.837234616279602\n",
      "epoch 130: loss=0.8359934091567993\n",
      "epoch 131: loss=0.8385957479476929\n",
      "epoch 132: loss=0.8385281562805176\n",
      "epoch 133: loss=0.8351153135299683\n",
      "epoch 134: loss=0.8358615636825562\n",
      "epoch 135: loss=0.835896909236908\n",
      "epoch 136: loss=0.8351153135299683\n",
      "epoch 137: loss=0.8353366851806641\n",
      "epoch 138: loss=0.8346924781799316\n",
      "epoch 139: loss=0.833652913570404\n",
      "epoch 140: loss=0.8339356184005737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 141: loss=0.8328201770782471\n",
      "epoch 142: loss=0.8336107730865479\n",
      "epoch 143: loss=0.8324874639511108\n",
      "epoch 144: loss=0.8312223553657532\n",
      "epoch 145: loss=0.8312864303588867\n",
      "epoch 146: loss=0.8307187557220459\n",
      "epoch 147: loss=0.8302585482597351\n",
      "epoch 148: loss=0.8311189413070679\n",
      "epoch 149: loss=0.8281779289245605\n",
      "epoch 150: loss=0.8293983340263367\n",
      "epoch 151: loss=0.8290927410125732\n",
      "epoch 152: loss=0.8287107944488525\n",
      "epoch 153: loss=0.8293707370758057\n",
      "epoch 154: loss=0.8272756934165955\n",
      "epoch 155: loss=0.827771782875061\n",
      "epoch 156: loss=0.8274259567260742\n",
      "epoch 157: loss=0.827691376209259\n",
      "epoch 158: loss=0.8273668885231018\n",
      "epoch 159: loss=0.8258699774742126\n",
      "epoch 160: loss=0.8260865211486816\n",
      "epoch 161: loss=0.8251214027404785\n",
      "epoch 162: loss=0.8245882987976074\n",
      "epoch 163: loss=0.8246131539344788\n",
      "epoch 164: loss=0.8232184648513794\n",
      "epoch 165: loss=0.8224926590919495\n",
      "epoch 166: loss=0.8238660097122192\n",
      "epoch 167: loss=0.8237289786338806\n",
      "epoch 168: loss=0.822067141532898\n",
      "epoch 169: loss=0.8212573528289795\n",
      "epoch 170: loss=0.8218013048171997\n",
      "epoch 171: loss=0.8206420540809631\n",
      "epoch 172: loss=0.8210666179656982\n",
      "epoch 173: loss=0.8210602402687073\n",
      "epoch 174: loss=0.8201358318328857\n",
      "epoch 175: loss=0.8201068639755249\n",
      "epoch 176: loss=0.8190636038780212\n",
      "epoch 177: loss=0.8192225694656372\n",
      "epoch 178: loss=0.8179545998573303\n",
      "epoch 179: loss=0.8190580010414124\n",
      "epoch 180: loss=0.8199434280395508\n",
      "epoch 181: loss=0.8168102502822876\n",
      "epoch 182: loss=0.8186902403831482\n",
      "epoch 183: loss=0.8182719349861145\n",
      "epoch 184: loss=0.8172262907028198\n",
      "epoch 185: loss=0.8173680305480957\n",
      "epoch 186: loss=0.8179821968078613\n",
      "epoch 187: loss=0.8174582123756409\n",
      "epoch 188: loss=0.8170641660690308\n",
      "epoch 189: loss=0.8160921931266785\n",
      "epoch 190: loss=0.8159369826316833\n",
      "epoch 191: loss=0.8163137435913086\n",
      "epoch 192: loss=0.8170708417892456\n",
      "epoch 193: loss=0.8177595138549805\n",
      "epoch 194: loss=0.8142171502113342\n",
      "epoch 195: loss=0.8157036900520325\n",
      "epoch 196: loss=0.8153567314147949\n",
      "epoch 197: loss=0.8158934116363525\n",
      "epoch 198: loss=0.8137444853782654\n",
      "epoch 199: loss=0.8142372965812683\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=9.854238510131836\n",
      "epoch 1: loss=9.552343368530273\n",
      "epoch 2: loss=9.022307395935059\n",
      "epoch 3: loss=8.313435554504395\n",
      "epoch 4: loss=7.401919841766357\n",
      "epoch 5: loss=6.441960334777832\n",
      "epoch 6: loss=5.770238399505615\n",
      "epoch 7: loss=5.555912494659424\n",
      "epoch 8: loss=5.461794376373291\n",
      "epoch 9: loss=5.018205642700195\n",
      "epoch 10: loss=4.493708610534668\n",
      "epoch 11: loss=3.984842538833618\n",
      "epoch 12: loss=3.534700393676758\n",
      "epoch 13: loss=3.1791036128997803\n",
      "epoch 14: loss=2.912464141845703\n",
      "epoch 15: loss=2.7023515701293945\n",
      "epoch 16: loss=2.5610105991363525\n",
      "epoch 17: loss=2.4377758502960205\n",
      "epoch 18: loss=2.3310630321502686\n",
      "epoch 19: loss=2.168179988861084\n",
      "epoch 20: loss=2.0424094200134277\n",
      "epoch 21: loss=1.9046432971954346\n",
      "epoch 22: loss=1.7897064685821533\n",
      "epoch 23: loss=1.642102599143982\n",
      "epoch 24: loss=1.4985004663467407\n",
      "epoch 25: loss=1.36992347240448\n",
      "epoch 26: loss=1.273590326309204\n",
      "epoch 27: loss=1.2090781927108765\n",
      "epoch 28: loss=1.1684199571609497\n",
      "epoch 29: loss=1.1482449769973755\n",
      "epoch 30: loss=1.1278784275054932\n",
      "epoch 31: loss=1.093408465385437\n",
      "epoch 32: loss=1.0568233728408813\n",
      "epoch 33: loss=1.0170071125030518\n",
      "epoch 34: loss=0.9910938739776611\n",
      "epoch 35: loss=0.9716498851776123\n",
      "epoch 36: loss=0.9621910452842712\n",
      "epoch 37: loss=0.9589259624481201\n",
      "epoch 38: loss=0.9554910659790039\n",
      "epoch 39: loss=0.9465194940567017\n",
      "epoch 40: loss=0.9363601207733154\n",
      "epoch 41: loss=0.9257649779319763\n",
      "epoch 42: loss=0.9166256189346313\n",
      "epoch 43: loss=0.9108564853668213\n",
      "epoch 44: loss=0.9093760848045349\n",
      "epoch 45: loss=0.9070658087730408\n",
      "epoch 46: loss=0.9076055884361267\n",
      "epoch 47: loss=0.903141975402832\n",
      "epoch 48: loss=0.8985492587089539\n",
      "epoch 49: loss=0.8953374028205872\n",
      "epoch 50: loss=0.8923563957214355\n",
      "epoch 51: loss=0.8915809392929077\n",
      "epoch 52: loss=0.889024019241333\n",
      "epoch 53: loss=0.8879137635231018\n",
      "epoch 54: loss=0.8855496048927307\n",
      "epoch 55: loss=0.8830305337905884\n",
      "epoch 56: loss=0.8796870708465576\n",
      "epoch 57: loss=0.878410816192627\n",
      "epoch 58: loss=0.8809165954589844\n",
      "epoch 59: loss=0.8797656893730164\n",
      "epoch 60: loss=0.8787757754325867\n",
      "epoch 61: loss=0.8767234086990356\n",
      "epoch 62: loss=0.8754609227180481\n",
      "epoch 63: loss=0.8755963444709778\n",
      "epoch 64: loss=0.8739140629768372\n",
      "epoch 65: loss=0.8729237914085388\n",
      "epoch 66: loss=0.8738057613372803\n",
      "epoch 67: loss=0.8744674921035767\n",
      "epoch 68: loss=0.8717334866523743\n",
      "epoch 69: loss=0.8704911470413208\n",
      "epoch 70: loss=0.8712216019630432\n",
      "epoch 71: loss=0.8704408407211304\n",
      "epoch 72: loss=0.8704808950424194\n",
      "epoch 73: loss=0.8690054416656494\n",
      "epoch 74: loss=0.8688728213310242\n",
      "epoch 75: loss=0.8672625422477722\n",
      "epoch 76: loss=0.8655816316604614\n",
      "epoch 77: loss=0.8663756847381592\n",
      "epoch 78: loss=0.8660003542900085\n",
      "epoch 79: loss=0.8655745387077332\n",
      "epoch 80: loss=0.866716742515564\n",
      "epoch 81: loss=0.866456151008606\n",
      "epoch 82: loss=0.8651906251907349\n",
      "epoch 83: loss=0.8636261224746704\n",
      "epoch 84: loss=0.8648679256439209\n",
      "epoch 85: loss=0.8644887804985046\n",
      "epoch 86: loss=0.86319899559021\n",
      "epoch 87: loss=0.8630331158638\n",
      "epoch 88: loss=0.86161208152771\n",
      "epoch 89: loss=0.8632446527481079\n",
      "epoch 90: loss=0.8609269857406616\n",
      "epoch 91: loss=0.8588001132011414\n",
      "epoch 92: loss=0.859974205493927\n",
      "epoch 93: loss=0.8611747026443481\n",
      "epoch 94: loss=0.8580679893493652\n",
      "epoch 95: loss=0.8589618802070618\n",
      "epoch 96: loss=0.8594686388969421\n",
      "epoch 97: loss=0.8593241572380066\n",
      "epoch 98: loss=0.8587872385978699\n",
      "epoch 99: loss=0.8584221601486206\n",
      "epoch 100: loss=0.8580706119537354\n",
      "epoch 101: loss=0.8568598031997681\n",
      "epoch 102: loss=0.8574360609054565\n",
      "epoch 103: loss=0.8564802408218384\n",
      "epoch 104: loss=0.8559423089027405\n",
      "epoch 105: loss=0.8537787795066833\n",
      "epoch 106: loss=0.8561497926712036\n",
      "epoch 107: loss=0.8542618751525879\n",
      "epoch 108: loss=0.8554837703704834\n",
      "epoch 109: loss=0.8548179864883423\n",
      "epoch 110: loss=0.8541299700737\n",
      "epoch 111: loss=0.8522993922233582\n",
      "epoch 112: loss=0.8535513281822205\n",
      "epoch 113: loss=0.8525664210319519\n",
      "epoch 114: loss=0.8527512550354004\n",
      "epoch 115: loss=0.8501192927360535\n",
      "epoch 116: loss=0.8514385223388672\n",
      "epoch 117: loss=0.8499829173088074\n",
      "epoch 118: loss=0.8487048745155334\n",
      "epoch 119: loss=0.8501758575439453\n",
      "epoch 120: loss=0.8501387238502502\n",
      "epoch 121: loss=0.8496108055114746\n",
      "epoch 122: loss=0.8472724556922913\n",
      "epoch 123: loss=0.8459600210189819\n",
      "epoch 124: loss=0.8462963104248047\n",
      "epoch 125: loss=0.8473240733146667\n",
      "epoch 126: loss=0.8451309204101562\n",
      "epoch 127: loss=0.8451763391494751\n",
      "epoch 128: loss=0.8441944122314453\n",
      "epoch 129: loss=0.84307861328125\n",
      "epoch 130: loss=0.8431735038757324\n",
      "epoch 131: loss=0.843621015548706\n",
      "epoch 132: loss=0.8427779674530029\n",
      "epoch 133: loss=0.8424563407897949\n",
      "epoch 134: loss=0.8423494100570679\n",
      "epoch 135: loss=0.8433953523635864\n",
      "epoch 136: loss=0.8410058617591858\n",
      "epoch 137: loss=0.8407379388809204\n",
      "epoch 138: loss=0.839908242225647\n",
      "epoch 139: loss=0.8400037884712219\n",
      "epoch 140: loss=0.8381556868553162\n",
      "epoch 141: loss=0.8395020365715027\n",
      "epoch 142: loss=0.8394400477409363\n",
      "epoch 143: loss=0.8382949233055115\n",
      "epoch 144: loss=0.8383785486221313\n",
      "epoch 145: loss=0.8368009328842163\n",
      "epoch 146: loss=0.837259829044342\n",
      "epoch 147: loss=0.8351722359657288\n",
      "epoch 148: loss=0.8346555233001709\n",
      "epoch 149: loss=0.8368574380874634\n",
      "epoch 150: loss=0.8358129858970642\n",
      "epoch 151: loss=0.8346028923988342\n",
      "epoch 152: loss=0.834775447845459\n",
      "epoch 153: loss=0.8324084281921387\n",
      "epoch 154: loss=0.8327667117118835\n",
      "epoch 155: loss=0.8340840935707092\n",
      "epoch 156: loss=0.8320611119270325\n",
      "epoch 157: loss=0.8307048678398132\n",
      "epoch 158: loss=0.8325638771057129\n",
      "epoch 159: loss=0.8319817781448364\n",
      "epoch 160: loss=0.8326526880264282\n",
      "epoch 161: loss=0.8309457898139954\n",
      "epoch 162: loss=0.8317241072654724\n",
      "epoch 163: loss=0.8302188515663147\n",
      "epoch 164: loss=0.8306943774223328\n",
      "epoch 165: loss=0.8298965692520142\n",
      "epoch 166: loss=0.8303194046020508\n",
      "epoch 167: loss=0.8290888667106628\n",
      "epoch 168: loss=0.8295143842697144\n",
      "epoch 169: loss=0.8292479515075684\n",
      "epoch 170: loss=0.8290945291519165\n",
      "epoch 171: loss=0.827994704246521\n",
      "epoch 172: loss=0.8281563520431519\n",
      "epoch 173: loss=0.829559326171875\n",
      "epoch 174: loss=0.8283488154411316\n",
      "epoch 175: loss=0.8282294273376465\n",
      "epoch 176: loss=0.8269333243370056\n",
      "epoch 177: loss=0.8256603479385376\n",
      "epoch 178: loss=0.8266345262527466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 179: loss=0.8262181878089905\n",
      "epoch 180: loss=0.8276176452636719\n",
      "epoch 181: loss=0.8261216878890991\n",
      "epoch 182: loss=0.8274633288383484\n",
      "epoch 183: loss=0.8253256678581238\n",
      "epoch 184: loss=0.8246385455131531\n",
      "epoch 185: loss=0.8259742856025696\n",
      "epoch 186: loss=0.8249831795692444\n",
      "epoch 187: loss=0.8246166706085205\n",
      "epoch 188: loss=0.8233503699302673\n",
      "epoch 189: loss=0.8235284686088562\n",
      "epoch 190: loss=0.8241551518440247\n",
      "epoch 191: loss=0.8249041438102722\n",
      "epoch 192: loss=0.8232429623603821\n",
      "epoch 193: loss=0.8240387439727783\n",
      "epoch 194: loss=0.8229272961616516\n",
      "epoch 195: loss=0.8229401707649231\n",
      "epoch 196: loss=0.8227136731147766\n",
      "epoch 197: loss=0.8218281865119934\n",
      "epoch 198: loss=0.8239840865135193\n",
      "epoch 199: loss=0.8220579624176025\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=9.836797714233398\n",
      "epoch 1: loss=9.48474407196045\n",
      "epoch 2: loss=8.913981437683105\n",
      "epoch 3: loss=8.201634407043457\n",
      "epoch 4: loss=7.210505485534668\n",
      "epoch 5: loss=6.1588006019592285\n",
      "epoch 6: loss=5.621395111083984\n",
      "epoch 7: loss=5.635925769805908\n",
      "epoch 8: loss=5.4273786544799805\n",
      "epoch 9: loss=4.926791191101074\n",
      "epoch 10: loss=4.266388893127441\n",
      "epoch 11: loss=3.6752331256866455\n",
      "epoch 12: loss=3.2518463134765625\n",
      "epoch 13: loss=2.9548747539520264\n",
      "epoch 14: loss=2.744058609008789\n",
      "epoch 15: loss=2.6175684928894043\n",
      "epoch 16: loss=2.490607976913452\n",
      "epoch 17: loss=2.3316946029663086\n",
      "epoch 18: loss=2.217547655105591\n",
      "epoch 19: loss=2.130260467529297\n",
      "epoch 20: loss=2.0147030353546143\n",
      "epoch 21: loss=1.8538098335266113\n",
      "epoch 22: loss=1.7063249349594116\n",
      "epoch 23: loss=1.5531913042068481\n",
      "epoch 24: loss=1.4306306838989258\n",
      "epoch 25: loss=1.314185380935669\n",
      "epoch 26: loss=1.251042366027832\n",
      "epoch 27: loss=1.213137149810791\n",
      "epoch 28: loss=1.1884478330612183\n",
      "epoch 29: loss=1.155495285987854\n",
      "epoch 30: loss=1.1107280254364014\n",
      "epoch 31: loss=1.0670193433761597\n",
      "epoch 32: loss=1.0366904735565186\n",
      "epoch 33: loss=1.0175889730453491\n",
      "epoch 34: loss=1.0027495622634888\n",
      "epoch 35: loss=1.0001051425933838\n",
      "epoch 36: loss=0.995592474937439\n",
      "epoch 37: loss=0.9804500341415405\n",
      "epoch 38: loss=0.9580842852592468\n",
      "epoch 39: loss=0.9500342011451721\n",
      "epoch 40: loss=0.9428512454032898\n",
      "epoch 41: loss=0.9394253492355347\n",
      "epoch 42: loss=0.9368563294410706\n",
      "epoch 43: loss=0.9370156526565552\n",
      "epoch 44: loss=0.9337406158447266\n",
      "epoch 45: loss=0.9232208132743835\n",
      "epoch 46: loss=0.9157726764678955\n",
      "epoch 47: loss=0.9124515652656555\n",
      "epoch 48: loss=0.9127277731895447\n",
      "epoch 49: loss=0.9151822924613953\n",
      "epoch 50: loss=0.9147301316261292\n",
      "epoch 51: loss=0.910968542098999\n",
      "epoch 52: loss=0.9082332253456116\n",
      "epoch 53: loss=0.902076005935669\n",
      "epoch 54: loss=0.9019848108291626\n",
      "epoch 55: loss=0.9014101624488831\n",
      "epoch 56: loss=0.9050393104553223\n",
      "epoch 57: loss=0.9006025195121765\n",
      "epoch 58: loss=0.8972887396812439\n",
      "epoch 59: loss=0.8968993425369263\n",
      "epoch 60: loss=0.8950552344322205\n",
      "epoch 61: loss=0.8949737548828125\n",
      "epoch 62: loss=0.8948212265968323\n",
      "epoch 63: loss=0.896736204624176\n",
      "epoch 64: loss=0.8930700421333313\n",
      "epoch 65: loss=0.8933659791946411\n",
      "epoch 66: loss=0.8925989270210266\n",
      "epoch 67: loss=0.8933303356170654\n",
      "epoch 68: loss=0.8909924030303955\n",
      "epoch 69: loss=0.8903351426124573\n",
      "epoch 70: loss=0.8916603326797485\n",
      "epoch 71: loss=0.891327440738678\n",
      "epoch 72: loss=0.8927761912345886\n",
      "epoch 73: loss=0.8891811370849609\n",
      "epoch 74: loss=0.8898041844367981\n",
      "epoch 75: loss=0.8895975947380066\n",
      "epoch 76: loss=0.8878483772277832\n",
      "epoch 77: loss=0.8876687288284302\n",
      "epoch 78: loss=0.8857874274253845\n",
      "epoch 79: loss=0.8871355056762695\n",
      "epoch 80: loss=0.885174036026001\n",
      "epoch 81: loss=0.8863406777381897\n",
      "epoch 82: loss=0.8858126997947693\n",
      "epoch 83: loss=0.8864097595214844\n",
      "epoch 84: loss=0.886080801486969\n",
      "epoch 85: loss=0.8846840858459473\n",
      "epoch 86: loss=0.884765088558197\n",
      "epoch 87: loss=0.8845660090446472\n",
      "epoch 88: loss=0.8835682272911072\n",
      "epoch 89: loss=0.8852291107177734\n",
      "epoch 90: loss=0.8831639289855957\n",
      "epoch 91: loss=0.8851994872093201\n",
      "epoch 92: loss=0.8851684927940369\n",
      "epoch 93: loss=0.8843010067939758\n",
      "epoch 94: loss=0.8818673491477966\n",
      "epoch 95: loss=0.8825518488883972\n",
      "epoch 96: loss=0.8834144473075867\n",
      "epoch 97: loss=0.8811744451522827\n",
      "epoch 98: loss=0.8815863728523254\n",
      "epoch 99: loss=0.8836041688919067\n",
      "epoch 100: loss=0.8828582167625427\n",
      "epoch 101: loss=0.8816792368888855\n",
      "epoch 102: loss=0.8825517892837524\n",
      "epoch 103: loss=0.8802696466445923\n",
      "epoch 104: loss=0.8816861510276794\n",
      "epoch 105: loss=0.881119966506958\n",
      "epoch 106: loss=0.8800837993621826\n",
      "epoch 107: loss=0.8810898065567017\n",
      "epoch 108: loss=0.881378173828125\n",
      "epoch 109: loss=0.879649817943573\n",
      "epoch 110: loss=0.8796402812004089\n",
      "epoch 111: loss=0.8806329369544983\n",
      "epoch 112: loss=0.8787966966629028\n",
      "epoch 113: loss=0.8776592016220093\n",
      "epoch 114: loss=0.8802352547645569\n",
      "epoch 115: loss=0.8777815103530884\n",
      "epoch 116: loss=0.8782325387001038\n",
      "epoch 117: loss=0.8779434561729431\n",
      "epoch 118: loss=0.8791365027427673\n",
      "epoch 119: loss=0.877572774887085\n",
      "epoch 120: loss=0.8789147138595581\n",
      "epoch 121: loss=0.8760467767715454\n",
      "epoch 122: loss=0.8771374821662903\n",
      "epoch 123: loss=0.878257155418396\n",
      "epoch 124: loss=0.8783717155456543\n",
      "epoch 125: loss=0.8765004277229309\n",
      "epoch 126: loss=0.8788292407989502\n",
      "epoch 127: loss=0.8755313754081726\n",
      "epoch 128: loss=0.8760870099067688\n",
      "epoch 129: loss=0.8761189579963684\n",
      "epoch 130: loss=0.8765605092048645\n",
      "epoch 131: loss=0.8769045472145081\n",
      "epoch 132: loss=0.8757653832435608\n",
      "epoch 133: loss=0.8752437829971313\n",
      "epoch 134: loss=0.8764299750328064\n",
      "epoch 135: loss=0.8753814101219177\n",
      "epoch 136: loss=0.8750415444374084\n",
      "epoch 137: loss=0.8759188652038574\n",
      "epoch 138: loss=0.8751652240753174\n",
      "epoch 139: loss=0.8728688955307007\n",
      "epoch 140: loss=0.8744781017303467\n",
      "epoch 141: loss=0.8763540983200073\n",
      "epoch 142: loss=0.8754902482032776\n",
      "epoch 143: loss=0.8739356994628906\n",
      "epoch 144: loss=0.8750771880149841\n",
      "epoch 145: loss=0.8731530904769897\n",
      "epoch 146: loss=0.8741322755813599\n",
      "epoch 147: loss=0.8730577826499939\n",
      "epoch 148: loss=0.8752973079681396\n",
      "epoch 149: loss=0.8732988238334656\n",
      "epoch 150: loss=0.872999906539917\n",
      "epoch 151: loss=0.8733927607536316\n",
      "epoch 152: loss=0.8737581372261047\n",
      "epoch 153: loss=0.8715593814849854\n",
      "epoch 154: loss=0.8740236163139343\n",
      "epoch 155: loss=0.8720996975898743\n",
      "epoch 156: loss=0.872847855091095\n",
      "epoch 157: loss=0.8731849193572998\n",
      "epoch 158: loss=0.8731592297554016\n",
      "epoch 159: loss=0.8711028099060059\n",
      "epoch 160: loss=0.8714773654937744\n",
      "epoch 161: loss=0.8715150952339172\n",
      "epoch 162: loss=0.8696152567863464\n",
      "epoch 163: loss=0.8703092932701111\n",
      "epoch 164: loss=0.8712441921234131\n",
      "epoch 165: loss=0.8727219104766846\n",
      "epoch 166: loss=0.8706358075141907\n",
      "epoch 167: loss=0.8710706830024719\n",
      "epoch 168: loss=0.8725214600563049\n",
      "epoch 169: loss=0.8717560768127441\n",
      "epoch 170: loss=0.8701653480529785\n",
      "epoch 171: loss=0.8704611659049988\n",
      "epoch 172: loss=0.8697960376739502\n",
      "epoch 173: loss=0.8701051473617554\n",
      "epoch 174: loss=0.8688988089561462\n",
      "epoch 175: loss=0.871478259563446\n",
      "epoch 176: loss=0.8702674508094788\n",
      "epoch 177: loss=0.8699673414230347\n",
      "epoch 178: loss=0.8711831569671631\n",
      "epoch 179: loss=0.8698671460151672\n",
      "epoch 180: loss=0.8702303171157837\n",
      "epoch 181: loss=0.870269238948822\n",
      "epoch 182: loss=0.866771399974823\n",
      "epoch 183: loss=0.8703694939613342\n",
      "epoch 184: loss=0.8703571557998657\n",
      "epoch 185: loss=0.8717178106307983\n",
      "epoch 186: loss=0.8700200915336609\n",
      "epoch 187: loss=0.8692545294761658\n",
      "epoch 188: loss=0.8714767098426819\n",
      "epoch 189: loss=0.870995044708252\n",
      "epoch 190: loss=0.8686979413032532\n",
      "epoch 191: loss=0.8675799369812012\n",
      "epoch 192: loss=0.8683448433876038\n",
      "epoch 193: loss=0.86838698387146\n",
      "epoch 194: loss=0.8680057525634766\n",
      "epoch 195: loss=0.8670223355293274\n",
      "epoch 196: loss=0.8672471642494202\n",
      "epoch 197: loss=0.8663564920425415\n",
      "epoch 198: loss=0.8651049733161926\n",
      "epoch 199: loss=0.8670367002487183\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=9.806778907775879\n",
      "epoch 1: loss=9.445932388305664\n",
      "epoch 2: loss=9.033889770507812\n",
      "epoch 3: loss=8.307271003723145\n",
      "epoch 4: loss=7.422383785247803\n",
      "epoch 5: loss=6.4773173332214355\n",
      "epoch 6: loss=5.880172252655029\n",
      "epoch 7: loss=5.782238483428955\n",
      "epoch 8: loss=5.631430625915527\n",
      "epoch 9: loss=5.2992262840271\n",
      "epoch 10: loss=4.675550937652588\n",
      "epoch 11: loss=4.055549144744873\n",
      "epoch 12: loss=3.5426251888275146\n",
      "epoch 13: loss=3.201512098312378\n",
      "epoch 14: loss=2.8840537071228027\n",
      "epoch 15: loss=2.7259514331817627\n",
      "epoch 16: loss=2.5793089866638184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: loss=2.4692635536193848\n",
      "epoch 18: loss=2.3047707080841064\n",
      "epoch 19: loss=2.190504312515259\n",
      "epoch 20: loss=2.0908584594726562\n",
      "epoch 21: loss=1.975724458694458\n",
      "epoch 22: loss=1.8328258991241455\n",
      "epoch 23: loss=1.6940510272979736\n",
      "epoch 24: loss=1.5834252834320068\n",
      "epoch 25: loss=1.4773026704788208\n",
      "epoch 26: loss=1.382257342338562\n",
      "epoch 27: loss=1.3149237632751465\n",
      "epoch 28: loss=1.2724971771240234\n",
      "epoch 29: loss=1.2282766103744507\n",
      "epoch 30: loss=1.192360281944275\n",
      "epoch 31: loss=1.1564842462539673\n",
      "epoch 32: loss=1.1258608102798462\n",
      "epoch 33: loss=1.1010671854019165\n",
      "epoch 34: loss=1.0893784761428833\n",
      "epoch 35: loss=1.0763256549835205\n",
      "epoch 36: loss=1.0623512268066406\n",
      "epoch 37: loss=1.0470212697982788\n",
      "epoch 38: loss=1.0362391471862793\n",
      "epoch 39: loss=1.0157685279846191\n",
      "epoch 40: loss=1.001866102218628\n",
      "epoch 41: loss=0.9913461804389954\n",
      "epoch 42: loss=0.9867218732833862\n",
      "epoch 43: loss=0.9814236760139465\n",
      "epoch 44: loss=0.9828828573226929\n",
      "epoch 45: loss=0.9810580611228943\n",
      "epoch 46: loss=0.9743592739105225\n",
      "epoch 47: loss=0.9685545563697815\n",
      "epoch 48: loss=0.9614713788032532\n",
      "epoch 49: loss=0.9563885927200317\n",
      "epoch 50: loss=0.9556413888931274\n",
      "epoch 51: loss=0.9541054964065552\n",
      "epoch 52: loss=0.9524187445640564\n",
      "epoch 53: loss=0.9507207274436951\n",
      "epoch 54: loss=0.9494791626930237\n",
      "epoch 55: loss=0.9475015997886658\n",
      "epoch 56: loss=0.9446333050727844\n",
      "epoch 57: loss=0.9386680722236633\n",
      "epoch 58: loss=0.9392822980880737\n",
      "epoch 59: loss=0.9399822354316711\n",
      "epoch 60: loss=0.9380940794944763\n",
      "epoch 61: loss=0.9358282685279846\n",
      "epoch 62: loss=0.9385292530059814\n",
      "epoch 63: loss=0.937828004360199\n",
      "epoch 64: loss=0.9372246861457825\n",
      "epoch 65: loss=0.9345802068710327\n",
      "epoch 66: loss=0.9313070774078369\n",
      "epoch 67: loss=0.9317874312400818\n",
      "epoch 68: loss=0.9309940338134766\n",
      "epoch 69: loss=0.9321799874305725\n",
      "epoch 70: loss=0.9312908053398132\n",
      "epoch 71: loss=0.9310129880905151\n",
      "epoch 72: loss=0.9289449453353882\n",
      "epoch 73: loss=0.9278146028518677\n",
      "epoch 74: loss=0.9276313781738281\n",
      "epoch 75: loss=0.9268834590911865\n",
      "epoch 76: loss=0.9268070459365845\n",
      "epoch 77: loss=0.9260029196739197\n",
      "epoch 78: loss=0.924548327922821\n",
      "epoch 79: loss=0.9246941804885864\n",
      "epoch 80: loss=0.9240588545799255\n",
      "epoch 81: loss=0.9235318899154663\n",
      "epoch 82: loss=0.9231874346733093\n",
      "epoch 83: loss=0.9244185090065002\n",
      "epoch 84: loss=0.9218546748161316\n",
      "epoch 85: loss=0.9221827983856201\n",
      "epoch 86: loss=0.9227918982505798\n",
      "epoch 87: loss=0.9227294325828552\n",
      "epoch 88: loss=0.9199680089950562\n",
      "epoch 89: loss=0.9218296408653259\n",
      "epoch 90: loss=0.9206667542457581\n",
      "epoch 91: loss=0.9206549525260925\n",
      "epoch 92: loss=0.9189828038215637\n",
      "epoch 93: loss=0.9191609025001526\n",
      "epoch 94: loss=0.9184951782226562\n",
      "epoch 95: loss=0.9188481569290161\n",
      "epoch 96: loss=0.9182305932044983\n",
      "epoch 97: loss=0.9181767702102661\n",
      "epoch 98: loss=0.9178856611251831\n",
      "epoch 99: loss=0.9187328815460205\n",
      "epoch 100: loss=0.9168522953987122\n",
      "epoch 101: loss=0.9161251187324524\n",
      "epoch 102: loss=0.9180492162704468\n",
      "epoch 103: loss=0.9148691892623901\n",
      "epoch 104: loss=0.9156544208526611\n",
      "epoch 105: loss=0.9150912165641785\n",
      "epoch 106: loss=0.915791392326355\n",
      "epoch 107: loss=0.9155540466308594\n",
      "epoch 108: loss=0.917682945728302\n",
      "epoch 109: loss=0.9153092503547668\n",
      "epoch 110: loss=0.9140442609786987\n",
      "epoch 111: loss=0.9155439734458923\n",
      "epoch 112: loss=0.9141643643379211\n",
      "epoch 113: loss=0.9144085049629211\n",
      "epoch 114: loss=0.9114890098571777\n",
      "epoch 115: loss=0.9119246602058411\n",
      "epoch 116: loss=0.9133959412574768\n",
      "epoch 117: loss=0.91401606798172\n",
      "epoch 118: loss=0.9133499264717102\n",
      "epoch 119: loss=0.9107328653335571\n",
      "epoch 120: loss=0.9109275341033936\n",
      "epoch 121: loss=0.9108284711837769\n",
      "epoch 122: loss=0.9112482070922852\n",
      "epoch 123: loss=0.9117704033851624\n",
      "epoch 124: loss=0.9110615849494934\n",
      "epoch 125: loss=0.9102689623832703\n",
      "epoch 126: loss=0.9095428586006165\n",
      "epoch 127: loss=0.9104954600334167\n",
      "epoch 128: loss=0.9092304110527039\n",
      "epoch 129: loss=0.9094575643539429\n",
      "epoch 130: loss=0.9087657928466797\n",
      "epoch 131: loss=0.9095420837402344\n",
      "epoch 132: loss=0.9090809226036072\n",
      "epoch 133: loss=0.90899258852005\n",
      "epoch 134: loss=0.9075596332550049\n",
      "epoch 135: loss=0.9044401049613953\n",
      "epoch 136: loss=0.906883955001831\n",
      "epoch 137: loss=0.9065985083580017\n",
      "epoch 138: loss=0.9057891964912415\n",
      "epoch 139: loss=0.90691077709198\n",
      "epoch 140: loss=0.9077144861221313\n",
      "epoch 141: loss=0.9071924686431885\n",
      "epoch 142: loss=0.9044288396835327\n",
      "epoch 143: loss=0.906233012676239\n",
      "epoch 144: loss=0.9048871397972107\n",
      "epoch 145: loss=0.9058255553245544\n",
      "epoch 146: loss=0.9051132798194885\n",
      "epoch 147: loss=0.9039744734764099\n",
      "epoch 148: loss=0.905381977558136\n",
      "epoch 149: loss=0.9036479592323303\n",
      "epoch 150: loss=0.9042859077453613\n",
      "epoch 151: loss=0.9030707478523254\n",
      "epoch 152: loss=0.9020811319351196\n",
      "epoch 153: loss=0.9039512872695923\n",
      "epoch 154: loss=0.902096688747406\n",
      "epoch 155: loss=0.9021780490875244\n",
      "epoch 156: loss=0.8998927474021912\n",
      "epoch 157: loss=0.9048781394958496\n",
      "epoch 158: loss=0.9039916396141052\n",
      "epoch 159: loss=0.9042220711708069\n",
      "epoch 160: loss=0.9006327390670776\n",
      "epoch 161: loss=0.9030666351318359\n",
      "epoch 162: loss=0.9021438360214233\n",
      "epoch 163: loss=0.9024274349212646\n",
      "epoch 164: loss=0.9025154709815979\n",
      "epoch 165: loss=0.9010710716247559\n",
      "epoch 166: loss=0.9027453660964966\n",
      "epoch 167: loss=0.9019851684570312\n",
      "epoch 168: loss=0.9030581712722778\n",
      "epoch 169: loss=0.9023314714431763\n",
      "epoch 170: loss=0.8998535871505737\n",
      "epoch 171: loss=0.9031373858451843\n",
      "epoch 172: loss=0.9002658128738403\n",
      "epoch 173: loss=0.9024894833564758\n",
      "epoch 174: loss=0.8998807668685913\n",
      "epoch 175: loss=0.8986366987228394\n",
      "epoch 176: loss=0.8997805118560791\n",
      "epoch 177: loss=0.9010486602783203\n",
      "epoch 178: loss=0.9003564119338989\n",
      "epoch 179: loss=0.8995495438575745\n",
      "epoch 180: loss=0.9001007080078125\n",
      "epoch 181: loss=0.8983595371246338\n",
      "epoch 182: loss=0.8992201685905457\n",
      "epoch 183: loss=0.8985854983329773\n",
      "epoch 184: loss=0.8988681435585022\n",
      "epoch 185: loss=0.8978551626205444\n",
      "epoch 186: loss=0.8967757821083069\n",
      "epoch 187: loss=0.8980557322502136\n",
      "epoch 188: loss=0.8988990187644958\n",
      "epoch 189: loss=0.8987047672271729\n",
      "epoch 190: loss=0.8987799286842346\n",
      "epoch 191: loss=0.8977146148681641\n",
      "epoch 192: loss=0.8979759812355042\n",
      "epoch 193: loss=0.8964915871620178\n",
      "epoch 194: loss=0.897062361240387\n",
      "epoch 195: loss=0.8978049159049988\n",
      "epoch 196: loss=0.8956838250160217\n",
      "epoch 197: loss=0.8950322866439819\n",
      "epoch 198: loss=0.8966120481491089\n",
      "epoch 199: loss=0.8961145877838135\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=9.87769889831543\n",
      "epoch 1: loss=9.526810646057129\n",
      "epoch 2: loss=9.098505973815918\n",
      "epoch 3: loss=8.502985000610352\n",
      "epoch 4: loss=7.640548229217529\n",
      "epoch 5: loss=6.795435428619385\n",
      "epoch 6: loss=5.9601216316223145\n",
      "epoch 7: loss=5.588426113128662\n",
      "epoch 8: loss=5.467785358428955\n",
      "epoch 9: loss=5.275518417358398\n",
      "epoch 10: loss=4.980104446411133\n",
      "epoch 11: loss=4.464216232299805\n",
      "epoch 12: loss=3.969397783279419\n",
      "epoch 13: loss=3.569455623626709\n",
      "epoch 14: loss=3.2022252082824707\n",
      "epoch 15: loss=2.9152817726135254\n",
      "epoch 16: loss=2.745243549346924\n",
      "epoch 17: loss=2.590574026107788\n",
      "epoch 18: loss=2.4360876083374023\n",
      "epoch 19: loss=2.3448338508605957\n",
      "epoch 20: loss=2.2203149795532227\n",
      "epoch 21: loss=2.121894359588623\n",
      "epoch 22: loss=1.9879902601242065\n",
      "epoch 23: loss=1.8670845031738281\n",
      "epoch 24: loss=1.736920714378357\n",
      "epoch 25: loss=1.5950692892074585\n",
      "epoch 26: loss=1.477729320526123\n",
      "epoch 27: loss=1.3737143278121948\n",
      "epoch 28: loss=1.2923375368118286\n",
      "epoch 29: loss=1.2375268936157227\n",
      "epoch 30: loss=1.197333574295044\n",
      "epoch 31: loss=1.173385739326477\n",
      "epoch 32: loss=1.1461598873138428\n",
      "epoch 33: loss=1.1144840717315674\n",
      "epoch 34: loss=1.0733582973480225\n",
      "epoch 35: loss=1.052204966545105\n",
      "epoch 36: loss=1.0345330238342285\n",
      "epoch 37: loss=1.019890546798706\n",
      "epoch 38: loss=1.0057100057601929\n",
      "epoch 39: loss=0.9980576038360596\n",
      "epoch 40: loss=0.9859015941619873\n",
      "epoch 41: loss=0.97262042760849\n",
      "epoch 42: loss=0.9607629179954529\n",
      "epoch 43: loss=0.9570189118385315\n",
      "epoch 44: loss=0.9520596861839294\n",
      "epoch 45: loss=0.9472219944000244\n",
      "epoch 46: loss=0.9439906477928162\n",
      "epoch 47: loss=0.9399294257164001\n",
      "epoch 48: loss=0.9336670637130737\n",
      "epoch 49: loss=0.9276139140129089\n",
      "epoch 50: loss=0.9228315949440002\n",
      "epoch 51: loss=0.9218285083770752\n",
      "epoch 52: loss=0.923728346824646\n",
      "epoch 53: loss=0.9213643670082092\n",
      "epoch 54: loss=0.9196795225143433\n",
      "epoch 55: loss=0.9142419695854187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56: loss=0.9146422147750854\n",
      "epoch 57: loss=0.9121953845024109\n",
      "epoch 58: loss=0.9110093712806702\n",
      "epoch 59: loss=0.9093100428581238\n",
      "epoch 60: loss=0.9068282842636108\n",
      "epoch 61: loss=0.9069851040840149\n",
      "epoch 62: loss=0.9068450927734375\n",
      "epoch 63: loss=0.9050913453102112\n",
      "epoch 64: loss=0.90406733751297\n",
      "epoch 65: loss=0.9042713642120361\n",
      "epoch 66: loss=0.9005565643310547\n",
      "epoch 67: loss=0.9018049240112305\n",
      "epoch 68: loss=0.9003584384918213\n",
      "epoch 69: loss=0.9018802046775818\n",
      "epoch 70: loss=0.899735689163208\n",
      "epoch 71: loss=0.8970149755477905\n",
      "epoch 72: loss=0.8981614708900452\n",
      "epoch 73: loss=0.8961700201034546\n",
      "epoch 74: loss=0.8988488912582397\n",
      "epoch 75: loss=0.8963766694068909\n",
      "epoch 76: loss=0.8968089818954468\n",
      "epoch 77: loss=0.8962172269821167\n",
      "epoch 78: loss=0.8958191275596619\n",
      "epoch 79: loss=0.8930596709251404\n",
      "epoch 80: loss=0.8943349123001099\n",
      "epoch 81: loss=0.8931719064712524\n",
      "epoch 82: loss=0.8924184441566467\n",
      "epoch 83: loss=0.8934106230735779\n",
      "epoch 84: loss=0.8936596512794495\n",
      "epoch 85: loss=0.8907319903373718\n",
      "epoch 86: loss=0.8933513164520264\n",
      "epoch 87: loss=0.8921500444412231\n",
      "epoch 88: loss=0.8897386789321899\n",
      "epoch 89: loss=0.8914403319358826\n",
      "epoch 90: loss=0.8902357816696167\n",
      "epoch 91: loss=0.8921215534210205\n",
      "epoch 92: loss=0.887687087059021\n",
      "epoch 93: loss=0.8898404240608215\n",
      "epoch 94: loss=0.8893269896507263\n",
      "epoch 95: loss=0.8886984586715698\n",
      "epoch 96: loss=0.8867728114128113\n",
      "epoch 97: loss=0.888823390007019\n",
      "epoch 98: loss=0.8853220343589783\n",
      "epoch 99: loss=0.8856173753738403\n",
      "epoch 100: loss=0.8859230875968933\n",
      "epoch 101: loss=0.8866636753082275\n",
      "epoch 102: loss=0.8847338557243347\n",
      "epoch 103: loss=0.8850570321083069\n",
      "epoch 104: loss=0.8845318555831909\n",
      "epoch 105: loss=0.8827109932899475\n",
      "epoch 106: loss=0.884404182434082\n",
      "epoch 107: loss=0.8818414807319641\n",
      "epoch 108: loss=0.8811829686164856\n",
      "epoch 109: loss=0.882262110710144\n",
      "epoch 110: loss=0.878906786441803\n",
      "epoch 111: loss=0.880097508430481\n",
      "epoch 112: loss=0.8804935216903687\n",
      "epoch 113: loss=0.8770167827606201\n",
      "epoch 114: loss=0.8790834546089172\n",
      "epoch 115: loss=0.8783863186836243\n",
      "epoch 116: loss=0.8783468008041382\n",
      "epoch 117: loss=0.8773815631866455\n",
      "epoch 118: loss=0.8759182095527649\n",
      "epoch 119: loss=0.8791301250457764\n",
      "epoch 120: loss=0.8770818114280701\n",
      "epoch 121: loss=0.8782355189323425\n",
      "epoch 122: loss=0.875555694103241\n",
      "epoch 123: loss=0.8770992159843445\n",
      "epoch 124: loss=0.8745646476745605\n",
      "epoch 125: loss=0.8744882941246033\n",
      "epoch 126: loss=0.873436689376831\n",
      "epoch 127: loss=0.8737751841545105\n",
      "epoch 128: loss=0.8736088871955872\n",
      "epoch 129: loss=0.8729033470153809\n",
      "epoch 130: loss=0.8718697428703308\n",
      "epoch 131: loss=0.8742279410362244\n",
      "epoch 132: loss=0.8717212080955505\n",
      "epoch 133: loss=0.8718390464782715\n",
      "epoch 134: loss=0.8701854348182678\n",
      "epoch 135: loss=0.8699986934661865\n",
      "epoch 136: loss=0.8720617294311523\n",
      "epoch 137: loss=0.8703734874725342\n",
      "epoch 138: loss=0.8689219951629639\n",
      "epoch 139: loss=0.8686341047286987\n",
      "epoch 140: loss=0.8695891499519348\n",
      "epoch 141: loss=0.8686246871948242\n",
      "epoch 142: loss=0.8686680197715759\n",
      "epoch 143: loss=0.8690224885940552\n",
      "epoch 144: loss=0.8680119514465332\n",
      "epoch 145: loss=0.8679684400558472\n",
      "epoch 146: loss=0.8673176169395447\n",
      "epoch 147: loss=0.8670573234558105\n",
      "epoch 148: loss=0.8656758666038513\n",
      "epoch 149: loss=0.8664753437042236\n",
      "epoch 150: loss=0.8662575483322144\n",
      "epoch 151: loss=0.8656960129737854\n",
      "epoch 152: loss=0.8613364696502686\n",
      "epoch 153: loss=0.8633091449737549\n",
      "epoch 154: loss=0.8629774451255798\n",
      "epoch 155: loss=0.863279402256012\n",
      "epoch 156: loss=0.8636581301689148\n",
      "epoch 157: loss=0.8606823086738586\n",
      "epoch 158: loss=0.8630059957504272\n",
      "epoch 159: loss=0.859798789024353\n",
      "epoch 160: loss=0.8582327365875244\n",
      "epoch 161: loss=0.8589246273040771\n",
      "epoch 162: loss=0.8584704399108887\n",
      "epoch 163: loss=0.8580446839332581\n",
      "epoch 164: loss=0.8591925501823425\n",
      "epoch 165: loss=0.8595041632652283\n",
      "epoch 166: loss=0.8561384081840515\n",
      "epoch 167: loss=0.8589470386505127\n",
      "epoch 168: loss=0.8569056987762451\n",
      "epoch 169: loss=0.8560566306114197\n",
      "epoch 170: loss=0.8562692403793335\n",
      "epoch 171: loss=0.8549608588218689\n",
      "epoch 172: loss=0.8512292504310608\n",
      "epoch 173: loss=0.8538299798965454\n",
      "epoch 174: loss=0.8549453616142273\n",
      "epoch 175: loss=0.8523403406143188\n",
      "epoch 176: loss=0.8519802689552307\n",
      "epoch 177: loss=0.8521936535835266\n",
      "epoch 178: loss=0.8506343960762024\n",
      "epoch 179: loss=0.8514421582221985\n",
      "epoch 180: loss=0.8499024510383606\n",
      "epoch 181: loss=0.8509628176689148\n",
      "epoch 182: loss=0.8493074774742126\n",
      "epoch 183: loss=0.8521708250045776\n",
      "epoch 184: loss=0.8489923477172852\n",
      "epoch 185: loss=0.8505087494850159\n",
      "epoch 186: loss=0.8494133353233337\n",
      "epoch 187: loss=0.8448691368103027\n",
      "epoch 188: loss=0.8499669432640076\n",
      "epoch 189: loss=0.8484575748443604\n",
      "epoch 190: loss=0.8478284478187561\n",
      "epoch 191: loss=0.8485916256904602\n",
      "epoch 192: loss=0.8484804630279541\n",
      "epoch 193: loss=0.8465119004249573\n",
      "epoch 194: loss=0.8480843305587769\n",
      "epoch 195: loss=0.8458194732666016\n",
      "epoch 196: loss=0.8463650345802307\n",
      "epoch 197: loss=0.8442216515541077\n",
      "epoch 198: loss=0.8445855975151062\n",
      "epoch 199: loss=0.8447508215904236\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=9.976214408874512\n",
      "epoch 1: loss=9.505249977111816\n",
      "epoch 2: loss=8.98954963684082\n",
      "epoch 3: loss=8.294032096862793\n",
      "epoch 4: loss=7.390093803405762\n",
      "epoch 5: loss=6.43012809753418\n",
      "epoch 6: loss=5.820927619934082\n",
      "epoch 7: loss=5.689431667327881\n",
      "epoch 8: loss=5.604984760284424\n",
      "epoch 9: loss=5.036231517791748\n",
      "epoch 10: loss=4.46169900894165\n",
      "epoch 11: loss=3.87069034576416\n",
      "epoch 12: loss=3.4216856956481934\n",
      "epoch 13: loss=3.064272880554199\n",
      "epoch 14: loss=2.872000217437744\n",
      "epoch 15: loss=2.7115399837493896\n",
      "epoch 16: loss=2.5445852279663086\n",
      "epoch 17: loss=2.428429126739502\n",
      "epoch 18: loss=2.293823719024658\n",
      "epoch 19: loss=2.1326804161071777\n",
      "epoch 20: loss=1.9944822788238525\n",
      "epoch 21: loss=1.8429158926010132\n",
      "epoch 22: loss=1.6933976411819458\n",
      "epoch 23: loss=1.5425972938537598\n",
      "epoch 24: loss=1.4183757305145264\n",
      "epoch 25: loss=1.3138713836669922\n",
      "epoch 26: loss=1.255865216255188\n",
      "epoch 27: loss=1.2110483646392822\n",
      "epoch 28: loss=1.2011799812316895\n",
      "epoch 29: loss=1.1709662675857544\n",
      "epoch 30: loss=1.1316791772842407\n",
      "epoch 31: loss=1.0946141481399536\n",
      "epoch 32: loss=1.053788423538208\n",
      "epoch 33: loss=1.0299352407455444\n",
      "epoch 34: loss=1.0182479619979858\n",
      "epoch 35: loss=1.0021969079971313\n",
      "epoch 36: loss=1.0007355213165283\n",
      "epoch 37: loss=0.9890150427818298\n",
      "epoch 38: loss=0.9751125574111938\n",
      "epoch 39: loss=0.9646403789520264\n",
      "epoch 40: loss=0.9550040364265442\n",
      "epoch 41: loss=0.9532237648963928\n",
      "epoch 42: loss=0.9508541226387024\n",
      "epoch 43: loss=0.9442278146743774\n",
      "epoch 44: loss=0.9404796361923218\n",
      "epoch 45: loss=0.9359738230705261\n",
      "epoch 46: loss=0.9351779818534851\n",
      "epoch 47: loss=0.9305878281593323\n",
      "epoch 48: loss=0.9322143793106079\n",
      "epoch 49: loss=0.924407958984375\n",
      "epoch 50: loss=0.9220929145812988\n",
      "epoch 51: loss=0.9188271164894104\n",
      "epoch 52: loss=0.9183120727539062\n",
      "epoch 53: loss=0.9167506694793701\n",
      "epoch 54: loss=0.919928789138794\n",
      "epoch 55: loss=0.9185313582420349\n",
      "epoch 56: loss=0.9141161441802979\n",
      "epoch 57: loss=0.9121929407119751\n",
      "epoch 58: loss=0.9119005799293518\n",
      "epoch 59: loss=0.9102705717086792\n",
      "epoch 60: loss=0.9156829118728638\n",
      "epoch 61: loss=0.9114389419555664\n",
      "epoch 62: loss=0.9088589549064636\n",
      "epoch 63: loss=0.9107426404953003\n",
      "epoch 64: loss=0.9101489782333374\n",
      "epoch 65: loss=0.9081100821495056\n",
      "epoch 66: loss=0.9069356918334961\n",
      "epoch 67: loss=0.9068437218666077\n",
      "epoch 68: loss=0.906066358089447\n",
      "epoch 69: loss=0.9053229689598083\n",
      "epoch 70: loss=0.903119683265686\n",
      "epoch 71: loss=0.9052577018737793\n",
      "epoch 72: loss=0.9042206406593323\n",
      "epoch 73: loss=0.907670795917511\n",
      "epoch 74: loss=0.9033593535423279\n",
      "epoch 75: loss=0.9048224687576294\n",
      "epoch 76: loss=0.9046683311462402\n",
      "epoch 77: loss=0.9027726650238037\n",
      "epoch 78: loss=0.9038500785827637\n",
      "epoch 79: loss=0.9028098583221436\n",
      "epoch 80: loss=0.9014711976051331\n",
      "epoch 81: loss=0.9000166058540344\n",
      "epoch 82: loss=0.9021530151367188\n",
      "epoch 83: loss=0.9033955931663513\n",
      "epoch 84: loss=0.9009951949119568\n",
      "epoch 85: loss=0.9006714820861816\n",
      "epoch 86: loss=0.9024136662483215\n",
      "epoch 87: loss=0.8998396992683411\n",
      "epoch 88: loss=0.8989401459693909\n",
      "epoch 89: loss=0.8979558944702148\n",
      "epoch 90: loss=0.8988894820213318\n",
      "epoch 91: loss=0.8989306688308716\n",
      "epoch 92: loss=0.898251473903656\n",
      "epoch 93: loss=0.9002273678779602\n",
      "epoch 94: loss=0.8989928960800171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95: loss=0.8983237743377686\n",
      "epoch 96: loss=0.8960514068603516\n",
      "epoch 97: loss=0.8975744843482971\n",
      "epoch 98: loss=0.8972873091697693\n",
      "epoch 99: loss=0.8977044224739075\n",
      "epoch 100: loss=0.8950944542884827\n",
      "epoch 101: loss=0.8963288068771362\n",
      "epoch 102: loss=0.8955268263816833\n",
      "epoch 103: loss=0.8965524435043335\n",
      "epoch 104: loss=0.8969441652297974\n",
      "epoch 105: loss=0.8949435353279114\n",
      "epoch 106: loss=0.8964037299156189\n",
      "epoch 107: loss=0.8946411609649658\n",
      "epoch 108: loss=0.8942927122116089\n",
      "epoch 109: loss=0.8936131596565247\n",
      "epoch 110: loss=0.8926625847816467\n",
      "epoch 111: loss=0.8919342756271362\n",
      "epoch 112: loss=0.8931565880775452\n",
      "epoch 113: loss=0.8918964862823486\n",
      "epoch 114: loss=0.8913187980651855\n",
      "epoch 115: loss=0.8925067186355591\n",
      "epoch 116: loss=0.8923026323318481\n",
      "epoch 117: loss=0.8919216990470886\n",
      "epoch 118: loss=0.8912386298179626\n",
      "epoch 119: loss=0.8908183574676514\n",
      "epoch 120: loss=0.8924145698547363\n",
      "epoch 121: loss=0.8909091949462891\n",
      "epoch 122: loss=0.8904023766517639\n",
      "epoch 123: loss=0.888584554195404\n",
      "epoch 124: loss=0.8894567489624023\n",
      "epoch 125: loss=0.8872526288032532\n",
      "epoch 126: loss=0.8890515565872192\n",
      "epoch 127: loss=0.8884896636009216\n",
      "epoch 128: loss=0.8897506594657898\n",
      "epoch 129: loss=0.8905375599861145\n",
      "epoch 130: loss=0.8876898288726807\n",
      "epoch 131: loss=0.8874906897544861\n",
      "epoch 132: loss=0.8867607712745667\n",
      "epoch 133: loss=0.8858953714370728\n",
      "epoch 134: loss=0.8871046304702759\n",
      "epoch 135: loss=0.8843075633049011\n",
      "epoch 136: loss=0.8843855857849121\n",
      "epoch 137: loss=0.8850734233856201\n",
      "epoch 138: loss=0.8831115961074829\n",
      "epoch 139: loss=0.885361909866333\n",
      "epoch 140: loss=0.8832029104232788\n",
      "epoch 141: loss=0.8823755383491516\n",
      "epoch 142: loss=0.8817194104194641\n",
      "epoch 143: loss=0.8825226426124573\n",
      "epoch 144: loss=0.8821576833724976\n",
      "epoch 145: loss=0.8818269968032837\n",
      "epoch 146: loss=0.8797812461853027\n",
      "epoch 147: loss=0.8801063895225525\n",
      "epoch 148: loss=0.879984974861145\n",
      "epoch 149: loss=0.8793565034866333\n",
      "epoch 150: loss=0.8794076442718506\n",
      "epoch 151: loss=0.8792856931686401\n",
      "epoch 152: loss=0.8781312108039856\n",
      "epoch 153: loss=0.881112813949585\n",
      "epoch 154: loss=0.8793451189994812\n",
      "epoch 155: loss=0.8776965141296387\n",
      "epoch 156: loss=0.876685619354248\n",
      "epoch 157: loss=0.8775862455368042\n",
      "epoch 158: loss=0.8754869103431702\n",
      "epoch 159: loss=0.8753719329833984\n",
      "epoch 160: loss=0.8764503002166748\n",
      "epoch 161: loss=0.872713565826416\n",
      "epoch 162: loss=0.8750960826873779\n",
      "epoch 163: loss=0.8758479952812195\n",
      "epoch 164: loss=0.8719362020492554\n",
      "epoch 165: loss=0.8736794590950012\n",
      "epoch 166: loss=0.8727150559425354\n",
      "epoch 167: loss=0.8687471747398376\n",
      "epoch 168: loss=0.8694208860397339\n",
      "epoch 169: loss=0.8686023354530334\n",
      "epoch 170: loss=0.8685897588729858\n",
      "epoch 171: loss=0.8676093816757202\n",
      "epoch 172: loss=0.8684191703796387\n",
      "epoch 173: loss=0.8670448660850525\n",
      "epoch 174: loss=0.8663257360458374\n",
      "epoch 175: loss=0.866237998008728\n",
      "epoch 176: loss=0.8691990971565247\n",
      "epoch 177: loss=0.8658950328826904\n",
      "epoch 178: loss=0.864067554473877\n",
      "epoch 179: loss=0.866597592830658\n",
      "epoch 180: loss=0.8646392822265625\n",
      "epoch 181: loss=0.8649961948394775\n",
      "epoch 182: loss=0.8652248382568359\n",
      "epoch 183: loss=0.8627997040748596\n",
      "epoch 184: loss=0.8608898520469666\n",
      "epoch 185: loss=0.865539014339447\n",
      "epoch 186: loss=0.865253746509552\n",
      "epoch 187: loss=0.8645938038825989\n",
      "epoch 188: loss=0.8632631301879883\n",
      "epoch 189: loss=0.8601291179656982\n",
      "epoch 190: loss=0.861864447593689\n",
      "epoch 191: loss=0.8628017902374268\n",
      "epoch 192: loss=0.8625069856643677\n",
      "epoch 193: loss=0.8610794544219971\n",
      "epoch 194: loss=0.8602651953697205\n",
      "epoch 195: loss=0.8596926331520081\n",
      "epoch 196: loss=0.8606330752372742\n",
      "epoch 197: loss=0.8620085120201111\n",
      "epoch 198: loss=0.8602999448776245\n",
      "epoch 199: loss=0.8599497079849243\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=9.95837688446045\n",
      "epoch 1: loss=9.478836059570312\n",
      "epoch 2: loss=8.978742599487305\n",
      "epoch 3: loss=8.282450675964355\n",
      "epoch 4: loss=7.30615234375\n",
      "epoch 5: loss=6.356541633605957\n",
      "epoch 6: loss=5.771141052246094\n",
      "epoch 7: loss=5.7676568031311035\n",
      "epoch 8: loss=5.684488773345947\n",
      "epoch 9: loss=5.108267784118652\n",
      "epoch 10: loss=4.521604061126709\n",
      "epoch 11: loss=3.931959629058838\n",
      "epoch 12: loss=3.460376024246216\n",
      "epoch 13: loss=3.1680476665496826\n",
      "epoch 14: loss=2.976654529571533\n",
      "epoch 15: loss=2.8236634731292725\n",
      "epoch 16: loss=2.678858518600464\n",
      "epoch 17: loss=2.511089563369751\n",
      "epoch 18: loss=2.357862949371338\n",
      "epoch 19: loss=2.2160322666168213\n",
      "epoch 20: loss=2.078658103942871\n",
      "epoch 21: loss=1.9814950227737427\n",
      "epoch 22: loss=1.8737348318099976\n",
      "epoch 23: loss=1.7389968633651733\n",
      "epoch 24: loss=1.6121063232421875\n",
      "epoch 25: loss=1.5074772834777832\n",
      "epoch 26: loss=1.4121795892715454\n",
      "epoch 27: loss=1.3305187225341797\n",
      "epoch 28: loss=1.272566318511963\n",
      "epoch 29: loss=1.2434276342391968\n",
      "epoch 30: loss=1.2145841121673584\n",
      "epoch 31: loss=1.1865453720092773\n",
      "epoch 32: loss=1.1346628665924072\n",
      "epoch 33: loss=1.1071629524230957\n",
      "epoch 34: loss=1.0864311456680298\n",
      "epoch 35: loss=1.0732957124710083\n",
      "epoch 36: loss=1.059800624847412\n",
      "epoch 37: loss=1.0501130819320679\n",
      "epoch 38: loss=1.0451040267944336\n",
      "epoch 39: loss=1.0296709537506104\n",
      "epoch 40: loss=1.0035574436187744\n",
      "epoch 41: loss=0.9949323534965515\n",
      "epoch 42: loss=0.9851345419883728\n",
      "epoch 43: loss=0.9831371903419495\n",
      "epoch 44: loss=0.9815059304237366\n",
      "epoch 45: loss=0.9792535305023193\n",
      "epoch 46: loss=0.9770452976226807\n",
      "epoch 47: loss=0.969748318195343\n",
      "epoch 48: loss=0.9603957533836365\n",
      "epoch 49: loss=0.9559383392333984\n",
      "epoch 50: loss=0.9546775221824646\n",
      "epoch 51: loss=0.9546743631362915\n",
      "epoch 52: loss=0.9521497488021851\n",
      "epoch 53: loss=0.9529327750205994\n",
      "epoch 54: loss=0.9497132897377014\n",
      "epoch 55: loss=0.9445767998695374\n",
      "epoch 56: loss=0.943925142288208\n",
      "epoch 57: loss=0.9413898587226868\n",
      "epoch 58: loss=0.9404782652854919\n",
      "epoch 59: loss=0.9414189457893372\n",
      "epoch 60: loss=0.9391051530838013\n",
      "epoch 61: loss=0.9378849864006042\n",
      "epoch 62: loss=0.9351102113723755\n",
      "epoch 63: loss=0.933279275894165\n",
      "epoch 64: loss=0.9341356158256531\n",
      "epoch 65: loss=0.9336974024772644\n",
      "epoch 66: loss=0.9360031485557556\n",
      "epoch 67: loss=0.9323550462722778\n",
      "epoch 68: loss=0.9331227540969849\n",
      "epoch 69: loss=0.9310660362243652\n",
      "epoch 70: loss=0.9304344058036804\n",
      "epoch 71: loss=0.9295139312744141\n",
      "epoch 72: loss=0.9290931224822998\n",
      "epoch 73: loss=0.9280931949615479\n",
      "epoch 74: loss=0.9268344044685364\n",
      "epoch 75: loss=0.9257436990737915\n",
      "epoch 76: loss=0.9256848096847534\n",
      "epoch 77: loss=0.9265961647033691\n",
      "epoch 78: loss=0.9248321652412415\n",
      "epoch 79: loss=0.9253312349319458\n",
      "epoch 80: loss=0.9279186129570007\n",
      "epoch 81: loss=0.925236701965332\n",
      "epoch 82: loss=0.9256405830383301\n",
      "epoch 83: loss=0.9248504042625427\n",
      "epoch 84: loss=0.9216246604919434\n",
      "epoch 85: loss=0.9214104413986206\n",
      "epoch 86: loss=0.9234119653701782\n",
      "epoch 87: loss=0.9239133596420288\n",
      "epoch 88: loss=0.9216852188110352\n",
      "epoch 89: loss=0.9215242862701416\n",
      "epoch 90: loss=0.9207647442817688\n",
      "epoch 91: loss=0.9205607771873474\n",
      "epoch 92: loss=0.9194421172142029\n",
      "epoch 93: loss=0.9197582006454468\n",
      "epoch 94: loss=0.9192746877670288\n",
      "epoch 95: loss=0.9172889590263367\n",
      "epoch 96: loss=0.917997419834137\n",
      "epoch 97: loss=0.9184990525245667\n",
      "epoch 98: loss=0.9167357087135315\n",
      "epoch 99: loss=0.9176523685455322\n",
      "epoch 100: loss=0.916693389415741\n",
      "epoch 101: loss=0.917454183101654\n",
      "epoch 102: loss=0.9157407879829407\n",
      "epoch 103: loss=0.9153414368629456\n",
      "epoch 104: loss=0.9156172275543213\n",
      "epoch 105: loss=0.916687548160553\n",
      "epoch 106: loss=0.9152759313583374\n",
      "epoch 107: loss=0.9155867099761963\n",
      "epoch 108: loss=0.9156484007835388\n",
      "epoch 109: loss=0.9151521921157837\n",
      "epoch 110: loss=0.915741503238678\n",
      "epoch 111: loss=0.9152972102165222\n",
      "epoch 112: loss=0.9130221605300903\n",
      "epoch 113: loss=0.913672924041748\n",
      "epoch 114: loss=0.9129323959350586\n",
      "epoch 115: loss=0.913133442401886\n",
      "epoch 116: loss=0.9112762808799744\n",
      "epoch 117: loss=0.9130774736404419\n",
      "epoch 118: loss=0.9107929468154907\n",
      "epoch 119: loss=0.9098896384239197\n",
      "epoch 120: loss=0.9117743968963623\n",
      "epoch 121: loss=0.9114736914634705\n",
      "epoch 122: loss=0.9094477891921997\n",
      "epoch 123: loss=0.9110609889030457\n",
      "epoch 124: loss=0.9113067388534546\n",
      "epoch 125: loss=0.9101779460906982\n",
      "epoch 126: loss=0.9077394604682922\n",
      "epoch 127: loss=0.9099367260932922\n",
      "epoch 128: loss=0.9088018536567688\n",
      "epoch 129: loss=0.9085550308227539\n",
      "epoch 130: loss=0.9096745848655701\n",
      "epoch 131: loss=0.9119316935539246\n",
      "epoch 132: loss=0.9081095457077026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133: loss=0.9077847003936768\n",
      "epoch 134: loss=0.9077498316764832\n",
      "epoch 135: loss=0.9074772000312805\n",
      "epoch 136: loss=0.9117391109466553\n",
      "epoch 137: loss=0.9056277871131897\n",
      "epoch 138: loss=0.9064087271690369\n",
      "epoch 139: loss=0.9091241955757141\n",
      "epoch 140: loss=0.906956672668457\n",
      "epoch 141: loss=0.9073452949523926\n",
      "epoch 142: loss=0.9069311618804932\n",
      "epoch 143: loss=0.9054607152938843\n",
      "epoch 144: loss=0.9056159257888794\n",
      "epoch 145: loss=0.9042180180549622\n",
      "epoch 146: loss=0.9066051840782166\n",
      "epoch 147: loss=0.9050642848014832\n",
      "epoch 148: loss=0.9040140509605408\n",
      "epoch 149: loss=0.9008211493492126\n",
      "epoch 150: loss=0.9053018093109131\n",
      "epoch 151: loss=0.9026740789413452\n",
      "epoch 152: loss=0.9037860631942749\n",
      "epoch 153: loss=0.904963493347168\n",
      "epoch 154: loss=0.901557981967926\n",
      "epoch 155: loss=0.9019482731819153\n",
      "epoch 156: loss=0.9019737243652344\n",
      "epoch 157: loss=0.9022897481918335\n",
      "epoch 158: loss=0.9030570387840271\n",
      "epoch 159: loss=0.9010623693466187\n",
      "epoch 160: loss=0.8991260528564453\n",
      "epoch 161: loss=0.8998500108718872\n",
      "epoch 162: loss=0.90092933177948\n",
      "epoch 163: loss=0.8990128040313721\n",
      "epoch 164: loss=0.8997334241867065\n",
      "epoch 165: loss=0.8974129557609558\n",
      "epoch 166: loss=0.8983277082443237\n",
      "epoch 167: loss=0.8951152563095093\n",
      "epoch 168: loss=0.8987193703651428\n",
      "epoch 169: loss=0.8978497982025146\n",
      "epoch 170: loss=0.897850751876831\n",
      "epoch 171: loss=0.8960515856742859\n",
      "epoch 172: loss=0.8955036401748657\n",
      "epoch 173: loss=0.8963789343833923\n",
      "epoch 174: loss=0.8966140747070312\n",
      "epoch 175: loss=0.8982732892036438\n",
      "epoch 176: loss=0.8953939080238342\n",
      "epoch 177: loss=0.8938120007514954\n",
      "epoch 178: loss=0.8955259919166565\n",
      "epoch 179: loss=0.8961799740791321\n",
      "epoch 180: loss=0.8966280817985535\n",
      "epoch 181: loss=0.898061215877533\n",
      "epoch 182: loss=0.8919410109519958\n",
      "epoch 183: loss=0.8936977386474609\n",
      "epoch 184: loss=0.8946681022644043\n",
      "epoch 185: loss=0.8925915956497192\n",
      "epoch 186: loss=0.8941331505775452\n",
      "epoch 187: loss=0.8929122686386108\n",
      "epoch 188: loss=0.8919005990028381\n",
      "epoch 189: loss=0.8936305046081543\n",
      "epoch 190: loss=0.8942502737045288\n",
      "epoch 191: loss=0.8898334503173828\n",
      "epoch 192: loss=0.8931342363357544\n",
      "epoch 193: loss=0.8905595541000366\n",
      "epoch 194: loss=0.8900663256645203\n",
      "epoch 195: loss=0.890290379524231\n",
      "epoch 196: loss=0.8915190696716309\n",
      "epoch 197: loss=0.8890067934989929\n",
      "epoch 198: loss=0.8916503190994263\n",
      "epoch 199: loss=0.8895325064659119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6797deddff41ce90965770bd7f285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 129243.3515625\n",
      "Epoch 10, Loss: 63089.5546875\n",
      "Epoch 20, Loss: 55992.08984375\n",
      "Epoch 30, Loss: 64612.9140625\n",
      "Epoch 40, Loss: 59507.66015625\n",
      "Epoch 50, Loss: 41699.62890625\n",
      "Epoch 60, Loss: 32851.078125\n",
      "Epoch 70, Loss: 30073.66015625\n",
      "Epoch 80, Loss: 29623.845703125\n",
      "Epoch 90, Loss: 29004.8515625\n",
      "Epoch 100, Loss: 28624.078125\n",
      "Epoch 110, Loss: 29158.572265625\n",
      "Epoch 120, Loss: 28781.98828125\n",
      "Epoch 130, Loss: 27962.46875\n",
      "Epoch 140, Loss: 28118.498046875\n",
      "Epoch 150, Loss: 28321.23046875\n",
      "Epoch 160, Loss: 27797.05859375\n",
      "Epoch 170, Loss: 27790.828125\n",
      "Epoch 180, Loss: 28006.3515625\n",
      "Epoch 190, Loss: 27775.96875\n",
      "training patch with 267352 edges\n",
      "epoch 0: loss=13.880630493164062\n",
      "epoch 1: loss=13.45230484008789\n",
      "epoch 2: loss=12.943751335144043\n",
      "epoch 3: loss=12.131314277648926\n",
      "epoch 4: loss=10.952062606811523\n",
      "epoch 5: loss=9.780250549316406\n",
      "epoch 6: loss=9.060901641845703\n",
      "epoch 7: loss=9.139625549316406\n",
      "epoch 8: loss=9.207820892333984\n",
      "epoch 9: loss=8.89724349975586\n",
      "epoch 10: loss=8.150101661682129\n",
      "epoch 11: loss=7.23114538192749\n",
      "epoch 12: loss=6.4102783203125\n",
      "epoch 13: loss=5.7244157791137695\n",
      "epoch 14: loss=5.195929050445557\n",
      "epoch 15: loss=4.827531814575195\n",
      "epoch 16: loss=4.561511993408203\n",
      "epoch 17: loss=4.2813849449157715\n",
      "epoch 18: loss=4.028999328613281\n",
      "epoch 19: loss=3.7791683673858643\n",
      "epoch 20: loss=3.460005760192871\n",
      "epoch 21: loss=3.187704086303711\n",
      "epoch 22: loss=2.9243271350860596\n",
      "epoch 23: loss=2.64792799949646\n",
      "epoch 24: loss=2.3559064865112305\n",
      "epoch 25: loss=2.0883917808532715\n",
      "epoch 26: loss=1.8495216369628906\n",
      "epoch 27: loss=1.6621668338775635\n",
      "epoch 28: loss=1.5199636220932007\n",
      "epoch 29: loss=1.4471101760864258\n",
      "epoch 30: loss=1.3835190534591675\n",
      "epoch 31: loss=1.3109732866287231\n",
      "epoch 32: loss=1.2071704864501953\n",
      "epoch 33: loss=1.1354682445526123\n",
      "epoch 34: loss=1.0995465517044067\n",
      "epoch 35: loss=1.0882428884506226\n",
      "epoch 36: loss=1.0745478868484497\n",
      "epoch 37: loss=1.0516749620437622\n",
      "epoch 38: loss=1.0179249048233032\n",
      "epoch 39: loss=0.9883496165275574\n",
      "epoch 40: loss=0.9766001105308533\n",
      "epoch 41: loss=0.9736015796661377\n",
      "epoch 42: loss=0.974042534828186\n",
      "epoch 43: loss=0.9656698703765869\n",
      "epoch 44: loss=0.9549664258956909\n",
      "epoch 45: loss=0.9465611577033997\n",
      "epoch 46: loss=0.9419325590133667\n",
      "epoch 47: loss=0.9419945478439331\n",
      "epoch 48: loss=0.9396708607673645\n",
      "epoch 49: loss=0.9338997602462769\n",
      "epoch 50: loss=0.929397702217102\n",
      "epoch 51: loss=0.9260373711585999\n",
      "epoch 52: loss=0.9243205785751343\n",
      "epoch 53: loss=0.9220322966575623\n",
      "epoch 54: loss=0.9198426008224487\n",
      "epoch 55: loss=0.9165360331535339\n",
      "epoch 56: loss=0.915131688117981\n",
      "epoch 57: loss=0.9145703315734863\n",
      "epoch 58: loss=0.9130678772926331\n",
      "epoch 59: loss=0.9119216799736023\n",
      "epoch 60: loss=0.9094198346138\n",
      "epoch 61: loss=0.9072414040565491\n",
      "epoch 62: loss=0.9076857566833496\n",
      "epoch 63: loss=0.905182421207428\n",
      "epoch 64: loss=0.9051145315170288\n",
      "epoch 65: loss=0.904870331287384\n",
      "epoch 66: loss=0.9034131765365601\n",
      "epoch 67: loss=0.9015668630599976\n",
      "epoch 68: loss=0.9015545845031738\n",
      "epoch 69: loss=0.9009766578674316\n",
      "epoch 70: loss=0.8991718292236328\n",
      "epoch 71: loss=0.8995944857597351\n",
      "epoch 72: loss=0.8980889320373535\n",
      "epoch 73: loss=0.8966848850250244\n",
      "epoch 74: loss=0.8950158953666687\n",
      "epoch 75: loss=0.8966729640960693\n",
      "epoch 76: loss=0.8935264348983765\n",
      "epoch 77: loss=0.8927755355834961\n",
      "epoch 78: loss=0.8923209309577942\n",
      "epoch 79: loss=0.89117032289505\n",
      "epoch 80: loss=0.8896510601043701\n",
      "epoch 81: loss=0.8876135349273682\n",
      "epoch 82: loss=0.8869917392730713\n",
      "epoch 83: loss=0.8849894404411316\n",
      "epoch 84: loss=0.880740225315094\n",
      "epoch 85: loss=0.8798547983169556\n",
      "epoch 86: loss=0.8770055174827576\n",
      "epoch 87: loss=0.8766328692436218\n",
      "epoch 88: loss=0.8750976920127869\n",
      "epoch 89: loss=0.8738352656364441\n",
      "epoch 90: loss=0.8728824853897095\n",
      "epoch 91: loss=0.8702270984649658\n",
      "epoch 92: loss=0.8702459335327148\n",
      "epoch 93: loss=0.8689948320388794\n",
      "epoch 94: loss=0.8673969507217407\n",
      "epoch 95: loss=0.8657207489013672\n",
      "epoch 96: loss=0.8648848533630371\n",
      "epoch 97: loss=0.8630471229553223\n",
      "epoch 98: loss=0.8615765571594238\n",
      "epoch 99: loss=0.8611911535263062\n",
      "epoch 100: loss=0.8605166673660278\n",
      "epoch 101: loss=0.8591322898864746\n",
      "epoch 102: loss=0.8588399887084961\n",
      "epoch 103: loss=0.8553504943847656\n",
      "epoch 104: loss=0.8558792471885681\n",
      "epoch 105: loss=0.8561236262321472\n",
      "epoch 106: loss=0.8552044630050659\n",
      "epoch 107: loss=0.8535299301147461\n",
      "epoch 108: loss=0.854032039642334\n",
      "epoch 109: loss=0.851240336894989\n",
      "epoch 110: loss=0.8514189124107361\n",
      "epoch 111: loss=0.850888729095459\n",
      "epoch 112: loss=0.8499802947044373\n",
      "epoch 113: loss=0.8502498269081116\n",
      "epoch 114: loss=0.8477943539619446\n",
      "epoch 115: loss=0.8485631346702576\n",
      "epoch 116: loss=0.8470069169998169\n",
      "epoch 117: loss=0.8461223244667053\n",
      "epoch 118: loss=0.8446056246757507\n",
      "epoch 119: loss=0.8432360291481018\n",
      "epoch 120: loss=0.8443739414215088\n",
      "epoch 121: loss=0.8432565331459045\n",
      "epoch 122: loss=0.8418492674827576\n",
      "epoch 123: loss=0.8414199352264404\n",
      "epoch 124: loss=0.8424280881881714\n",
      "epoch 125: loss=0.840796947479248\n",
      "epoch 126: loss=0.839387059211731\n",
      "epoch 127: loss=0.8405758142471313\n",
      "epoch 128: loss=0.8391925692558289\n",
      "epoch 129: loss=0.8374494910240173\n",
      "epoch 130: loss=0.838314950466156\n",
      "epoch 131: loss=0.8362764716148376\n",
      "epoch 132: loss=0.8375357389450073\n",
      "epoch 133: loss=0.8355709910392761\n",
      "epoch 134: loss=0.8358484506607056\n",
      "epoch 135: loss=0.8359065651893616\n",
      "epoch 136: loss=0.8332187533378601\n",
      "epoch 137: loss=0.8329604268074036\n",
      "epoch 138: loss=0.8338841199874878\n",
      "epoch 139: loss=0.8332324624061584\n",
      "epoch 140: loss=0.8326546549797058\n",
      "epoch 141: loss=0.8325531482696533\n",
      "epoch 142: loss=0.8326024413108826\n",
      "epoch 143: loss=0.83183354139328\n",
      "epoch 144: loss=0.8321495056152344\n",
      "epoch 145: loss=0.8296283483505249\n",
      "epoch 146: loss=0.8299771547317505\n",
      "epoch 147: loss=0.83049476146698\n",
      "epoch 148: loss=0.8299463987350464\n",
      "epoch 149: loss=0.8294769525527954\n",
      "epoch 150: loss=0.8294153213500977\n",
      "epoch 151: loss=0.8281938433647156\n",
      "epoch 152: loss=0.8268586993217468\n",
      "epoch 153: loss=0.8263668417930603\n",
      "epoch 154: loss=0.8278669714927673\n",
      "epoch 155: loss=0.8282681107521057\n",
      "epoch 156: loss=0.8257449865341187\n",
      "epoch 157: loss=0.8248506784439087\n",
      "epoch 158: loss=0.8250846862792969\n",
      "epoch 159: loss=0.8260987997055054\n",
      "epoch 160: loss=0.8251997828483582\n",
      "epoch 161: loss=0.8250790238380432\n",
      "epoch 162: loss=0.8243968486785889\n",
      "epoch 163: loss=0.8232558369636536\n",
      "epoch 164: loss=0.8237174153327942\n",
      "epoch 165: loss=0.8235407471656799\n",
      "epoch 166: loss=0.8229024410247803\n",
      "epoch 167: loss=0.8231001496315002\n",
      "epoch 168: loss=0.8238613605499268\n",
      "epoch 169: loss=0.8220110535621643\n",
      "epoch 170: loss=0.8216925859451294\n",
      "epoch 171: loss=0.8218047618865967\n",
      "epoch 172: loss=0.8209104537963867\n",
      "epoch 173: loss=0.8206372261047363\n",
      "epoch 174: loss=0.8200894594192505\n",
      "epoch 175: loss=0.8206503391265869\n",
      "epoch 176: loss=0.8199506998062134\n",
      "epoch 177: loss=0.8200110197067261\n",
      "epoch 178: loss=0.8190867304801941\n",
      "epoch 179: loss=0.8176982402801514\n",
      "epoch 180: loss=0.8183022737503052\n",
      "epoch 181: loss=0.8189865946769714\n",
      "epoch 182: loss=0.8180425763130188\n",
      "epoch 183: loss=0.817706286907196\n",
      "epoch 184: loss=0.8181875944137573\n",
      "epoch 185: loss=0.8168600797653198\n",
      "epoch 186: loss=0.8158587217330933\n",
      "epoch 187: loss=0.817984938621521\n",
      "epoch 188: loss=0.8163899779319763\n",
      "epoch 189: loss=0.8167651295661926\n",
      "epoch 190: loss=0.8150100111961365\n",
      "epoch 191: loss=0.8161794543266296\n",
      "epoch 192: loss=0.8151055574417114\n",
      "epoch 193: loss=0.8153860569000244\n",
      "epoch 194: loss=0.8137992024421692\n",
      "epoch 195: loss=0.8166638612747192\n",
      "epoch 196: loss=0.8159458637237549\n",
      "epoch 197: loss=0.8145673274993896\n",
      "epoch 198: loss=0.8140357732772827\n",
      "epoch 199: loss=0.8142392039299011\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=13.934324264526367\n",
      "epoch 1: loss=13.493230819702148\n",
      "epoch 2: loss=13.006516456604004\n",
      "epoch 3: loss=12.134970664978027\n",
      "epoch 4: loss=10.910735130310059\n",
      "epoch 5: loss=9.61491584777832\n",
      "epoch 6: loss=8.954251289367676\n",
      "epoch 7: loss=9.197187423706055\n",
      "epoch 8: loss=9.314912796020508\n",
      "epoch 9: loss=8.872723579406738\n",
      "epoch 10: loss=8.052431106567383\n",
      "epoch 11: loss=7.045210838317871\n",
      "epoch 12: loss=6.101315975189209\n",
      "epoch 13: loss=5.409542083740234\n",
      "epoch 14: loss=4.936862468719482\n",
      "epoch 15: loss=4.634368896484375\n",
      "epoch 16: loss=4.397241592407227\n",
      "epoch 17: loss=4.178095817565918\n",
      "epoch 18: loss=3.9129586219787598\n",
      "epoch 19: loss=3.631945848464966\n",
      "epoch 20: loss=3.4000823497772217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: loss=3.146009922027588\n",
      "epoch 22: loss=2.9142768383026123\n",
      "epoch 23: loss=2.6443052291870117\n",
      "epoch 24: loss=2.380457639694214\n",
      "epoch 25: loss=2.1364197731018066\n",
      "epoch 26: loss=1.9249874353408813\n",
      "epoch 27: loss=1.7732977867126465\n",
      "epoch 28: loss=1.6436665058135986\n",
      "epoch 29: loss=1.5657093524932861\n",
      "epoch 30: loss=1.4953218698501587\n",
      "epoch 31: loss=1.4060007333755493\n",
      "epoch 32: loss=1.3203949928283691\n",
      "epoch 33: loss=1.2452006340026855\n",
      "epoch 34: loss=1.1929402351379395\n",
      "epoch 35: loss=1.1582053899765015\n",
      "epoch 36: loss=1.1315624713897705\n",
      "epoch 37: loss=1.1061623096466064\n",
      "epoch 38: loss=1.0735690593719482\n",
      "epoch 39: loss=1.0418343544006348\n",
      "epoch 40: loss=1.0179299116134644\n",
      "epoch 41: loss=1.005709171295166\n",
      "epoch 42: loss=0.9990662336349487\n",
      "epoch 43: loss=0.991994321346283\n",
      "epoch 44: loss=0.9845830798149109\n",
      "epoch 45: loss=0.9707670211791992\n",
      "epoch 46: loss=0.9613291621208191\n",
      "epoch 47: loss=0.9548384547233582\n",
      "epoch 48: loss=0.9514996409416199\n",
      "epoch 49: loss=0.948627769947052\n",
      "epoch 50: loss=0.9450130462646484\n",
      "epoch 51: loss=0.9395450949668884\n",
      "epoch 52: loss=0.9364369511604309\n",
      "epoch 53: loss=0.9337114691734314\n",
      "epoch 54: loss=0.9323550462722778\n",
      "epoch 55: loss=0.9295448064804077\n",
      "epoch 56: loss=0.9266694188117981\n",
      "epoch 57: loss=0.9249086976051331\n",
      "epoch 58: loss=0.9241796731948853\n",
      "epoch 59: loss=0.9223256707191467\n",
      "epoch 60: loss=0.9196035861968994\n",
      "epoch 61: loss=0.9190693497657776\n",
      "epoch 62: loss=0.9177326560020447\n",
      "epoch 63: loss=0.9163165092468262\n",
      "epoch 64: loss=0.9159560203552246\n",
      "epoch 65: loss=0.9147277474403381\n",
      "epoch 66: loss=0.9129049181938171\n",
      "epoch 67: loss=0.9129243493080139\n",
      "epoch 68: loss=0.9115857481956482\n",
      "epoch 69: loss=0.9112092852592468\n",
      "epoch 70: loss=0.9105672240257263\n",
      "epoch 71: loss=0.9079936742782593\n",
      "epoch 72: loss=0.9094085693359375\n",
      "epoch 73: loss=0.908033013343811\n",
      "epoch 74: loss=0.9063470959663391\n",
      "epoch 75: loss=0.9066423773765564\n",
      "epoch 76: loss=0.9070883393287659\n",
      "epoch 77: loss=0.9041239619255066\n",
      "epoch 78: loss=0.9052507877349854\n",
      "epoch 79: loss=0.9047906994819641\n",
      "epoch 80: loss=0.9046639800071716\n",
      "epoch 81: loss=0.9031849503517151\n",
      "epoch 82: loss=0.9027692079544067\n",
      "epoch 83: loss=0.9035359621047974\n",
      "epoch 84: loss=0.9004870653152466\n",
      "epoch 85: loss=0.8996263742446899\n",
      "epoch 86: loss=0.9013752937316895\n",
      "epoch 87: loss=0.9000178575515747\n",
      "epoch 88: loss=0.8998141884803772\n",
      "epoch 89: loss=0.8997402191162109\n",
      "epoch 90: loss=0.8992093205451965\n",
      "epoch 91: loss=0.8969025015830994\n",
      "epoch 92: loss=0.8959101438522339\n",
      "epoch 93: loss=0.8961217999458313\n",
      "epoch 94: loss=0.8955928683280945\n",
      "epoch 95: loss=0.8953564763069153\n",
      "epoch 96: loss=0.8950176239013672\n",
      "epoch 97: loss=0.8921695351600647\n",
      "epoch 98: loss=0.8934867978096008\n",
      "epoch 99: loss=0.8940661549568176\n",
      "epoch 100: loss=0.8917106986045837\n",
      "epoch 101: loss=0.8919106721878052\n",
      "epoch 102: loss=0.8899297118186951\n",
      "epoch 103: loss=0.890878438949585\n",
      "epoch 104: loss=0.8898789286613464\n",
      "epoch 105: loss=0.8875926733016968\n",
      "epoch 106: loss=0.8885677456855774\n",
      "epoch 107: loss=0.8871238827705383\n",
      "epoch 108: loss=0.8867608308792114\n",
      "epoch 109: loss=0.8850448727607727\n",
      "epoch 110: loss=0.8854783177375793\n",
      "epoch 111: loss=0.8862385749816895\n",
      "epoch 112: loss=0.8843608498573303\n",
      "epoch 113: loss=0.8851844072341919\n",
      "epoch 114: loss=0.8824136853218079\n",
      "epoch 115: loss=0.8822717666625977\n",
      "epoch 116: loss=0.8823357224464417\n",
      "epoch 117: loss=0.8819348812103271\n",
      "epoch 118: loss=0.880462646484375\n",
      "epoch 119: loss=0.8790149092674255\n",
      "epoch 120: loss=0.8787389397621155\n",
      "epoch 121: loss=0.8783828020095825\n",
      "epoch 122: loss=0.877526581287384\n",
      "epoch 123: loss=0.8764548897743225\n",
      "epoch 124: loss=0.8769141435623169\n",
      "epoch 125: loss=0.8743740916252136\n",
      "epoch 126: loss=0.8753053545951843\n",
      "epoch 127: loss=0.8741136789321899\n",
      "epoch 128: loss=0.8734923005104065\n",
      "epoch 129: loss=0.8718318939208984\n",
      "epoch 130: loss=0.8712089657783508\n",
      "epoch 131: loss=0.8706021308898926\n",
      "epoch 132: loss=0.8674226403236389\n",
      "epoch 133: loss=0.8673803806304932\n",
      "epoch 134: loss=0.866125762462616\n",
      "epoch 135: loss=0.8661096692085266\n",
      "epoch 136: loss=0.8654690384864807\n",
      "epoch 137: loss=0.8622924089431763\n",
      "epoch 138: loss=0.8626422882080078\n",
      "epoch 139: loss=0.8638753294944763\n",
      "epoch 140: loss=0.8626917004585266\n",
      "epoch 141: loss=0.8615447878837585\n",
      "epoch 142: loss=0.8582931160926819\n",
      "epoch 143: loss=0.8581579923629761\n",
      "epoch 144: loss=0.8579704165458679\n",
      "epoch 145: loss=0.8566951751708984\n",
      "epoch 146: loss=0.8559208512306213\n",
      "epoch 147: loss=0.8553491830825806\n",
      "epoch 148: loss=0.8550787568092346\n",
      "epoch 149: loss=0.8559033274650574\n",
      "epoch 150: loss=0.8534214496612549\n",
      "epoch 151: loss=0.8536825776100159\n",
      "epoch 152: loss=0.8531151413917542\n",
      "epoch 153: loss=0.8522982597351074\n",
      "epoch 154: loss=0.8514829277992249\n",
      "epoch 155: loss=0.8518595695495605\n",
      "epoch 156: loss=0.8508011698722839\n",
      "epoch 157: loss=0.8517917990684509\n",
      "epoch 158: loss=0.851158857345581\n",
      "epoch 159: loss=0.8507055640220642\n",
      "epoch 160: loss=0.849639892578125\n",
      "epoch 161: loss=0.8500782251358032\n",
      "epoch 162: loss=0.8487093448638916\n",
      "epoch 163: loss=0.848442554473877\n",
      "epoch 164: loss=0.8483348488807678\n",
      "epoch 165: loss=0.8481941223144531\n",
      "epoch 166: loss=0.8478934168815613\n",
      "epoch 167: loss=0.847632110118866\n",
      "epoch 168: loss=0.8467481732368469\n",
      "epoch 169: loss=0.8467413187026978\n",
      "epoch 170: loss=0.8471765518188477\n",
      "epoch 171: loss=0.8470348119735718\n",
      "epoch 172: loss=0.8457792401313782\n",
      "epoch 173: loss=0.845072865486145\n",
      "epoch 174: loss=0.8454574942588806\n",
      "epoch 175: loss=0.8456218838691711\n",
      "epoch 176: loss=0.8435274362564087\n",
      "epoch 177: loss=0.8437492251396179\n",
      "epoch 178: loss=0.8437126278877258\n",
      "epoch 179: loss=0.8452604413032532\n",
      "epoch 180: loss=0.8439821004867554\n",
      "epoch 181: loss=0.8439340591430664\n",
      "epoch 182: loss=0.8428757190704346\n",
      "epoch 183: loss=0.8411604762077332\n",
      "epoch 184: loss=0.8414278626441956\n",
      "epoch 185: loss=0.8426898717880249\n",
      "epoch 186: loss=0.8421006202697754\n",
      "epoch 187: loss=0.8408122062683105\n",
      "epoch 188: loss=0.842368483543396\n",
      "epoch 189: loss=0.841569185256958\n",
      "epoch 190: loss=0.8423734903335571\n",
      "epoch 191: loss=0.84200519323349\n",
      "epoch 192: loss=0.8414750099182129\n",
      "epoch 193: loss=0.840659499168396\n",
      "epoch 194: loss=0.8402557373046875\n",
      "epoch 195: loss=0.8412238955497742\n",
      "epoch 196: loss=0.8414311408996582\n",
      "epoch 197: loss=0.8417902588844299\n",
      "epoch 198: loss=0.8407220244407654\n",
      "epoch 199: loss=0.8387879133224487\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=14.03919792175293\n",
      "epoch 1: loss=13.541569709777832\n",
      "epoch 2: loss=13.117082595825195\n",
      "epoch 3: loss=12.43966293334961\n",
      "epoch 4: loss=11.250628471374512\n",
      "epoch 5: loss=10.229063987731934\n",
      "epoch 6: loss=9.277176856994629\n",
      "epoch 7: loss=9.195783615112305\n",
      "epoch 8: loss=9.53342056274414\n",
      "epoch 9: loss=9.0006685256958\n",
      "epoch 10: loss=7.850212574005127\n",
      "epoch 11: loss=6.567529678344727\n",
      "epoch 12: loss=5.571638107299805\n",
      "epoch 13: loss=4.911707878112793\n",
      "epoch 14: loss=4.534233570098877\n",
      "epoch 15: loss=4.1867475509643555\n",
      "epoch 16: loss=3.895289182662964\n",
      "epoch 17: loss=3.5709290504455566\n",
      "epoch 18: loss=3.30741286277771\n",
      "epoch 19: loss=3.121952772140503\n",
      "epoch 20: loss=2.9015259742736816\n",
      "epoch 21: loss=2.596453905105591\n",
      "epoch 22: loss=2.3152458667755127\n",
      "epoch 23: loss=2.0915915966033936\n",
      "epoch 24: loss=1.932956337928772\n",
      "epoch 25: loss=1.789345622062683\n",
      "epoch 26: loss=1.708603024482727\n",
      "epoch 27: loss=1.631788730621338\n",
      "epoch 28: loss=1.52798330783844\n",
      "epoch 29: loss=1.4257789850234985\n",
      "epoch 30: loss=1.3491990566253662\n",
      "epoch 31: loss=1.2903200387954712\n",
      "epoch 32: loss=1.257436990737915\n",
      "epoch 33: loss=1.243973731994629\n",
      "epoch 34: loss=1.2185860872268677\n",
      "epoch 35: loss=1.191551685333252\n",
      "epoch 36: loss=1.1589657068252563\n",
      "epoch 37: loss=1.1313731670379639\n",
      "epoch 38: loss=1.1152563095092773\n",
      "epoch 39: loss=1.10006582736969\n",
      "epoch 40: loss=1.084065318107605\n",
      "epoch 41: loss=1.080023169517517\n",
      "epoch 42: loss=1.0686206817626953\n",
      "epoch 43: loss=1.0628044605255127\n",
      "epoch 44: loss=1.0571317672729492\n",
      "epoch 45: loss=1.0496138334274292\n",
      "epoch 46: loss=1.0423078536987305\n",
      "epoch 47: loss=1.0379934310913086\n",
      "epoch 48: loss=1.0330545902252197\n",
      "epoch 49: loss=1.0283550024032593\n",
      "epoch 50: loss=1.0281208753585815\n",
      "epoch 51: loss=1.0245628356933594\n",
      "epoch 52: loss=1.0207862854003906\n",
      "epoch 53: loss=1.02206552028656\n",
      "epoch 54: loss=1.0184515714645386\n",
      "epoch 55: loss=1.0112913846969604\n",
      "epoch 56: loss=1.011269211769104\n",
      "epoch 57: loss=1.0078667402267456\n",
      "epoch 58: loss=1.0111030340194702\n",
      "epoch 59: loss=1.0081572532653809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60: loss=1.0069658756256104\n",
      "epoch 61: loss=1.002956509590149\n",
      "epoch 62: loss=1.0013834238052368\n",
      "epoch 63: loss=0.9983572959899902\n",
      "epoch 64: loss=0.9983720183372498\n",
      "epoch 65: loss=0.9969650506973267\n",
      "epoch 66: loss=0.9971053600311279\n",
      "epoch 67: loss=0.9947400689125061\n",
      "epoch 68: loss=0.9912360310554504\n",
      "epoch 69: loss=0.9920618534088135\n",
      "epoch 70: loss=0.9899166226387024\n",
      "epoch 71: loss=0.9879851341247559\n",
      "epoch 72: loss=0.9906581044197083\n",
      "epoch 73: loss=0.9854864478111267\n",
      "epoch 74: loss=0.9875898361206055\n",
      "epoch 75: loss=0.9836710691452026\n",
      "epoch 76: loss=0.9849417209625244\n",
      "epoch 77: loss=0.9829520583152771\n",
      "epoch 78: loss=0.9798560738563538\n",
      "epoch 79: loss=0.9814488291740417\n",
      "epoch 80: loss=0.9791464805603027\n",
      "epoch 81: loss=0.9789893627166748\n",
      "epoch 82: loss=0.979325532913208\n",
      "epoch 83: loss=0.980412483215332\n",
      "epoch 84: loss=0.975812554359436\n",
      "epoch 85: loss=0.9744805693626404\n",
      "epoch 86: loss=0.9740525484085083\n",
      "epoch 87: loss=0.9725667834281921\n",
      "epoch 88: loss=0.9700926542282104\n",
      "epoch 89: loss=0.9706140756607056\n",
      "epoch 90: loss=0.9664372801780701\n",
      "epoch 91: loss=0.9702509045600891\n",
      "epoch 92: loss=0.9664897322654724\n",
      "epoch 93: loss=0.9659637212753296\n",
      "epoch 94: loss=0.966328501701355\n",
      "epoch 95: loss=0.9641430377960205\n",
      "epoch 96: loss=0.9626425504684448\n",
      "epoch 97: loss=0.962413489818573\n",
      "epoch 98: loss=0.9606468677520752\n",
      "epoch 99: loss=0.9574021697044373\n",
      "epoch 100: loss=0.9585736989974976\n",
      "epoch 101: loss=0.9586389064788818\n",
      "epoch 102: loss=0.9568329453468323\n",
      "epoch 103: loss=0.9546536207199097\n",
      "epoch 104: loss=0.9532028436660767\n",
      "epoch 105: loss=0.9516087174415588\n",
      "epoch 106: loss=0.9503946900367737\n",
      "epoch 107: loss=0.9504767656326294\n",
      "epoch 108: loss=0.9484105706214905\n",
      "epoch 109: loss=0.9509474039077759\n",
      "epoch 110: loss=0.9474152326583862\n",
      "epoch 111: loss=0.9469344019889832\n",
      "epoch 112: loss=0.9474053978919983\n",
      "epoch 113: loss=0.9439404010772705\n",
      "epoch 114: loss=0.9423584342002869\n",
      "epoch 115: loss=0.94142085313797\n",
      "epoch 116: loss=0.9435859322547913\n",
      "epoch 117: loss=0.94107586145401\n",
      "epoch 118: loss=0.9406425952911377\n",
      "epoch 119: loss=0.9407853484153748\n",
      "epoch 120: loss=0.9377461075782776\n",
      "epoch 121: loss=0.9390327334403992\n",
      "epoch 122: loss=0.9402798414230347\n",
      "epoch 123: loss=0.941652774810791\n",
      "epoch 124: loss=0.9369639158248901\n",
      "epoch 125: loss=0.9385796785354614\n",
      "epoch 126: loss=0.9362190365791321\n",
      "epoch 127: loss=0.9341269135475159\n",
      "epoch 128: loss=0.9352468848228455\n",
      "epoch 129: loss=0.9372996091842651\n",
      "epoch 130: loss=0.9349912405014038\n",
      "epoch 131: loss=0.9345324635505676\n",
      "epoch 132: loss=0.9325181245803833\n",
      "epoch 133: loss=0.9321736693382263\n",
      "epoch 134: loss=0.9325280785560608\n",
      "epoch 135: loss=0.9324462413787842\n",
      "epoch 136: loss=0.9319676756858826\n",
      "epoch 137: loss=0.929807722568512\n",
      "epoch 138: loss=0.9318447113037109\n",
      "epoch 139: loss=0.9296626448631287\n",
      "epoch 140: loss=0.9315056800842285\n",
      "epoch 141: loss=0.9277287721633911\n",
      "epoch 142: loss=0.9321115016937256\n",
      "epoch 143: loss=0.9313448071479797\n",
      "epoch 144: loss=0.9247252941131592\n",
      "epoch 145: loss=0.9302941560745239\n",
      "epoch 146: loss=0.9284741878509521\n",
      "epoch 147: loss=0.9299722909927368\n",
      "epoch 148: loss=0.9274873733520508\n",
      "epoch 149: loss=0.927204430103302\n",
      "epoch 150: loss=0.9245876669883728\n",
      "epoch 151: loss=0.9285259246826172\n",
      "epoch 152: loss=0.9299598336219788\n",
      "epoch 153: loss=0.928569495677948\n",
      "epoch 154: loss=0.9219118356704712\n",
      "epoch 155: loss=0.928014874458313\n",
      "epoch 156: loss=0.9255618453025818\n",
      "epoch 157: loss=0.9246748685836792\n",
      "epoch 158: loss=0.9257829785346985\n",
      "epoch 159: loss=0.9235116839408875\n",
      "epoch 160: loss=0.9251035451889038\n",
      "epoch 161: loss=0.9256733059883118\n",
      "epoch 162: loss=0.9252103567123413\n",
      "epoch 163: loss=0.922802209854126\n",
      "epoch 164: loss=0.9218384623527527\n",
      "epoch 165: loss=0.9212235808372498\n",
      "epoch 166: loss=0.9218165874481201\n",
      "epoch 167: loss=0.9247194528579712\n",
      "epoch 168: loss=0.9246159791946411\n",
      "epoch 169: loss=0.9196191430091858\n",
      "epoch 170: loss=0.92253178358078\n",
      "epoch 171: loss=0.9204238057136536\n",
      "epoch 172: loss=0.9243654608726501\n",
      "epoch 173: loss=0.9235806465148926\n",
      "epoch 174: loss=0.9197900891304016\n",
      "epoch 175: loss=0.9209257364273071\n",
      "epoch 176: loss=0.9210245609283447\n",
      "epoch 177: loss=0.9210609197616577\n",
      "epoch 178: loss=0.919976532459259\n",
      "epoch 179: loss=0.9175296425819397\n",
      "epoch 180: loss=0.9176851511001587\n",
      "epoch 181: loss=0.9206352829933167\n",
      "epoch 182: loss=0.9211330413818359\n",
      "epoch 183: loss=0.9194482564926147\n",
      "epoch 184: loss=0.9184681177139282\n",
      "epoch 185: loss=0.9176889657974243\n",
      "epoch 186: loss=0.9190970063209534\n",
      "epoch 187: loss=0.9209815859794617\n",
      "epoch 188: loss=0.9169736504554749\n",
      "epoch 189: loss=0.9180760979652405\n",
      "epoch 190: loss=0.9162242412567139\n",
      "epoch 191: loss=0.9210517406463623\n",
      "epoch 192: loss=0.9233592748641968\n",
      "epoch 193: loss=0.9151991009712219\n",
      "epoch 194: loss=0.9213399887084961\n",
      "epoch 195: loss=0.9167815446853638\n",
      "epoch 196: loss=0.9167341589927673\n",
      "epoch 197: loss=0.915586531162262\n",
      "epoch 198: loss=0.9143097400665283\n",
      "epoch 199: loss=0.9165417551994324\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=14.016800880432129\n",
      "epoch 1: loss=13.858665466308594\n",
      "epoch 2: loss=13.94082260131836\n",
      "epoch 3: loss=13.11940860748291\n",
      "epoch 4: loss=12.321901321411133\n",
      "epoch 5: loss=12.824125289916992\n",
      "epoch 6: loss=12.17990493774414\n",
      "epoch 7: loss=11.263214111328125\n",
      "epoch 8: loss=10.780043601989746\n",
      "epoch 9: loss=10.93269157409668\n",
      "epoch 10: loss=10.214882850646973\n",
      "epoch 11: loss=10.574141502380371\n",
      "epoch 12: loss=10.902809143066406\n",
      "epoch 13: loss=9.84090805053711\n",
      "epoch 14: loss=9.2318754196167\n",
      "epoch 15: loss=8.667824745178223\n",
      "epoch 16: loss=8.09427261352539\n",
      "epoch 17: loss=7.671098709106445\n",
      "epoch 18: loss=6.919431686401367\n",
      "epoch 19: loss=6.913620948791504\n",
      "epoch 20: loss=5.361194133758545\n",
      "epoch 21: loss=5.73596715927124\n",
      "epoch 22: loss=4.910998821258545\n",
      "epoch 23: loss=4.710963726043701\n",
      "epoch 24: loss=4.689888954162598\n",
      "epoch 25: loss=4.649085521697998\n",
      "epoch 26: loss=3.725883960723877\n",
      "epoch 27: loss=3.461043357849121\n",
      "epoch 28: loss=3.3858420848846436\n",
      "epoch 29: loss=3.1140034198760986\n",
      "epoch 30: loss=2.667818546295166\n",
      "epoch 31: loss=2.7642788887023926\n",
      "epoch 32: loss=2.5126664638519287\n",
      "epoch 33: loss=2.296552896499634\n",
      "epoch 34: loss=2.4107825756073\n",
      "epoch 35: loss=2.158313035964966\n",
      "epoch 36: loss=1.9619865417480469\n",
      "epoch 37: loss=2.0689144134521484\n",
      "epoch 38: loss=1.9805724620819092\n",
      "epoch 39: loss=1.804876685142517\n",
      "epoch 40: loss=1.8484928607940674\n",
      "epoch 41: loss=1.8904790878295898\n",
      "epoch 42: loss=1.871252417564392\n",
      "epoch 43: loss=1.7439961433410645\n",
      "epoch 44: loss=1.9356988668441772\n",
      "epoch 45: loss=1.7307677268981934\n",
      "epoch 46: loss=1.692225456237793\n",
      "epoch 47: loss=1.7467005252838135\n",
      "epoch 48: loss=1.6963169574737549\n",
      "epoch 49: loss=1.626387357711792\n",
      "epoch 50: loss=1.6760447025299072\n",
      "epoch 51: loss=1.7374831438064575\n",
      "epoch 52: loss=1.6633565425872803\n",
      "epoch 53: loss=1.7136911153793335\n",
      "epoch 54: loss=1.7022136449813843\n",
      "epoch 55: loss=1.6764492988586426\n",
      "epoch 56: loss=1.6587109565734863\n",
      "epoch 57: loss=1.7397303581237793\n",
      "epoch 58: loss=1.6295554637908936\n",
      "epoch 59: loss=1.661623477935791\n",
      "epoch 60: loss=1.6500005722045898\n",
      "epoch 61: loss=1.6477502584457397\n",
      "epoch 62: loss=1.602034568786621\n",
      "epoch 63: loss=1.687288761138916\n",
      "epoch 64: loss=1.7596321105957031\n",
      "epoch 65: loss=1.6791480779647827\n",
      "epoch 66: loss=1.6161072254180908\n",
      "epoch 67: loss=1.5644727945327759\n",
      "epoch 68: loss=1.6099164485931396\n",
      "epoch 69: loss=1.7267791032791138\n",
      "epoch 70: loss=1.5896580219268799\n",
      "epoch 71: loss=1.5688130855560303\n",
      "epoch 72: loss=1.6257679462432861\n",
      "epoch 73: loss=1.6845778226852417\n",
      "epoch 74: loss=1.590938687324524\n",
      "epoch 75: loss=1.6031246185302734\n",
      "epoch 76: loss=1.5376259088516235\n",
      "epoch 77: loss=1.669979214668274\n",
      "epoch 78: loss=1.7626599073410034\n",
      "epoch 79: loss=1.5688433647155762\n",
      "epoch 80: loss=1.611201524734497\n",
      "epoch 81: loss=1.6572017669677734\n",
      "epoch 82: loss=1.5980589389801025\n",
      "epoch 83: loss=1.6669116020202637\n",
      "epoch 84: loss=1.6517486572265625\n",
      "epoch 85: loss=1.6908583641052246\n",
      "epoch 86: loss=1.7727515697479248\n",
      "epoch 87: loss=1.5481665134429932\n",
      "epoch 88: loss=1.5431277751922607\n",
      "epoch 89: loss=1.6485987901687622\n",
      "epoch 90: loss=1.5610445737838745\n",
      "epoch 91: loss=1.6512175798416138\n",
      "epoch 92: loss=1.6334655284881592\n",
      "epoch 93: loss=1.6397721767425537\n",
      "epoch 94: loss=1.5978870391845703\n",
      "epoch 95: loss=1.5595706701278687\n",
      "epoch 96: loss=1.6764715909957886\n",
      "epoch 97: loss=1.6154171228408813\n",
      "epoch 98: loss=1.7114828824996948\n",
      "epoch 99: loss=1.6018526554107666\n",
      "epoch 100: loss=1.500427484512329\n",
      "epoch 101: loss=1.6357473134994507\n",
      "epoch 102: loss=1.618316888809204\n",
      "epoch 103: loss=1.5986361503601074\n",
      "epoch 104: loss=1.6418066024780273\n",
      "epoch 105: loss=1.5812091827392578\n",
      "epoch 106: loss=1.776340365409851\n",
      "epoch 107: loss=1.619450330734253\n",
      "epoch 108: loss=1.5184227228164673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109: loss=1.614492416381836\n",
      "epoch 110: loss=1.6087791919708252\n",
      "epoch 111: loss=1.6039087772369385\n",
      "epoch 112: loss=1.5232419967651367\n",
      "epoch 113: loss=1.5808252096176147\n",
      "epoch 114: loss=1.6551865339279175\n",
      "epoch 115: loss=1.5433921813964844\n",
      "epoch 116: loss=1.6636394262313843\n",
      "epoch 117: loss=1.5414456129074097\n",
      "epoch 118: loss=1.5468542575836182\n",
      "epoch 119: loss=1.6051615476608276\n",
      "epoch 120: loss=1.7466351985931396\n",
      "epoch 121: loss=1.6098865270614624\n",
      "epoch 122: loss=1.6354010105133057\n",
      "epoch 123: loss=1.540870189666748\n",
      "epoch 124: loss=1.6293818950653076\n",
      "epoch 125: loss=1.614262342453003\n",
      "epoch 126: loss=1.6655625104904175\n",
      "epoch 127: loss=1.5866703987121582\n",
      "epoch 128: loss=1.6581963300704956\n",
      "epoch 129: loss=1.67935049533844\n",
      "epoch 130: loss=1.5655797719955444\n",
      "epoch 131: loss=1.6187440156936646\n",
      "epoch 132: loss=1.6124604940414429\n",
      "epoch 133: loss=1.641411542892456\n",
      "epoch 134: loss=1.6333037614822388\n",
      "epoch 135: loss=1.6141314506530762\n",
      "epoch 136: loss=1.65723717212677\n",
      "epoch 137: loss=1.6211307048797607\n",
      "epoch 138: loss=1.5707029104232788\n",
      "epoch 139: loss=1.6969491243362427\n",
      "epoch 140: loss=1.6418613195419312\n",
      "epoch 141: loss=1.6281391382217407\n",
      "epoch 142: loss=1.5941287279129028\n",
      "epoch 143: loss=1.6420754194259644\n",
      "epoch 144: loss=1.628954529762268\n",
      "epoch 145: loss=1.6891438961029053\n",
      "epoch 146: loss=1.5933618545532227\n",
      "epoch 147: loss=1.638201355934143\n",
      "epoch 148: loss=1.5601873397827148\n",
      "epoch 149: loss=1.6004892587661743\n",
      "epoch 150: loss=1.6525499820709229\n",
      "epoch 151: loss=1.5975189208984375\n",
      "epoch 152: loss=1.650408387184143\n",
      "epoch 153: loss=1.58518385887146\n",
      "epoch 154: loss=1.6365848779678345\n",
      "epoch 155: loss=1.6428219079971313\n",
      "epoch 156: loss=1.6283106803894043\n",
      "epoch 157: loss=1.598789930343628\n",
      "epoch 158: loss=1.5300602912902832\n",
      "epoch 159: loss=1.5635769367218018\n",
      "epoch 160: loss=1.5205585956573486\n",
      "epoch 161: loss=1.6327862739562988\n",
      "epoch 162: loss=1.5633543729782104\n",
      "epoch 163: loss=1.5837087631225586\n",
      "epoch 164: loss=1.6417732238769531\n",
      "epoch 165: loss=1.5191280841827393\n",
      "epoch 166: loss=1.5383706092834473\n",
      "epoch 167: loss=1.5268316268920898\n",
      "epoch 168: loss=1.5047318935394287\n",
      "epoch 169: loss=1.5271230936050415\n",
      "epoch 170: loss=1.646391749382019\n",
      "epoch 171: loss=1.551906704902649\n",
      "epoch 172: loss=1.6171702146530151\n",
      "epoch 173: loss=1.648808240890503\n",
      "epoch 174: loss=1.6194665431976318\n",
      "epoch 175: loss=1.5535557270050049\n",
      "epoch 176: loss=1.641552209854126\n",
      "epoch 177: loss=1.5987430810928345\n",
      "epoch 178: loss=1.562286376953125\n",
      "epoch 179: loss=1.5693386793136597\n",
      "epoch 180: loss=1.5132502317428589\n",
      "epoch 181: loss=1.5099806785583496\n",
      "epoch 182: loss=1.642393946647644\n",
      "epoch 183: loss=1.6219886541366577\n",
      "epoch 184: loss=1.6609481573104858\n",
      "epoch 185: loss=1.6157331466674805\n",
      "epoch 186: loss=1.555554986000061\n",
      "epoch 187: loss=1.6025187969207764\n",
      "epoch 188: loss=1.6057251691818237\n",
      "epoch 189: loss=1.6411100625991821\n",
      "epoch 190: loss=1.6320573091506958\n",
      "epoch 191: loss=1.5428928136825562\n",
      "epoch 192: loss=1.5819463729858398\n",
      "epoch 193: loss=1.6047968864440918\n",
      "epoch 194: loss=1.5889421701431274\n",
      "epoch 195: loss=1.5463926792144775\n",
      "epoch 196: loss=1.6376545429229736\n",
      "epoch 197: loss=1.5579872131347656\n",
      "epoch 198: loss=1.6784095764160156\n",
      "epoch 199: loss=1.5389081239700317\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=14.040660858154297\n",
      "epoch 1: loss=13.633317947387695\n",
      "epoch 2: loss=12.951449394226074\n",
      "epoch 3: loss=11.990837097167969\n",
      "epoch 4: loss=10.517324447631836\n",
      "epoch 5: loss=9.24106216430664\n",
      "epoch 6: loss=8.83546257019043\n",
      "epoch 7: loss=9.21956729888916\n",
      "epoch 8: loss=9.303399085998535\n",
      "epoch 9: loss=8.7570161819458\n",
      "epoch 10: loss=7.897777080535889\n",
      "epoch 11: loss=6.946227073669434\n",
      "epoch 12: loss=6.103219509124756\n",
      "epoch 13: loss=5.4270477294921875\n",
      "epoch 14: loss=5.0147223472595215\n",
      "epoch 15: loss=4.776574611663818\n",
      "epoch 16: loss=4.595503807067871\n",
      "epoch 17: loss=4.39812707901001\n",
      "epoch 18: loss=4.18187952041626\n",
      "epoch 19: loss=3.9487624168395996\n",
      "epoch 20: loss=3.748027801513672\n",
      "epoch 21: loss=3.492459535598755\n",
      "epoch 22: loss=3.275275945663452\n",
      "epoch 23: loss=3.0275352001190186\n",
      "epoch 24: loss=2.7354824542999268\n",
      "epoch 25: loss=2.4892005920410156\n",
      "epoch 26: loss=2.243069648742676\n",
      "epoch 27: loss=2.039891004562378\n",
      "epoch 28: loss=1.875641107559204\n",
      "epoch 29: loss=1.7469733953475952\n",
      "epoch 30: loss=1.6572821140289307\n",
      "epoch 31: loss=1.5683835744857788\n",
      "epoch 32: loss=1.4640772342681885\n",
      "epoch 33: loss=1.3578482866287231\n",
      "epoch 34: loss=1.2650221586227417\n",
      "epoch 35: loss=1.211316466331482\n",
      "epoch 36: loss=1.167971134185791\n",
      "epoch 37: loss=1.1358312368392944\n",
      "epoch 38: loss=1.1078473329544067\n",
      "epoch 39: loss=1.064207911491394\n",
      "epoch 40: loss=1.0216505527496338\n",
      "epoch 41: loss=0.9912797808647156\n",
      "epoch 42: loss=0.9787756204605103\n",
      "epoch 43: loss=0.975410521030426\n",
      "epoch 44: loss=0.9680582880973816\n",
      "epoch 45: loss=0.9530605673789978\n",
      "epoch 46: loss=0.9374216794967651\n",
      "epoch 47: loss=0.9293832182884216\n",
      "epoch 48: loss=0.9243761301040649\n",
      "epoch 49: loss=0.9219339489936829\n",
      "epoch 50: loss=0.9170620441436768\n",
      "epoch 51: loss=0.912409782409668\n",
      "epoch 52: loss=0.9072994589805603\n",
      "epoch 53: loss=0.9034380316734314\n",
      "epoch 54: loss=0.9001108407974243\n",
      "epoch 55: loss=0.8970096111297607\n",
      "epoch 56: loss=0.8955538272857666\n",
      "epoch 57: loss=0.8951252698898315\n",
      "epoch 58: loss=0.8916664719581604\n",
      "epoch 59: loss=0.8898414373397827\n",
      "epoch 60: loss=0.8882067799568176\n",
      "epoch 61: loss=0.8863511085510254\n",
      "epoch 62: loss=0.8856185674667358\n",
      "epoch 63: loss=0.8837398290634155\n",
      "epoch 64: loss=0.883951723575592\n",
      "epoch 65: loss=0.8819820284843445\n",
      "epoch 66: loss=0.8795629739761353\n",
      "epoch 67: loss=0.8804751634597778\n",
      "epoch 68: loss=0.8804629445075989\n",
      "epoch 69: loss=0.8787851929664612\n",
      "epoch 70: loss=0.8782272934913635\n",
      "epoch 71: loss=0.8757669925689697\n",
      "epoch 72: loss=0.8746861219406128\n",
      "epoch 73: loss=0.87404465675354\n",
      "epoch 74: loss=0.8740029335021973\n",
      "epoch 75: loss=0.8752472996711731\n",
      "epoch 76: loss=0.8732873201370239\n",
      "epoch 77: loss=0.8716298341751099\n",
      "epoch 78: loss=0.8721146583557129\n",
      "epoch 79: loss=0.871027410030365\n",
      "epoch 80: loss=0.8717727661132812\n",
      "epoch 81: loss=0.8708397150039673\n",
      "epoch 82: loss=0.8693506121635437\n",
      "epoch 83: loss=0.8698851466178894\n",
      "epoch 84: loss=0.8686960935592651\n",
      "epoch 85: loss=0.867502748966217\n",
      "epoch 86: loss=0.8675902485847473\n",
      "epoch 87: loss=0.8663924932479858\n",
      "epoch 88: loss=0.8670907616615295\n",
      "epoch 89: loss=0.8662861585617065\n",
      "epoch 90: loss=0.8655164837837219\n",
      "epoch 91: loss=0.8649287819862366\n",
      "epoch 92: loss=0.8645185828208923\n",
      "epoch 93: loss=0.8641546964645386\n",
      "epoch 94: loss=0.8635886311531067\n",
      "epoch 95: loss=0.8642463088035583\n",
      "epoch 96: loss=0.8629093170166016\n",
      "epoch 97: loss=0.8628211617469788\n",
      "epoch 98: loss=0.8604780435562134\n",
      "epoch 99: loss=0.8609765768051147\n",
      "epoch 100: loss=0.8627835512161255\n",
      "epoch 101: loss=0.8609997034072876\n",
      "epoch 102: loss=0.8599506616592407\n",
      "epoch 103: loss=0.8606081604957581\n",
      "epoch 104: loss=0.8598975539207458\n",
      "epoch 105: loss=0.8593161702156067\n",
      "epoch 106: loss=0.8579491376876831\n",
      "epoch 107: loss=0.8585076928138733\n",
      "epoch 108: loss=0.8575080037117004\n",
      "epoch 109: loss=0.8573610782623291\n",
      "epoch 110: loss=0.8564562201499939\n",
      "epoch 111: loss=0.8555330038070679\n",
      "epoch 112: loss=0.8542229533195496\n",
      "epoch 113: loss=0.8544816970825195\n",
      "epoch 114: loss=0.8546258807182312\n",
      "epoch 115: loss=0.8540141582489014\n",
      "epoch 116: loss=0.8540453910827637\n",
      "epoch 117: loss=0.8549009561538696\n",
      "epoch 118: loss=0.8534463047981262\n",
      "epoch 119: loss=0.852247416973114\n",
      "epoch 120: loss=0.852356493473053\n",
      "epoch 121: loss=0.8526087403297424\n",
      "epoch 122: loss=0.8515813946723938\n",
      "epoch 123: loss=0.8530781865119934\n",
      "epoch 124: loss=0.8515092730522156\n",
      "epoch 125: loss=0.8513544797897339\n",
      "epoch 126: loss=0.8500993251800537\n",
      "epoch 127: loss=0.849831759929657\n",
      "epoch 128: loss=0.8510253429412842\n",
      "epoch 129: loss=0.848651111125946\n",
      "epoch 130: loss=0.8493778109550476\n",
      "epoch 131: loss=0.8501923084259033\n",
      "epoch 132: loss=0.8496553301811218\n",
      "epoch 133: loss=0.8486452698707581\n",
      "epoch 134: loss=0.8489457368850708\n",
      "epoch 135: loss=0.8475839495658875\n",
      "epoch 136: loss=0.8465752005577087\n",
      "epoch 137: loss=0.8461830019950867\n",
      "epoch 138: loss=0.8466870188713074\n",
      "epoch 139: loss=0.8478954434394836\n",
      "epoch 140: loss=0.846679151058197\n",
      "epoch 141: loss=0.845619261264801\n",
      "epoch 142: loss=0.8448643684387207\n",
      "epoch 143: loss=0.8463794589042664\n",
      "epoch 144: loss=0.8452021479606628\n",
      "epoch 145: loss=0.8455368876457214\n",
      "epoch 146: loss=0.8436499834060669\n",
      "epoch 147: loss=0.8435612320899963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 148: loss=0.8439738750457764\n",
      "epoch 149: loss=0.8437774777412415\n",
      "epoch 150: loss=0.8438276052474976\n",
      "epoch 151: loss=0.8430336117744446\n",
      "epoch 152: loss=0.8428494930267334\n",
      "epoch 153: loss=0.8441103100776672\n",
      "epoch 154: loss=0.8429052233695984\n",
      "epoch 155: loss=0.8436486721038818\n",
      "epoch 156: loss=0.8427429795265198\n",
      "epoch 157: loss=0.8434165120124817\n",
      "epoch 158: loss=0.8429301977157593\n",
      "epoch 159: loss=0.8425595164299011\n",
      "epoch 160: loss=0.8419880270957947\n",
      "epoch 161: loss=0.8420171141624451\n",
      "epoch 162: loss=0.8426094651222229\n",
      "epoch 163: loss=0.8400880098342896\n",
      "epoch 164: loss=0.841805100440979\n",
      "epoch 165: loss=0.8399391770362854\n",
      "epoch 166: loss=0.8410397171974182\n",
      "epoch 167: loss=0.8408209681510925\n",
      "epoch 168: loss=0.8400357365608215\n",
      "epoch 169: loss=0.8385016322135925\n",
      "epoch 170: loss=0.8391072750091553\n",
      "epoch 171: loss=0.8393403887748718\n",
      "epoch 172: loss=0.8405753970146179\n",
      "epoch 173: loss=0.8395401239395142\n",
      "epoch 174: loss=0.8375493884086609\n",
      "epoch 175: loss=0.8395301103591919\n",
      "epoch 176: loss=0.8398131132125854\n",
      "epoch 177: loss=0.837727427482605\n",
      "epoch 178: loss=0.8399363160133362\n",
      "epoch 179: loss=0.839127779006958\n",
      "epoch 180: loss=0.8382034301757812\n",
      "epoch 181: loss=0.8376520872116089\n",
      "epoch 182: loss=0.8373090624809265\n",
      "epoch 183: loss=0.837032675743103\n",
      "epoch 184: loss=0.8380137085914612\n",
      "epoch 185: loss=0.8372530341148376\n",
      "epoch 186: loss=0.8371320366859436\n",
      "epoch 187: loss=0.8361963033676147\n",
      "epoch 188: loss=0.8362043499946594\n",
      "epoch 189: loss=0.8348732590675354\n",
      "epoch 190: loss=0.8362974524497986\n",
      "epoch 191: loss=0.8360720276832581\n",
      "epoch 192: loss=0.836647093296051\n",
      "epoch 193: loss=0.8363812565803528\n",
      "epoch 194: loss=0.8371453881263733\n",
      "epoch 195: loss=0.8367959260940552\n",
      "epoch 196: loss=0.8354530930519104\n",
      "epoch 197: loss=0.8360569477081299\n",
      "epoch 198: loss=0.8338747620582581\n",
      "epoch 199: loss=0.8345038294792175\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=13.938013076782227\n",
      "epoch 1: loss=13.561850547790527\n",
      "epoch 2: loss=13.099440574645996\n",
      "epoch 3: loss=12.409926414489746\n",
      "epoch 4: loss=11.596918106079102\n",
      "epoch 5: loss=10.468243598937988\n",
      "epoch 6: loss=9.42131233215332\n",
      "epoch 7: loss=8.8606595993042\n",
      "epoch 8: loss=8.743138313293457\n",
      "epoch 9: loss=8.80875015258789\n",
      "epoch 10: loss=8.456376075744629\n",
      "epoch 11: loss=7.702833652496338\n",
      "epoch 12: loss=6.8999128341674805\n",
      "epoch 13: loss=6.060711860656738\n",
      "epoch 14: loss=5.361624717712402\n",
      "epoch 15: loss=4.781922817230225\n",
      "epoch 16: loss=4.390398025512695\n",
      "epoch 17: loss=4.141396999359131\n",
      "epoch 18: loss=3.914053440093994\n",
      "epoch 19: loss=3.7684998512268066\n",
      "epoch 20: loss=3.5152902603149414\n",
      "epoch 21: loss=3.2580783367156982\n",
      "epoch 22: loss=2.948357105255127\n",
      "epoch 23: loss=2.6434714794158936\n",
      "epoch 24: loss=2.3643875122070312\n",
      "epoch 25: loss=2.155609130859375\n",
      "epoch 26: loss=1.9775751829147339\n",
      "epoch 27: loss=1.8149757385253906\n",
      "epoch 28: loss=1.6942310333251953\n",
      "epoch 29: loss=1.572662353515625\n",
      "epoch 30: loss=1.459022045135498\n",
      "epoch 31: loss=1.3632516860961914\n",
      "epoch 32: loss=1.2732406854629517\n",
      "epoch 33: loss=1.2156916856765747\n",
      "epoch 34: loss=1.1657921075820923\n",
      "epoch 35: loss=1.1283034086227417\n",
      "epoch 36: loss=1.0803451538085938\n",
      "epoch 37: loss=1.0398359298706055\n",
      "epoch 38: loss=1.0128470659255981\n",
      "epoch 39: loss=0.9862003326416016\n",
      "epoch 40: loss=0.9765715599060059\n",
      "epoch 41: loss=0.9740162491798401\n",
      "epoch 42: loss=0.9692063331604004\n",
      "epoch 43: loss=0.9588397145271301\n",
      "epoch 44: loss=0.9458022117614746\n",
      "epoch 45: loss=0.9354860186576843\n",
      "epoch 46: loss=0.9277486801147461\n",
      "epoch 47: loss=0.92551189661026\n",
      "epoch 48: loss=0.9195262789726257\n",
      "epoch 49: loss=0.9193533062934875\n",
      "epoch 50: loss=0.9148122072219849\n",
      "epoch 51: loss=0.9104421734809875\n",
      "epoch 52: loss=0.906031608581543\n",
      "epoch 53: loss=0.9026815891265869\n",
      "epoch 54: loss=0.8990792036056519\n",
      "epoch 55: loss=0.8984739184379578\n",
      "epoch 56: loss=0.896286129951477\n",
      "epoch 57: loss=0.8974077701568604\n",
      "epoch 58: loss=0.8957065343856812\n",
      "epoch 59: loss=0.8915833830833435\n",
      "epoch 60: loss=0.8893272876739502\n",
      "epoch 61: loss=0.8906742334365845\n",
      "epoch 62: loss=0.8901379108428955\n",
      "epoch 63: loss=0.890849232673645\n",
      "epoch 64: loss=0.8901058435440063\n",
      "epoch 65: loss=0.8863101005554199\n",
      "epoch 66: loss=0.8856695890426636\n",
      "epoch 67: loss=0.8854860067367554\n",
      "epoch 68: loss=0.883307158946991\n",
      "epoch 69: loss=0.8841274976730347\n",
      "epoch 70: loss=0.8844987154006958\n",
      "epoch 71: loss=0.8814547061920166\n",
      "epoch 72: loss=0.8823545575141907\n",
      "epoch 73: loss=0.8815313577651978\n",
      "epoch 74: loss=0.8808904886245728\n",
      "epoch 75: loss=0.8807222843170166\n",
      "epoch 76: loss=0.8797447085380554\n",
      "epoch 77: loss=0.8777166604995728\n",
      "epoch 78: loss=0.8788071274757385\n",
      "epoch 79: loss=0.8796242475509644\n",
      "epoch 80: loss=0.8769661784172058\n",
      "epoch 81: loss=0.8764906525611877\n",
      "epoch 82: loss=0.8782432079315186\n",
      "epoch 83: loss=0.8773679137229919\n",
      "epoch 84: loss=0.8746688365936279\n",
      "epoch 85: loss=0.8756418228149414\n",
      "epoch 86: loss=0.8759143352508545\n",
      "epoch 87: loss=0.8750582933425903\n",
      "epoch 88: loss=0.8745056986808777\n",
      "epoch 89: loss=0.8754816055297852\n",
      "epoch 90: loss=0.8740792274475098\n",
      "epoch 91: loss=0.8730787634849548\n",
      "epoch 92: loss=0.8734702467918396\n",
      "epoch 93: loss=0.8722036480903625\n",
      "epoch 94: loss=0.8734261989593506\n",
      "epoch 95: loss=0.8746815919876099\n",
      "epoch 96: loss=0.8723472356796265\n",
      "epoch 97: loss=0.8717057108879089\n",
      "epoch 98: loss=0.8712764978408813\n",
      "epoch 99: loss=0.8721005916595459\n",
      "epoch 100: loss=0.8716484904289246\n",
      "epoch 101: loss=0.8708298206329346\n",
      "epoch 102: loss=0.8708294630050659\n",
      "epoch 103: loss=0.8707438111305237\n",
      "epoch 104: loss=0.871403694152832\n",
      "epoch 105: loss=0.8692894577980042\n",
      "epoch 106: loss=0.8690711855888367\n",
      "epoch 107: loss=0.867955207824707\n",
      "epoch 108: loss=0.8709195852279663\n",
      "epoch 109: loss=0.8693491220474243\n",
      "epoch 110: loss=0.8679563999176025\n",
      "epoch 111: loss=0.8688675761222839\n",
      "epoch 112: loss=0.868722140789032\n",
      "epoch 113: loss=0.866546094417572\n",
      "epoch 114: loss=0.8670954704284668\n",
      "epoch 115: loss=0.8685891032218933\n",
      "epoch 116: loss=0.8676223754882812\n",
      "epoch 117: loss=0.8649186491966248\n",
      "epoch 118: loss=0.8649356961250305\n",
      "epoch 119: loss=0.8654815554618835\n",
      "epoch 120: loss=0.8640440106391907\n",
      "epoch 121: loss=0.8636782169342041\n",
      "epoch 122: loss=0.8637905716896057\n",
      "epoch 123: loss=0.86417555809021\n",
      "epoch 124: loss=0.8636325597763062\n",
      "epoch 125: loss=0.8626633286476135\n",
      "epoch 126: loss=0.8619245290756226\n",
      "epoch 127: loss=0.8640726208686829\n",
      "epoch 128: loss=0.8615086674690247\n",
      "epoch 129: loss=0.8599321246147156\n",
      "epoch 130: loss=0.8623011708259583\n",
      "epoch 131: loss=0.8594508171081543\n",
      "epoch 132: loss=0.8600862622261047\n",
      "epoch 133: loss=0.8603788614273071\n",
      "epoch 134: loss=0.8596084713935852\n",
      "epoch 135: loss=0.8575059771537781\n",
      "epoch 136: loss=0.8569753170013428\n",
      "epoch 137: loss=0.856998860836029\n",
      "epoch 138: loss=0.856257975101471\n",
      "epoch 139: loss=0.8567362427711487\n",
      "epoch 140: loss=0.8544326424598694\n",
      "epoch 141: loss=0.852946400642395\n",
      "epoch 142: loss=0.8543061017990112\n",
      "epoch 143: loss=0.854077160358429\n",
      "epoch 144: loss=0.8518994450569153\n",
      "epoch 145: loss=0.8552201986312866\n",
      "epoch 146: loss=0.8524014353752136\n",
      "epoch 147: loss=0.8520547747612\n",
      "epoch 148: loss=0.8499375581741333\n",
      "epoch 149: loss=0.8512407541275024\n",
      "epoch 150: loss=0.8513367176055908\n",
      "epoch 151: loss=0.8494229316711426\n",
      "epoch 152: loss=0.8470468521118164\n",
      "epoch 153: loss=0.8473027348518372\n",
      "epoch 154: loss=0.8479703068733215\n",
      "epoch 155: loss=0.84726881980896\n",
      "epoch 156: loss=0.8465919494628906\n",
      "epoch 157: loss=0.8446985483169556\n",
      "epoch 158: loss=0.8450363874435425\n",
      "epoch 159: loss=0.8449893593788147\n",
      "epoch 160: loss=0.8435512185096741\n",
      "epoch 161: loss=0.8437039852142334\n",
      "epoch 162: loss=0.8427705764770508\n",
      "epoch 163: loss=0.841011643409729\n",
      "epoch 164: loss=0.8411849737167358\n",
      "epoch 165: loss=0.8405956029891968\n",
      "epoch 166: loss=0.842477023601532\n",
      "epoch 167: loss=0.8414266109466553\n",
      "epoch 168: loss=0.8395470976829529\n",
      "epoch 169: loss=0.8388577103614807\n",
      "epoch 170: loss=0.8380129337310791\n",
      "epoch 171: loss=0.8393911123275757\n",
      "epoch 172: loss=0.8371042013168335\n",
      "epoch 173: loss=0.8392869830131531\n",
      "epoch 174: loss=0.8360636830329895\n",
      "epoch 175: loss=0.8352164030075073\n",
      "epoch 176: loss=0.8363133668899536\n",
      "epoch 177: loss=0.8356572985649109\n",
      "epoch 178: loss=0.8350549936294556\n",
      "epoch 179: loss=0.8343517780303955\n",
      "epoch 180: loss=0.832821249961853\n",
      "epoch 181: loss=0.8335297107696533\n",
      "epoch 182: loss=0.8348328471183777\n",
      "epoch 183: loss=0.833537220954895\n",
      "epoch 184: loss=0.8343720436096191\n",
      "epoch 185: loss=0.8326064348220825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 186: loss=0.8309985399246216\n",
      "epoch 187: loss=0.8321555852890015\n",
      "epoch 188: loss=0.8329688906669617\n",
      "epoch 189: loss=0.8312157392501831\n",
      "epoch 190: loss=0.8321583867073059\n",
      "epoch 191: loss=0.8315711617469788\n",
      "epoch 192: loss=0.8306806087493896\n",
      "epoch 193: loss=0.8318169713020325\n",
      "epoch 194: loss=0.8313502073287964\n",
      "epoch 195: loss=0.8300181031227112\n",
      "epoch 196: loss=0.8311289548873901\n",
      "epoch 197: loss=0.8297457098960876\n",
      "epoch 198: loss=0.8295686841011047\n",
      "epoch 199: loss=0.8284214735031128\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=13.923578262329102\n",
      "epoch 1: loss=13.535249710083008\n",
      "epoch 2: loss=12.905999183654785\n",
      "epoch 3: loss=11.89925479888916\n",
      "epoch 4: loss=10.604826927185059\n",
      "epoch 5: loss=9.14210033416748\n",
      "epoch 6: loss=8.734512329101562\n",
      "epoch 7: loss=9.207103729248047\n",
      "epoch 8: loss=9.112265586853027\n",
      "epoch 9: loss=8.442790031433105\n",
      "epoch 10: loss=7.362751483917236\n",
      "epoch 11: loss=6.423101425170898\n",
      "epoch 12: loss=5.566478252410889\n",
      "epoch 13: loss=4.9913716316223145\n",
      "epoch 14: loss=4.618778228759766\n",
      "epoch 15: loss=4.388713359832764\n",
      "epoch 16: loss=4.1177473068237305\n",
      "epoch 17: loss=3.9261481761932373\n",
      "epoch 18: loss=3.68312931060791\n",
      "epoch 19: loss=3.424158811569214\n",
      "epoch 20: loss=3.2284882068634033\n",
      "epoch 21: loss=2.998814105987549\n",
      "epoch 22: loss=2.718740224838257\n",
      "epoch 23: loss=2.4520580768585205\n",
      "epoch 24: loss=2.1935927867889404\n",
      "epoch 25: loss=1.9960225820541382\n",
      "epoch 26: loss=1.8254457712173462\n",
      "epoch 27: loss=1.6942756175994873\n",
      "epoch 28: loss=1.6011470556259155\n",
      "epoch 29: loss=1.5182101726531982\n",
      "epoch 30: loss=1.448205828666687\n",
      "epoch 31: loss=1.3500936031341553\n",
      "epoch 32: loss=1.2737737894058228\n",
      "epoch 33: loss=1.2036633491516113\n",
      "epoch 34: loss=1.1694186925888062\n",
      "epoch 35: loss=1.1328049898147583\n",
      "epoch 36: loss=1.1236575841903687\n",
      "epoch 37: loss=1.0933852195739746\n",
      "epoch 38: loss=1.0657795667648315\n",
      "epoch 39: loss=1.03337824344635\n",
      "epoch 40: loss=1.0181124210357666\n",
      "epoch 41: loss=1.0039857625961304\n",
      "epoch 42: loss=1.0032589435577393\n",
      "epoch 43: loss=0.9913102388381958\n",
      "epoch 44: loss=0.9813804626464844\n",
      "epoch 45: loss=0.9663734436035156\n",
      "epoch 46: loss=0.9650380611419678\n",
      "epoch 47: loss=0.9605381488800049\n",
      "epoch 48: loss=0.9607634544372559\n",
      "epoch 49: loss=0.9543315172195435\n",
      "epoch 50: loss=0.9478433728218079\n",
      "epoch 51: loss=0.9449225664138794\n",
      "epoch 52: loss=0.9422435760498047\n",
      "epoch 53: loss=0.9419134259223938\n",
      "epoch 54: loss=0.9390546083450317\n",
      "epoch 55: loss=0.9353042840957642\n",
      "epoch 56: loss=0.9327342510223389\n",
      "epoch 57: loss=0.9319430589675903\n",
      "epoch 58: loss=0.9304519295692444\n",
      "epoch 59: loss=0.9278932213783264\n",
      "epoch 60: loss=0.9281835556030273\n",
      "epoch 61: loss=0.9258682131767273\n",
      "epoch 62: loss=0.9243291020393372\n",
      "epoch 63: loss=0.925609827041626\n",
      "epoch 64: loss=0.9242725968360901\n",
      "epoch 65: loss=0.9218099117279053\n",
      "epoch 66: loss=0.9221501350402832\n",
      "epoch 67: loss=0.9225196242332458\n",
      "epoch 68: loss=0.9227177500724792\n",
      "epoch 69: loss=0.917256772518158\n",
      "epoch 70: loss=0.9188216328620911\n",
      "epoch 71: loss=0.9203705787658691\n",
      "epoch 72: loss=0.9190242886543274\n",
      "epoch 73: loss=0.9178534746170044\n",
      "epoch 74: loss=0.9185256958007812\n",
      "epoch 75: loss=0.9183022975921631\n",
      "epoch 76: loss=0.9179753065109253\n",
      "epoch 77: loss=0.9158865809440613\n",
      "epoch 78: loss=0.9148101806640625\n",
      "epoch 79: loss=0.9173213839530945\n",
      "epoch 80: loss=0.9150705933570862\n",
      "epoch 81: loss=0.9139718413352966\n",
      "epoch 82: loss=0.9134426116943359\n",
      "epoch 83: loss=0.9144317507743835\n",
      "epoch 84: loss=0.914920449256897\n",
      "epoch 85: loss=0.9142168760299683\n",
      "epoch 86: loss=0.9129337668418884\n",
      "epoch 87: loss=0.9112510681152344\n",
      "epoch 88: loss=0.914475679397583\n",
      "epoch 89: loss=0.9134050607681274\n",
      "epoch 90: loss=0.9117332100868225\n",
      "epoch 91: loss=0.910407543182373\n",
      "epoch 92: loss=0.9122848510742188\n",
      "epoch 93: loss=0.9107219576835632\n",
      "epoch 94: loss=0.9114751219749451\n",
      "epoch 95: loss=0.9094628095626831\n",
      "epoch 96: loss=0.9092088341712952\n",
      "epoch 97: loss=0.910167932510376\n",
      "epoch 98: loss=0.908911406993866\n",
      "epoch 99: loss=0.9113316535949707\n",
      "epoch 100: loss=0.9078049063682556\n",
      "epoch 101: loss=0.9090076684951782\n",
      "epoch 102: loss=0.9087041616439819\n",
      "epoch 103: loss=0.9097266793251038\n",
      "epoch 104: loss=0.9092257022857666\n",
      "epoch 105: loss=0.9089590311050415\n",
      "epoch 106: loss=0.9071792960166931\n",
      "epoch 107: loss=0.9067111015319824\n",
      "epoch 108: loss=0.9080870151519775\n",
      "epoch 109: loss=0.9086828827857971\n",
      "epoch 110: loss=0.9067513942718506\n",
      "epoch 111: loss=0.907204806804657\n",
      "epoch 112: loss=0.9071622490882874\n",
      "epoch 113: loss=0.9072737097740173\n",
      "epoch 114: loss=0.907078742980957\n",
      "epoch 115: loss=0.9085192680358887\n",
      "epoch 116: loss=0.907706618309021\n",
      "epoch 117: loss=0.9063724875450134\n",
      "epoch 118: loss=0.9044618606567383\n",
      "epoch 119: loss=0.9052595496177673\n",
      "epoch 120: loss=0.9085702300071716\n",
      "epoch 121: loss=0.9031683802604675\n",
      "epoch 122: loss=0.9068796634674072\n",
      "epoch 123: loss=0.9027469754219055\n",
      "epoch 124: loss=0.9041371941566467\n",
      "epoch 125: loss=0.9052526950836182\n",
      "epoch 126: loss=0.9021546840667725\n",
      "epoch 127: loss=0.9023432731628418\n",
      "epoch 128: loss=0.9019485116004944\n",
      "epoch 129: loss=0.9023050665855408\n",
      "epoch 130: loss=0.9009110927581787\n",
      "epoch 131: loss=0.9017046689987183\n",
      "epoch 132: loss=0.9033862352371216\n",
      "epoch 133: loss=0.9007941484451294\n",
      "epoch 134: loss=0.8993939161300659\n",
      "epoch 135: loss=0.8983367085456848\n",
      "epoch 136: loss=0.8996593356132507\n",
      "epoch 137: loss=0.8989915251731873\n",
      "epoch 138: loss=0.8988794684410095\n",
      "epoch 139: loss=0.9000117778778076\n",
      "epoch 140: loss=0.8988819122314453\n",
      "epoch 141: loss=0.898689866065979\n",
      "epoch 142: loss=0.8972088694572449\n",
      "epoch 143: loss=0.8955315947532654\n",
      "epoch 144: loss=0.8987833857536316\n",
      "epoch 145: loss=0.8953086733818054\n",
      "epoch 146: loss=0.8995916247367859\n",
      "epoch 147: loss=0.897361159324646\n",
      "epoch 148: loss=0.89532071352005\n",
      "epoch 149: loss=0.8955171704292297\n",
      "epoch 150: loss=0.8944811820983887\n",
      "epoch 151: loss=0.8961345553398132\n",
      "epoch 152: loss=0.8942590951919556\n",
      "epoch 153: loss=0.895362377166748\n",
      "epoch 154: loss=0.8943766355514526\n",
      "epoch 155: loss=0.8946253061294556\n",
      "epoch 156: loss=0.89609694480896\n",
      "epoch 157: loss=0.8931831121444702\n",
      "epoch 158: loss=0.8936212062835693\n",
      "epoch 159: loss=0.8936395049095154\n",
      "epoch 160: loss=0.8917679190635681\n",
      "epoch 161: loss=0.8927059173583984\n",
      "epoch 162: loss=0.8906203508377075\n",
      "epoch 163: loss=0.8943684697151184\n",
      "epoch 164: loss=0.8913447856903076\n",
      "epoch 165: loss=0.8922658562660217\n",
      "epoch 166: loss=0.8917637467384338\n",
      "epoch 167: loss=0.892298698425293\n",
      "epoch 168: loss=0.8906747698783875\n",
      "epoch 169: loss=0.8917258977890015\n",
      "epoch 170: loss=0.8902397155761719\n",
      "epoch 171: loss=0.8916037082672119\n",
      "epoch 172: loss=0.8891428112983704\n",
      "epoch 173: loss=0.8902736902236938\n",
      "epoch 174: loss=0.8894915580749512\n",
      "epoch 175: loss=0.8909302949905396\n",
      "epoch 176: loss=0.8898923397064209\n",
      "epoch 177: loss=0.8861819505691528\n",
      "epoch 178: loss=0.8857053518295288\n",
      "epoch 179: loss=0.8893557786941528\n",
      "epoch 180: loss=0.887434720993042\n",
      "epoch 181: loss=0.8876014351844788\n",
      "epoch 182: loss=0.8882880210876465\n",
      "epoch 183: loss=0.8884720802307129\n",
      "epoch 184: loss=0.885796844959259\n",
      "epoch 185: loss=0.8876564502716064\n",
      "epoch 186: loss=0.8854074478149414\n",
      "epoch 187: loss=0.8873165845870972\n",
      "epoch 188: loss=0.8866147994995117\n",
      "epoch 189: loss=0.8855441212654114\n",
      "epoch 190: loss=0.8853588104248047\n",
      "epoch 191: loss=0.8858469128608704\n",
      "epoch 192: loss=0.8841066360473633\n",
      "epoch 193: loss=0.8862752318382263\n",
      "epoch 194: loss=0.8832118511199951\n",
      "epoch 195: loss=0.8859736919403076\n",
      "epoch 196: loss=0.8840939402580261\n",
      "epoch 197: loss=0.8835439085960388\n",
      "epoch 198: loss=0.8828444480895996\n",
      "epoch 199: loss=0.884100079536438\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=13.920207977294922\n",
      "epoch 1: loss=13.609903335571289\n",
      "epoch 2: loss=12.959925651550293\n",
      "epoch 3: loss=12.130009651184082\n",
      "epoch 4: loss=10.99555492401123\n",
      "epoch 5: loss=9.660921096801758\n",
      "epoch 6: loss=8.979459762573242\n",
      "epoch 7: loss=9.288816452026367\n",
      "epoch 8: loss=9.458226203918457\n",
      "epoch 9: loss=8.899489402770996\n",
      "epoch 10: loss=7.855453014373779\n",
      "epoch 11: loss=6.822528839111328\n",
      "epoch 12: loss=5.805449962615967\n",
      "epoch 13: loss=5.134689807891846\n",
      "epoch 14: loss=4.663158893585205\n",
      "epoch 15: loss=4.42634916305542\n",
      "epoch 16: loss=4.188871383666992\n",
      "epoch 17: loss=3.9621741771698\n",
      "epoch 18: loss=3.650428295135498\n",
      "epoch 19: loss=3.383030891418457\n",
      "epoch 20: loss=3.176313638687134\n",
      "epoch 21: loss=2.9368979930877686\n",
      "epoch 22: loss=2.6930503845214844\n",
      "epoch 23: loss=2.429805278778076\n",
      "epoch 24: loss=2.2013938426971436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: loss=1.9801768064498901\n",
      "epoch 26: loss=1.8107582330703735\n",
      "epoch 27: loss=1.7088490724563599\n",
      "epoch 28: loss=1.6251335144042969\n",
      "epoch 29: loss=1.5368728637695312\n",
      "epoch 30: loss=1.475776195526123\n",
      "epoch 31: loss=1.3861298561096191\n",
      "epoch 32: loss=1.3056995868682861\n",
      "epoch 33: loss=1.247663974761963\n",
      "epoch 34: loss=1.2138698101043701\n",
      "epoch 35: loss=1.191258192062378\n",
      "epoch 36: loss=1.1717044115066528\n",
      "epoch 37: loss=1.1415860652923584\n",
      "epoch 38: loss=1.1038564443588257\n",
      "epoch 39: loss=1.078015685081482\n",
      "epoch 40: loss=1.0600883960723877\n",
      "epoch 41: loss=1.046674132347107\n",
      "epoch 42: loss=1.048437237739563\n",
      "epoch 43: loss=1.0368599891662598\n",
      "epoch 44: loss=1.021203875541687\n",
      "epoch 45: loss=1.0125818252563477\n",
      "epoch 46: loss=1.006502628326416\n",
      "epoch 47: loss=1.0022294521331787\n",
      "epoch 48: loss=0.9971178770065308\n",
      "epoch 49: loss=0.9953593015670776\n",
      "epoch 50: loss=0.9911788105964661\n",
      "epoch 51: loss=0.9834120273590088\n",
      "epoch 52: loss=0.9821004867553711\n",
      "epoch 53: loss=0.9793100357055664\n",
      "epoch 54: loss=0.9753661155700684\n",
      "epoch 55: loss=0.9737258553504944\n",
      "epoch 56: loss=0.9714989066123962\n",
      "epoch 57: loss=0.9694199562072754\n",
      "epoch 58: loss=0.966670036315918\n",
      "epoch 59: loss=0.967828631401062\n",
      "epoch 60: loss=0.9654009938240051\n",
      "epoch 61: loss=0.9655423164367676\n",
      "epoch 62: loss=0.9654129147529602\n",
      "epoch 63: loss=0.9631484746932983\n",
      "epoch 64: loss=0.9620358943939209\n",
      "epoch 65: loss=0.9603118896484375\n",
      "epoch 66: loss=0.9593766927719116\n",
      "epoch 67: loss=0.9579039216041565\n",
      "epoch 68: loss=0.9545904397964478\n",
      "epoch 69: loss=0.957737386226654\n",
      "epoch 70: loss=0.9560659527778625\n",
      "epoch 71: loss=0.9554648995399475\n",
      "epoch 72: loss=0.9548681974411011\n",
      "epoch 73: loss=0.9539520144462585\n",
      "epoch 74: loss=0.9544497132301331\n",
      "epoch 75: loss=0.9522020816802979\n",
      "epoch 76: loss=0.9503548741340637\n",
      "epoch 77: loss=0.9524480104446411\n",
      "epoch 78: loss=0.9528802633285522\n",
      "epoch 79: loss=0.9535077214241028\n",
      "epoch 80: loss=0.95380038022995\n",
      "epoch 81: loss=0.9512849450111389\n",
      "epoch 82: loss=0.9478731751441956\n",
      "epoch 83: loss=0.95023113489151\n",
      "epoch 84: loss=0.9486979246139526\n",
      "epoch 85: loss=0.9469035267829895\n",
      "epoch 86: loss=0.9519664645195007\n",
      "epoch 87: loss=0.9500449299812317\n",
      "epoch 88: loss=0.9478651881217957\n",
      "epoch 89: loss=0.9472367167472839\n",
      "epoch 90: loss=0.9461488127708435\n",
      "epoch 91: loss=0.9465231895446777\n",
      "epoch 92: loss=0.9462405443191528\n",
      "epoch 93: loss=0.945004403591156\n",
      "epoch 94: loss=0.9452354907989502\n",
      "epoch 95: loss=0.9464489817619324\n",
      "epoch 96: loss=0.9457228779792786\n",
      "epoch 97: loss=0.9445502161979675\n",
      "epoch 98: loss=0.9456878900527954\n",
      "epoch 99: loss=0.9460175037384033\n",
      "epoch 100: loss=0.9446399807929993\n",
      "epoch 101: loss=0.9425036907196045\n",
      "epoch 102: loss=0.9446283578872681\n",
      "epoch 103: loss=0.9430757761001587\n",
      "epoch 104: loss=0.9428061246871948\n",
      "epoch 105: loss=0.9433963298797607\n",
      "epoch 106: loss=0.9416436553001404\n",
      "epoch 107: loss=0.9411409497261047\n",
      "epoch 108: loss=0.9410853981971741\n",
      "epoch 109: loss=0.9403510093688965\n",
      "epoch 110: loss=0.9394010305404663\n",
      "epoch 111: loss=0.9393234848976135\n",
      "epoch 112: loss=0.9397113919258118\n",
      "epoch 113: loss=0.9398002624511719\n",
      "epoch 114: loss=0.9381514191627502\n",
      "epoch 115: loss=0.9381288290023804\n",
      "epoch 116: loss=0.9369449019432068\n",
      "epoch 117: loss=0.9378827810287476\n",
      "epoch 118: loss=0.9378648996353149\n",
      "epoch 119: loss=0.9399976134300232\n",
      "epoch 120: loss=0.9368640184402466\n",
      "epoch 121: loss=0.9367766380310059\n",
      "epoch 122: loss=0.9364179372787476\n",
      "epoch 123: loss=0.9379547238349915\n",
      "epoch 124: loss=0.9347599744796753\n",
      "epoch 125: loss=0.9352536201477051\n",
      "epoch 126: loss=0.9336698651313782\n",
      "epoch 127: loss=0.9367316961288452\n",
      "epoch 128: loss=0.9341962933540344\n",
      "epoch 129: loss=0.9341042041778564\n",
      "epoch 130: loss=0.9322651028633118\n",
      "epoch 131: loss=0.9335329532623291\n",
      "epoch 132: loss=0.9333133101463318\n",
      "epoch 133: loss=0.9325037598609924\n",
      "epoch 134: loss=0.9311822056770325\n",
      "epoch 135: loss=0.9310425519943237\n",
      "epoch 136: loss=0.9315271377563477\n",
      "epoch 137: loss=0.9316253066062927\n",
      "epoch 138: loss=0.9311680793762207\n",
      "epoch 139: loss=0.9299504160881042\n",
      "epoch 140: loss=0.9306035041809082\n",
      "epoch 141: loss=0.9289103150367737\n",
      "epoch 142: loss=0.9310389161109924\n",
      "epoch 143: loss=0.9285660982131958\n",
      "epoch 144: loss=0.9290273189544678\n",
      "epoch 145: loss=0.9308878183364868\n",
      "epoch 146: loss=0.9285776615142822\n",
      "epoch 147: loss=0.9290806651115417\n",
      "epoch 148: loss=0.9290388822555542\n",
      "epoch 149: loss=0.9292855858802795\n",
      "epoch 150: loss=0.9254937171936035\n",
      "epoch 151: loss=0.9317892789840698\n",
      "epoch 152: loss=0.9277589917182922\n",
      "epoch 153: loss=0.9259809851646423\n",
      "epoch 154: loss=0.9271490573883057\n",
      "epoch 155: loss=0.9271879196166992\n",
      "epoch 156: loss=0.9260067343711853\n",
      "epoch 157: loss=0.9273521900177002\n",
      "epoch 158: loss=0.9269453287124634\n",
      "epoch 159: loss=0.9263805747032166\n",
      "epoch 160: loss=0.9245008230209351\n",
      "epoch 161: loss=0.9237298369407654\n",
      "epoch 162: loss=0.9226376414299011\n",
      "epoch 163: loss=0.9250631928443909\n",
      "epoch 164: loss=0.923500657081604\n",
      "epoch 165: loss=0.9246132373809814\n",
      "epoch 166: loss=0.9228945970535278\n",
      "epoch 167: loss=0.9228252172470093\n",
      "epoch 168: loss=0.9217694401741028\n",
      "epoch 169: loss=0.9212198853492737\n",
      "epoch 170: loss=0.9238258004188538\n",
      "epoch 171: loss=0.9220629334449768\n",
      "epoch 172: loss=0.9209396243095398\n",
      "epoch 173: loss=0.922042965888977\n",
      "epoch 174: loss=0.9220731258392334\n",
      "epoch 175: loss=0.9209779500961304\n",
      "epoch 176: loss=0.9212788939476013\n",
      "epoch 177: loss=0.9219819903373718\n",
      "epoch 178: loss=0.9205513596534729\n",
      "epoch 179: loss=0.9223750829696655\n",
      "epoch 180: loss=0.9180637001991272\n",
      "epoch 181: loss=0.9191649556159973\n",
      "epoch 182: loss=0.9200519919395447\n",
      "epoch 183: loss=0.9183820486068726\n",
      "epoch 184: loss=0.9202830195426941\n",
      "epoch 185: loss=0.9203109741210938\n",
      "epoch 186: loss=0.9171651601791382\n",
      "epoch 187: loss=0.9188886880874634\n",
      "epoch 188: loss=0.9172899127006531\n",
      "epoch 189: loss=0.9174422025680542\n",
      "epoch 190: loss=0.9175237417221069\n",
      "epoch 191: loss=0.9180021286010742\n",
      "epoch 192: loss=0.9176287055015564\n",
      "epoch 193: loss=0.9175100922584534\n",
      "epoch 194: loss=0.9149293899536133\n",
      "epoch 195: loss=0.9153827428817749\n",
      "epoch 196: loss=0.9180328845977783\n",
      "epoch 197: loss=0.916588544845581\n",
      "epoch 198: loss=0.9162540435791016\n",
      "epoch 199: loss=0.9165378212928772\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=13.803413391113281\n",
      "epoch 1: loss=13.51111125946045\n",
      "epoch 2: loss=12.909972190856934\n",
      "epoch 3: loss=11.971860885620117\n",
      "epoch 4: loss=10.61352252960205\n",
      "epoch 5: loss=9.314602851867676\n",
      "epoch 6: loss=8.75798225402832\n",
      "epoch 7: loss=8.903654098510742\n",
      "epoch 8: loss=8.974063873291016\n",
      "epoch 9: loss=8.55057430267334\n",
      "epoch 10: loss=7.884763717651367\n",
      "epoch 11: loss=7.0216755867004395\n",
      "epoch 12: loss=6.161678314208984\n",
      "epoch 13: loss=5.5481696128845215\n",
      "epoch 14: loss=5.078131675720215\n",
      "epoch 15: loss=4.682572841644287\n",
      "epoch 16: loss=4.408263206481934\n",
      "epoch 17: loss=4.156729698181152\n",
      "epoch 18: loss=3.9361212253570557\n",
      "epoch 19: loss=3.69927978515625\n",
      "epoch 20: loss=3.4714810848236084\n",
      "epoch 21: loss=3.2240612506866455\n",
      "epoch 22: loss=3.0116982460021973\n",
      "epoch 23: loss=2.783949851989746\n",
      "epoch 24: loss=2.5164566040039062\n",
      "epoch 25: loss=2.295870542526245\n",
      "epoch 26: loss=2.0627450942993164\n",
      "epoch 27: loss=1.8700968027114868\n",
      "epoch 28: loss=1.6952862739562988\n",
      "epoch 29: loss=1.5650707483291626\n",
      "epoch 30: loss=1.4760491847991943\n",
      "epoch 31: loss=1.4067041873931885\n",
      "epoch 32: loss=1.3428398370742798\n",
      "epoch 33: loss=1.2684297561645508\n",
      "epoch 34: loss=1.188151478767395\n",
      "epoch 35: loss=1.1298069953918457\n",
      "epoch 36: loss=1.097673773765564\n",
      "epoch 37: loss=1.0867302417755127\n",
      "epoch 38: loss=1.077364444732666\n",
      "epoch 39: loss=1.064212441444397\n",
      "epoch 40: loss=1.0375877618789673\n",
      "epoch 41: loss=1.0136667490005493\n",
      "epoch 42: loss=0.994319498538971\n",
      "epoch 43: loss=0.9823494553565979\n",
      "epoch 44: loss=0.9805982708930969\n",
      "epoch 45: loss=0.9807947874069214\n",
      "epoch 46: loss=0.9795356392860413\n",
      "epoch 47: loss=0.9663313627243042\n",
      "epoch 48: loss=0.9587090015411377\n",
      "epoch 49: loss=0.9572727084159851\n",
      "epoch 50: loss=0.9512771964073181\n",
      "epoch 51: loss=0.950393557548523\n",
      "epoch 52: loss=0.950613260269165\n",
      "epoch 53: loss=0.9501174688339233\n",
      "epoch 54: loss=0.9444743990898132\n",
      "epoch 55: loss=0.9368025064468384\n",
      "epoch 56: loss=0.9356961250305176\n",
      "epoch 57: loss=0.9355765581130981\n",
      "epoch 58: loss=0.9330295920372009\n",
      "epoch 59: loss=0.9372231960296631\n",
      "epoch 60: loss=0.9355498552322388\n",
      "epoch 61: loss=0.9308920502662659\n",
      "epoch 62: loss=0.9286768436431885\n",
      "epoch 63: loss=0.9303286671638489\n",
      "epoch 64: loss=0.9281101226806641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65: loss=0.9286976456642151\n",
      "epoch 66: loss=0.9296814799308777\n",
      "epoch 67: loss=0.9273192286491394\n",
      "epoch 68: loss=0.9244181513786316\n",
      "epoch 69: loss=0.9241966605186462\n",
      "epoch 70: loss=0.9238753318786621\n",
      "epoch 71: loss=0.9242700338363647\n",
      "epoch 72: loss=0.925260066986084\n",
      "epoch 73: loss=0.9215502142906189\n",
      "epoch 74: loss=0.9221975803375244\n",
      "epoch 75: loss=0.920093834400177\n",
      "epoch 76: loss=0.92045658826828\n",
      "epoch 77: loss=0.9195341467857361\n",
      "epoch 78: loss=0.9193552732467651\n",
      "epoch 79: loss=0.9188169240951538\n",
      "epoch 80: loss=0.9197532534599304\n",
      "epoch 81: loss=0.9165698885917664\n",
      "epoch 82: loss=0.917048454284668\n",
      "epoch 83: loss=0.9169097542762756\n",
      "epoch 84: loss=0.9165093302726746\n",
      "epoch 85: loss=0.914009153842926\n",
      "epoch 86: loss=0.9145084619522095\n",
      "epoch 87: loss=0.9128962159156799\n",
      "epoch 88: loss=0.9150664210319519\n",
      "epoch 89: loss=0.9139654636383057\n",
      "epoch 90: loss=0.9119506478309631\n",
      "epoch 91: loss=0.9130185842514038\n",
      "epoch 92: loss=0.9126116633415222\n",
      "epoch 93: loss=0.9104383587837219\n",
      "epoch 94: loss=0.9095422029495239\n",
      "epoch 95: loss=0.9117211699485779\n",
      "epoch 96: loss=0.908652663230896\n",
      "epoch 97: loss=0.9100432991981506\n",
      "epoch 98: loss=0.9092212915420532\n",
      "epoch 99: loss=0.907187819480896\n",
      "epoch 100: loss=0.9079433679580688\n",
      "epoch 101: loss=0.9075580835342407\n",
      "epoch 102: loss=0.907347559928894\n",
      "epoch 103: loss=0.9076508283615112\n",
      "epoch 104: loss=0.9047654867172241\n",
      "epoch 105: loss=0.9061934947967529\n",
      "epoch 106: loss=0.9032439589500427\n",
      "epoch 107: loss=0.9024190902709961\n",
      "epoch 108: loss=0.9019047617912292\n",
      "epoch 109: loss=0.9018299579620361\n",
      "epoch 110: loss=0.9014247059822083\n",
      "epoch 111: loss=0.8998663425445557\n",
      "epoch 112: loss=0.901414155960083\n",
      "epoch 113: loss=0.8997406959533691\n",
      "epoch 114: loss=0.8983551859855652\n",
      "epoch 115: loss=0.9001595377922058\n",
      "epoch 116: loss=0.8991681933403015\n",
      "epoch 117: loss=0.8967130780220032\n",
      "epoch 118: loss=0.8984690308570862\n",
      "epoch 119: loss=0.8965886235237122\n",
      "epoch 120: loss=0.8973137140274048\n",
      "epoch 121: loss=0.896298348903656\n",
      "epoch 122: loss=0.8957870006561279\n",
      "epoch 123: loss=0.8956978917121887\n",
      "epoch 124: loss=0.8961899876594543\n",
      "epoch 125: loss=0.8977344632148743\n",
      "epoch 126: loss=0.8969123959541321\n",
      "epoch 127: loss=0.895214319229126\n",
      "epoch 128: loss=0.8929585218429565\n",
      "epoch 129: loss=0.8967216610908508\n",
      "epoch 130: loss=0.8931543827056885\n",
      "epoch 131: loss=0.8928766250610352\n",
      "epoch 132: loss=0.8924548625946045\n",
      "epoch 133: loss=0.8921151161193848\n",
      "epoch 134: loss=0.8927565813064575\n",
      "epoch 135: loss=0.890592098236084\n",
      "epoch 136: loss=0.8922217488288879\n",
      "epoch 137: loss=0.893505334854126\n",
      "epoch 138: loss=0.8914434313774109\n",
      "epoch 139: loss=0.8930377960205078\n",
      "epoch 140: loss=0.8904507160186768\n",
      "epoch 141: loss=0.8898102045059204\n",
      "epoch 142: loss=0.8898811340332031\n",
      "epoch 143: loss=0.8892550468444824\n",
      "epoch 144: loss=0.8884796500205994\n",
      "epoch 145: loss=0.8896124958992004\n",
      "epoch 146: loss=0.889064371585846\n",
      "epoch 147: loss=0.887343168258667\n",
      "epoch 148: loss=0.8863604068756104\n",
      "epoch 149: loss=0.8891955018043518\n",
      "epoch 150: loss=0.887062668800354\n",
      "epoch 151: loss=0.8879643678665161\n",
      "epoch 152: loss=0.8866168260574341\n",
      "epoch 153: loss=0.8847140669822693\n",
      "epoch 154: loss=0.8846880197525024\n",
      "epoch 155: loss=0.8843023180961609\n",
      "epoch 156: loss=0.8885283470153809\n",
      "epoch 157: loss=0.8848468065261841\n",
      "epoch 158: loss=0.8850775957107544\n",
      "epoch 159: loss=0.8827533721923828\n",
      "epoch 160: loss=0.8831661939620972\n",
      "epoch 161: loss=0.8847126960754395\n",
      "epoch 162: loss=0.8832157254219055\n",
      "epoch 163: loss=0.8847227692604065\n",
      "epoch 164: loss=0.884695827960968\n",
      "epoch 165: loss=0.8834829926490784\n",
      "epoch 166: loss=0.8828083872795105\n",
      "epoch 167: loss=0.8830611109733582\n",
      "epoch 168: loss=0.8817615509033203\n",
      "epoch 169: loss=0.8806017637252808\n",
      "epoch 170: loss=0.8829015493392944\n",
      "epoch 171: loss=0.8814260959625244\n",
      "epoch 172: loss=0.8794904947280884\n",
      "epoch 173: loss=0.8807167410850525\n",
      "epoch 174: loss=0.8790906667709351\n",
      "epoch 175: loss=0.878353476524353\n",
      "epoch 176: loss=0.8801427483558655\n",
      "epoch 177: loss=0.8783333897590637\n",
      "epoch 178: loss=0.8787935972213745\n",
      "epoch 179: loss=0.8786113262176514\n",
      "epoch 180: loss=0.8751631379127502\n",
      "epoch 181: loss=0.8784575462341309\n",
      "epoch 182: loss=0.8780899047851562\n",
      "epoch 183: loss=0.8786332011222839\n",
      "epoch 184: loss=0.877471923828125\n",
      "epoch 185: loss=0.8768311738967896\n",
      "epoch 186: loss=0.876418948173523\n",
      "epoch 187: loss=0.8755348920822144\n",
      "epoch 188: loss=0.8753612041473389\n",
      "epoch 189: loss=0.874081552028656\n",
      "epoch 190: loss=0.8733739852905273\n",
      "epoch 191: loss=0.8738309144973755\n",
      "epoch 192: loss=0.8713157176971436\n",
      "epoch 193: loss=0.8723479509353638\n",
      "epoch 194: loss=0.8724223971366882\n",
      "epoch 195: loss=0.8744748830795288\n",
      "epoch 196: loss=0.8738218545913696\n",
      "epoch 197: loss=0.8715370297431946\n",
      "epoch 198: loss=0.8723931908607483\n",
      "epoch 199: loss=0.8714119791984558\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=13.94789981842041\n",
      "epoch 1: loss=13.562519073486328\n",
      "epoch 2: loss=12.984163284301758\n",
      "epoch 3: loss=12.202459335327148\n",
      "epoch 4: loss=11.13646125793457\n",
      "epoch 5: loss=9.88761043548584\n",
      "epoch 6: loss=9.045768737792969\n",
      "epoch 7: loss=9.07469654083252\n",
      "epoch 8: loss=9.224950790405273\n",
      "epoch 9: loss=8.798229217529297\n",
      "epoch 10: loss=7.7921929359436035\n",
      "epoch 11: loss=6.5929341316223145\n",
      "epoch 12: loss=5.598845958709717\n",
      "epoch 13: loss=4.844821929931641\n",
      "epoch 14: loss=4.363382816314697\n",
      "epoch 15: loss=4.118398189544678\n",
      "epoch 16: loss=3.918750286102295\n",
      "epoch 17: loss=3.714712381362915\n",
      "epoch 18: loss=3.472229480743408\n",
      "epoch 19: loss=3.1589138507843018\n",
      "epoch 20: loss=2.8378772735595703\n",
      "epoch 21: loss=2.5443756580352783\n",
      "epoch 22: loss=2.2745094299316406\n",
      "epoch 23: loss=2.0377514362335205\n",
      "epoch 24: loss=1.8688268661499023\n",
      "epoch 25: loss=1.7455981969833374\n",
      "epoch 26: loss=1.6810965538024902\n",
      "epoch 27: loss=1.6141341924667358\n",
      "epoch 28: loss=1.5322890281677246\n",
      "epoch 29: loss=1.4152261018753052\n",
      "epoch 30: loss=1.3230448961257935\n",
      "epoch 31: loss=1.2543509006500244\n",
      "epoch 32: loss=1.203710675239563\n",
      "epoch 33: loss=1.1693146228790283\n",
      "epoch 34: loss=1.1490336656570435\n",
      "epoch 35: loss=1.1138947010040283\n",
      "epoch 36: loss=1.0741578340530396\n",
      "epoch 37: loss=1.0525805950164795\n",
      "epoch 38: loss=1.0412794351577759\n",
      "epoch 39: loss=1.0364176034927368\n",
      "epoch 40: loss=1.0357098579406738\n",
      "epoch 41: loss=1.023916244506836\n",
      "epoch 42: loss=1.0122748613357544\n",
      "epoch 43: loss=0.9970089197158813\n",
      "epoch 44: loss=0.9907476902008057\n",
      "epoch 45: loss=0.9860503077507019\n",
      "epoch 46: loss=0.9884965419769287\n",
      "epoch 47: loss=0.9847697615623474\n",
      "epoch 48: loss=0.9790839552879333\n",
      "epoch 49: loss=0.9711827039718628\n",
      "epoch 50: loss=0.9663950800895691\n",
      "epoch 51: loss=0.9642637968063354\n",
      "epoch 52: loss=0.9621871709823608\n",
      "epoch 53: loss=0.9597338438034058\n",
      "epoch 54: loss=0.9596207737922668\n",
      "epoch 55: loss=0.9572601318359375\n",
      "epoch 56: loss=0.9546554684638977\n",
      "epoch 57: loss=0.9554659724235535\n",
      "epoch 58: loss=0.9512749314308167\n",
      "epoch 59: loss=0.9518511891365051\n",
      "epoch 60: loss=0.9505021572113037\n",
      "epoch 61: loss=0.9505760669708252\n",
      "epoch 62: loss=0.9501214623451233\n",
      "epoch 63: loss=0.9477835893630981\n",
      "epoch 64: loss=0.948772132396698\n",
      "epoch 65: loss=0.946912944316864\n",
      "epoch 66: loss=0.9459167718887329\n",
      "epoch 67: loss=0.9462540149688721\n",
      "epoch 68: loss=0.9463549256324768\n",
      "epoch 69: loss=0.9428890347480774\n",
      "epoch 70: loss=0.9432762265205383\n",
      "epoch 71: loss=0.9454059600830078\n",
      "epoch 72: loss=0.943979024887085\n",
      "epoch 73: loss=0.9422532320022583\n",
      "epoch 74: loss=0.9454976916313171\n",
      "epoch 75: loss=0.9405181407928467\n",
      "epoch 76: loss=0.9401856064796448\n",
      "epoch 77: loss=0.9413490295410156\n",
      "epoch 78: loss=0.9407657384872437\n",
      "epoch 79: loss=0.9424166083335876\n",
      "epoch 80: loss=0.9422603249549866\n",
      "epoch 81: loss=0.9406550526618958\n",
      "epoch 82: loss=0.9403656125068665\n",
      "epoch 83: loss=0.9367516040802002\n",
      "epoch 84: loss=0.9402955770492554\n",
      "epoch 85: loss=0.9388529062271118\n",
      "epoch 86: loss=0.9384126663208008\n",
      "epoch 87: loss=0.9390631318092346\n",
      "epoch 88: loss=0.9373496174812317\n",
      "epoch 89: loss=0.9363319873809814\n",
      "epoch 90: loss=0.9350408315658569\n",
      "epoch 91: loss=0.9389744400978088\n",
      "epoch 92: loss=0.9372293949127197\n",
      "epoch 93: loss=0.9328615069389343\n",
      "epoch 94: loss=0.9353621602058411\n",
      "epoch 95: loss=0.9345293045043945\n",
      "epoch 96: loss=0.936497151851654\n",
      "epoch 97: loss=0.9333222508430481\n",
      "epoch 98: loss=0.9362791776657104\n",
      "epoch 99: loss=0.9354202747344971\n",
      "epoch 100: loss=0.9345241189002991\n",
      "epoch 101: loss=0.9329330325126648\n",
      "epoch 102: loss=0.934515655040741\n",
      "epoch 103: loss=0.9359157085418701\n",
      "epoch 104: loss=0.9310498833656311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 105: loss=0.9355403780937195\n",
      "epoch 106: loss=0.9341874122619629\n",
      "epoch 107: loss=0.9340915679931641\n",
      "epoch 108: loss=0.9339803457260132\n",
      "epoch 109: loss=0.9331355094909668\n",
      "epoch 110: loss=0.9315740466117859\n",
      "epoch 111: loss=0.9344075322151184\n",
      "epoch 112: loss=0.9345396757125854\n",
      "epoch 113: loss=0.9329255223274231\n",
      "epoch 114: loss=0.9325127601623535\n",
      "epoch 115: loss=0.9322464466094971\n",
      "epoch 116: loss=0.9287794828414917\n",
      "epoch 117: loss=0.9316392540931702\n",
      "epoch 118: loss=0.9308521151542664\n",
      "epoch 119: loss=0.931586742401123\n",
      "epoch 120: loss=0.9315756559371948\n",
      "epoch 121: loss=0.931288480758667\n",
      "epoch 122: loss=0.9283710718154907\n",
      "epoch 123: loss=0.9302006959915161\n",
      "epoch 124: loss=0.9288034439086914\n",
      "epoch 125: loss=0.9293642044067383\n",
      "epoch 126: loss=0.9304415583610535\n",
      "epoch 127: loss=0.9274952411651611\n",
      "epoch 128: loss=0.9267542362213135\n",
      "epoch 129: loss=0.928544282913208\n",
      "epoch 130: loss=0.9281026124954224\n",
      "epoch 131: loss=0.9277812838554382\n",
      "epoch 132: loss=0.9254634380340576\n",
      "epoch 133: loss=0.9292412996292114\n",
      "epoch 134: loss=0.9260080456733704\n",
      "epoch 135: loss=0.9287260174751282\n",
      "epoch 136: loss=0.9296916723251343\n",
      "epoch 137: loss=0.9288579225540161\n",
      "epoch 138: loss=0.9266684055328369\n",
      "epoch 139: loss=0.92643141746521\n",
      "epoch 140: loss=0.927476167678833\n",
      "epoch 141: loss=0.9274134039878845\n",
      "epoch 142: loss=0.9217219352722168\n",
      "epoch 143: loss=0.9238083958625793\n",
      "epoch 144: loss=0.9254616498947144\n",
      "epoch 145: loss=0.9238835573196411\n",
      "epoch 146: loss=0.9217920303344727\n",
      "epoch 147: loss=0.9258241653442383\n",
      "epoch 148: loss=0.9224612712860107\n",
      "epoch 149: loss=0.9233627319335938\n",
      "epoch 150: loss=0.9243904948234558\n",
      "epoch 151: loss=0.9221599698066711\n",
      "epoch 152: loss=0.9224187731742859\n",
      "epoch 153: loss=0.922616183757782\n",
      "epoch 154: loss=0.9235623478889465\n",
      "epoch 155: loss=0.9209322929382324\n",
      "epoch 156: loss=0.9223059415817261\n",
      "epoch 157: loss=0.921379566192627\n",
      "epoch 158: loss=0.9192972183227539\n",
      "epoch 159: loss=0.9202082753181458\n",
      "epoch 160: loss=0.9212000966072083\n",
      "epoch 161: loss=0.9199925065040588\n",
      "epoch 162: loss=0.9205456972122192\n",
      "epoch 163: loss=0.9178565144538879\n",
      "epoch 164: loss=0.9187173843383789\n",
      "epoch 165: loss=0.9218778610229492\n",
      "epoch 166: loss=0.9198881983757019\n",
      "epoch 167: loss=0.9177618026733398\n",
      "epoch 168: loss=0.9194622039794922\n",
      "epoch 169: loss=0.9167129397392273\n",
      "epoch 170: loss=0.9168789982795715\n",
      "epoch 171: loss=0.9175330996513367\n",
      "epoch 172: loss=0.9176763296127319\n",
      "epoch 173: loss=0.9158539175987244\n",
      "epoch 174: loss=0.9166463017463684\n",
      "epoch 175: loss=0.9184950590133667\n",
      "epoch 176: loss=0.9166465997695923\n",
      "epoch 177: loss=0.9131886959075928\n",
      "epoch 178: loss=0.9153301119804382\n",
      "epoch 179: loss=0.9149456024169922\n",
      "epoch 180: loss=0.9131770133972168\n",
      "epoch 181: loss=0.9146304726600647\n",
      "epoch 182: loss=0.9122217893600464\n",
      "epoch 183: loss=0.9139718413352966\n",
      "epoch 184: loss=0.9134284853935242\n",
      "epoch 185: loss=0.9114924669265747\n",
      "epoch 186: loss=0.9124499559402466\n",
      "epoch 187: loss=0.9123768210411072\n",
      "epoch 188: loss=0.9129353761672974\n",
      "epoch 189: loss=0.9115522503852844\n",
      "epoch 190: loss=0.9083579778671265\n",
      "epoch 191: loss=0.9101250767707825\n",
      "epoch 192: loss=0.9078990817070007\n",
      "epoch 193: loss=0.910658597946167\n",
      "epoch 194: loss=0.9074825048446655\n",
      "epoch 195: loss=0.90964275598526\n",
      "epoch 196: loss=0.9092221856117249\n",
      "epoch 197: loss=0.9082278609275818\n",
      "epoch 198: loss=0.9060422778129578\n",
      "epoch 199: loss=0.904828667640686\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=14.018209457397461\n",
      "epoch 1: loss=13.595171928405762\n",
      "epoch 2: loss=13.175698280334473\n",
      "epoch 3: loss=12.583662986755371\n",
      "epoch 4: loss=11.665985107421875\n",
      "epoch 5: loss=10.699650764465332\n",
      "epoch 6: loss=9.661355018615723\n",
      "epoch 7: loss=8.885048866271973\n",
      "epoch 8: loss=9.109668731689453\n",
      "epoch 9: loss=9.137054443359375\n",
      "epoch 10: loss=8.797686576843262\n",
      "epoch 11: loss=7.873317718505859\n",
      "epoch 12: loss=6.8150739669799805\n",
      "epoch 13: loss=5.863760948181152\n",
      "epoch 14: loss=5.14842414855957\n",
      "epoch 15: loss=4.66999626159668\n",
      "epoch 16: loss=4.337221622467041\n",
      "epoch 17: loss=4.066965103149414\n",
      "epoch 18: loss=3.845247507095337\n",
      "epoch 19: loss=3.567683219909668\n",
      "epoch 20: loss=3.337440252304077\n",
      "epoch 21: loss=3.134955644607544\n",
      "epoch 22: loss=2.9154486656188965\n",
      "epoch 23: loss=2.689525842666626\n",
      "epoch 24: loss=2.413764238357544\n",
      "epoch 25: loss=2.185032844543457\n",
      "epoch 26: loss=1.992607831954956\n",
      "epoch 27: loss=1.829880714416504\n",
      "epoch 28: loss=1.7112581729888916\n",
      "epoch 29: loss=1.640779733657837\n",
      "epoch 30: loss=1.5785472393035889\n",
      "epoch 31: loss=1.498114824295044\n",
      "epoch 32: loss=1.4238039255142212\n",
      "epoch 33: loss=1.3549847602844238\n",
      "epoch 34: loss=1.301432490348816\n",
      "epoch 35: loss=1.2629673480987549\n",
      "epoch 36: loss=1.2251659631729126\n",
      "epoch 37: loss=1.1988003253936768\n",
      "epoch 38: loss=1.169247031211853\n",
      "epoch 39: loss=1.1267213821411133\n",
      "epoch 40: loss=1.100356101989746\n",
      "epoch 41: loss=1.076889991760254\n",
      "epoch 42: loss=1.069750189781189\n",
      "epoch 43: loss=1.0624781847000122\n",
      "epoch 44: loss=1.0527909994125366\n",
      "epoch 45: loss=1.0387741327285767\n",
      "epoch 46: loss=1.0297338962554932\n",
      "epoch 47: loss=1.0177279710769653\n",
      "epoch 48: loss=1.012440800666809\n",
      "epoch 49: loss=1.00615656375885\n",
      "epoch 50: loss=1.005239486694336\n",
      "epoch 51: loss=1.001654028892517\n",
      "epoch 52: loss=0.997184693813324\n",
      "epoch 53: loss=0.9901406764984131\n",
      "epoch 54: loss=0.9887733459472656\n",
      "epoch 55: loss=0.9874784350395203\n",
      "epoch 56: loss=0.9835583567619324\n",
      "epoch 57: loss=0.9815840125083923\n",
      "epoch 58: loss=0.9818981885910034\n",
      "epoch 59: loss=0.9826614260673523\n",
      "epoch 60: loss=0.9778563976287842\n",
      "epoch 61: loss=0.9750175476074219\n",
      "epoch 62: loss=0.9758234620094299\n",
      "epoch 63: loss=0.9717230200767517\n",
      "epoch 64: loss=0.9725618958473206\n",
      "epoch 65: loss=0.9719653725624084\n",
      "epoch 66: loss=0.969011127948761\n",
      "epoch 67: loss=0.9681044816970825\n",
      "epoch 68: loss=0.9683277010917664\n",
      "epoch 69: loss=0.9673957824707031\n",
      "epoch 70: loss=0.9672053456306458\n",
      "epoch 71: loss=0.9652804136276245\n",
      "epoch 72: loss=0.9664208889007568\n",
      "epoch 73: loss=0.9655210971832275\n",
      "epoch 74: loss=0.9666966795921326\n",
      "epoch 75: loss=0.9608597755432129\n",
      "epoch 76: loss=0.9614414572715759\n",
      "epoch 77: loss=0.9630457758903503\n",
      "epoch 78: loss=0.9659408926963806\n",
      "epoch 79: loss=0.9609771370887756\n",
      "epoch 80: loss=0.9641209840774536\n",
      "epoch 81: loss=0.961703360080719\n",
      "epoch 82: loss=0.9620184898376465\n",
      "epoch 83: loss=0.9628294110298157\n",
      "epoch 84: loss=0.9599112868309021\n",
      "epoch 85: loss=0.9587056040763855\n",
      "epoch 86: loss=0.9583058953285217\n",
      "epoch 87: loss=0.9594027400016785\n",
      "epoch 88: loss=0.9603354930877686\n",
      "epoch 89: loss=0.9560373425483704\n",
      "epoch 90: loss=0.9608587622642517\n",
      "epoch 91: loss=0.9575709700584412\n",
      "epoch 92: loss=0.9568671584129333\n",
      "epoch 93: loss=0.9562472701072693\n",
      "epoch 94: loss=0.9573724865913391\n",
      "epoch 95: loss=0.9551957249641418\n",
      "epoch 96: loss=0.9567321538925171\n",
      "epoch 97: loss=0.9564287662506104\n",
      "epoch 98: loss=0.9560259580612183\n",
      "epoch 99: loss=0.9558732509613037\n",
      "epoch 100: loss=0.9573435187339783\n",
      "epoch 101: loss=0.9561431407928467\n",
      "epoch 102: loss=0.9563306570053101\n",
      "epoch 103: loss=0.9554402232170105\n",
      "epoch 104: loss=0.9559671878814697\n",
      "epoch 105: loss=0.9546112418174744\n",
      "epoch 106: loss=0.9546293020248413\n",
      "epoch 107: loss=0.9513055086135864\n",
      "epoch 108: loss=0.953609824180603\n",
      "epoch 109: loss=0.9523386359214783\n",
      "epoch 110: loss=0.9548656940460205\n",
      "epoch 111: loss=0.953561007976532\n",
      "epoch 112: loss=0.9523550868034363\n",
      "epoch 113: loss=0.9523154497146606\n",
      "epoch 114: loss=0.9542331695556641\n",
      "epoch 115: loss=0.9529466032981873\n",
      "epoch 116: loss=0.95149165391922\n",
      "epoch 117: loss=0.9523534178733826\n",
      "epoch 118: loss=0.9508713483810425\n",
      "epoch 119: loss=0.9518781900405884\n",
      "epoch 120: loss=0.9496857523918152\n",
      "epoch 121: loss=0.9507811665534973\n",
      "epoch 122: loss=0.9488153457641602\n",
      "epoch 123: loss=0.9479492902755737\n",
      "epoch 124: loss=0.9484821557998657\n",
      "epoch 125: loss=0.9496474862098694\n",
      "epoch 126: loss=0.9498569965362549\n",
      "epoch 127: loss=0.9482960104942322\n",
      "epoch 128: loss=0.9473397135734558\n",
      "epoch 129: loss=0.9460651874542236\n",
      "epoch 130: loss=0.9472375512123108\n",
      "epoch 131: loss=0.9478703737258911\n",
      "epoch 132: loss=0.946471095085144\n",
      "epoch 133: loss=0.9465346336364746\n",
      "epoch 134: loss=0.9461937546730042\n",
      "epoch 135: loss=0.9441484808921814\n",
      "epoch 136: loss=0.9459471106529236\n",
      "epoch 137: loss=0.9461029171943665\n",
      "epoch 138: loss=0.943996012210846\n",
      "epoch 139: loss=0.9441114664077759\n",
      "epoch 140: loss=0.9428494572639465\n",
      "epoch 141: loss=0.9409955143928528\n",
      "epoch 142: loss=0.9425985217094421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 143: loss=0.9416229128837585\n",
      "epoch 144: loss=0.9392995238304138\n",
      "epoch 145: loss=0.9411044716835022\n",
      "epoch 146: loss=0.9406139254570007\n",
      "epoch 147: loss=0.9387670159339905\n",
      "epoch 148: loss=0.940093994140625\n",
      "epoch 149: loss=0.9388634562492371\n",
      "epoch 150: loss=0.9376776814460754\n",
      "epoch 151: loss=0.9377339482307434\n",
      "epoch 152: loss=0.9373413920402527\n",
      "epoch 153: loss=0.9357717633247375\n",
      "epoch 154: loss=0.9359387755393982\n",
      "epoch 155: loss=0.9361255764961243\n",
      "epoch 156: loss=0.9353300929069519\n",
      "epoch 157: loss=0.9331946969032288\n",
      "epoch 158: loss=0.9334695339202881\n",
      "epoch 159: loss=0.9338854551315308\n",
      "epoch 160: loss=0.9315911531448364\n",
      "epoch 161: loss=0.9307677745819092\n",
      "epoch 162: loss=0.9309917688369751\n",
      "epoch 163: loss=0.9307948350906372\n",
      "epoch 164: loss=0.9287460446357727\n",
      "epoch 165: loss=0.9292322397232056\n",
      "epoch 166: loss=0.9270939826965332\n",
      "epoch 167: loss=0.9292794466018677\n",
      "epoch 168: loss=0.9279640316963196\n",
      "epoch 169: loss=0.9260625243186951\n",
      "epoch 170: loss=0.9269822835922241\n",
      "epoch 171: loss=0.9253575801849365\n",
      "epoch 172: loss=0.9237464666366577\n",
      "epoch 173: loss=0.9202792644500732\n",
      "epoch 174: loss=0.9225690364837646\n",
      "epoch 175: loss=0.9218147397041321\n",
      "epoch 176: loss=0.9205122590065002\n",
      "epoch 177: loss=0.920600414276123\n",
      "epoch 178: loss=0.9187451004981995\n",
      "epoch 179: loss=0.9205565452575684\n",
      "epoch 180: loss=0.9159494042396545\n",
      "epoch 181: loss=0.9173896312713623\n",
      "epoch 182: loss=0.9165585041046143\n",
      "epoch 183: loss=0.9160813689231873\n",
      "epoch 184: loss=0.9150887131690979\n",
      "epoch 185: loss=0.9152066707611084\n",
      "epoch 186: loss=0.9149112701416016\n",
      "epoch 187: loss=0.9137454032897949\n",
      "epoch 188: loss=0.9137787222862244\n",
      "epoch 189: loss=0.9114665985107422\n",
      "epoch 190: loss=0.9132274389266968\n",
      "epoch 191: loss=0.9095019698143005\n",
      "epoch 192: loss=0.9130319952964783\n",
      "epoch 193: loss=0.9144721627235413\n",
      "epoch 194: loss=0.9126017093658447\n",
      "epoch 195: loss=0.9095172882080078\n",
      "epoch 196: loss=0.9104055166244507\n",
      "epoch 197: loss=0.9092559218406677\n",
      "epoch 198: loss=0.9090626239776611\n",
      "epoch 199: loss=0.9071850776672363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e8ebeff86248e1a12efaf41408134a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 135848.65625\n",
      "Epoch 10, Loss: 85483.6875\n",
      "Epoch 20, Loss: 47892.6640625\n",
      "Epoch 30, Loss: 29345.630859375\n",
      "Epoch 40, Loss: 28837.556640625\n",
      "Epoch 50, Loss: 29913.97265625\n",
      "Epoch 60, Loss: 27674.98046875\n",
      "Epoch 70, Loss: 28268.03515625\n",
      "Epoch 80, Loss: 27566.5078125\n",
      "Epoch 90, Loss: 29430.65625\n",
      "Epoch 100, Loss: 28829.908203125\n",
      "Epoch 110, Loss: 27552.29296875\n",
      "Epoch 120, Loss: 27692.357421875\n",
      "Epoch 130, Loss: 30171.017578125\n",
      "Epoch 140, Loss: 28042.298828125\n",
      "Epoch 150, Loss: 27977.244140625\n",
      "Epoch 160, Loss: 26594.029296875\n",
      "Epoch 170, Loss: 32018.458984375\n",
      "Epoch 180, Loss: 28455.28515625\n",
      "Epoch 190, Loss: 29161.708984375\n",
      "training patch with 267352 edges\n",
      "epoch 0: loss=18.162399291992188\n",
      "epoch 1: loss=17.789989471435547\n",
      "epoch 2: loss=17.35088348388672\n",
      "epoch 3: loss=16.473346710205078\n",
      "epoch 4: loss=15.19595718383789\n",
      "epoch 5: loss=13.759605407714844\n",
      "epoch 6: loss=12.751821517944336\n",
      "epoch 7: loss=12.905418395996094\n",
      "epoch 8: loss=13.498082160949707\n",
      "epoch 9: loss=13.789023399353027\n",
      "epoch 10: loss=13.3402099609375\n",
      "epoch 11: loss=12.433419227600098\n",
      "epoch 12: loss=11.415471076965332\n",
      "epoch 13: loss=10.417795181274414\n",
      "epoch 14: loss=9.530024528503418\n",
      "epoch 15: loss=8.854701042175293\n",
      "epoch 16: loss=8.462813377380371\n",
      "epoch 17: loss=8.091333389282227\n",
      "epoch 18: loss=7.868792533874512\n",
      "epoch 19: loss=7.613531112670898\n",
      "epoch 20: loss=7.293800354003906\n",
      "epoch 21: loss=7.00283670425415\n",
      "epoch 22: loss=6.701914310455322\n",
      "epoch 23: loss=6.31150484085083\n",
      "epoch 24: loss=5.897127628326416\n",
      "epoch 25: loss=5.466572284698486\n",
      "epoch 26: loss=5.0224480628967285\n",
      "epoch 27: loss=4.486088275909424\n",
      "epoch 28: loss=3.9865243434906006\n",
      "epoch 29: loss=3.5255799293518066\n",
      "epoch 30: loss=3.07523512840271\n",
      "epoch 31: loss=2.7025856971740723\n",
      "epoch 32: loss=2.3879454135894775\n",
      "epoch 33: loss=2.161144971847534\n",
      "epoch 34: loss=1.938267469406128\n",
      "epoch 35: loss=1.7318689823150635\n",
      "epoch 36: loss=1.5612735748291016\n",
      "epoch 37: loss=1.4280853271484375\n",
      "epoch 38: loss=1.3257167339324951\n",
      "epoch 39: loss=1.240766167640686\n",
      "epoch 40: loss=1.1729156970977783\n",
      "epoch 41: loss=1.1155802011489868\n",
      "epoch 42: loss=1.0747400522232056\n",
      "epoch 43: loss=1.0471107959747314\n",
      "epoch 44: loss=1.0245544910430908\n",
      "epoch 45: loss=1.0150765180587769\n",
      "epoch 46: loss=1.0041719675064087\n",
      "epoch 47: loss=0.9900426864624023\n",
      "epoch 48: loss=0.9794462323188782\n",
      "epoch 49: loss=0.9714038372039795\n",
      "epoch 50: loss=0.9675267934799194\n",
      "epoch 51: loss=0.9613311886787415\n",
      "epoch 52: loss=0.9566646218299866\n",
      "epoch 53: loss=0.9501223564147949\n",
      "epoch 54: loss=0.9454387426376343\n",
      "epoch 55: loss=0.9412034153938293\n",
      "epoch 56: loss=0.9378864169120789\n",
      "epoch 57: loss=0.9341229796409607\n",
      "epoch 58: loss=0.9326417446136475\n",
      "epoch 59: loss=0.929211437702179\n",
      "epoch 60: loss=0.9296540021896362\n",
      "epoch 61: loss=0.9270210862159729\n",
      "epoch 62: loss=0.9251787066459656\n",
      "epoch 63: loss=0.9214680194854736\n",
      "epoch 64: loss=0.9200966358184814\n",
      "epoch 65: loss=0.9177389144897461\n",
      "epoch 66: loss=0.9148232936859131\n",
      "epoch 67: loss=0.9127205014228821\n",
      "epoch 68: loss=0.9100937843322754\n",
      "epoch 69: loss=0.9074201583862305\n",
      "epoch 70: loss=0.9051512479782104\n",
      "epoch 71: loss=0.9040050506591797\n",
      "epoch 72: loss=0.9027267694473267\n",
      "epoch 73: loss=0.9010167717933655\n",
      "epoch 74: loss=0.9010508060455322\n",
      "epoch 75: loss=0.9010384678840637\n",
      "epoch 76: loss=0.8983014822006226\n",
      "epoch 77: loss=0.8964008688926697\n",
      "epoch 78: loss=0.8963149189949036\n",
      "epoch 79: loss=0.8942675590515137\n",
      "epoch 80: loss=0.8922945261001587\n",
      "epoch 81: loss=0.8927232027053833\n",
      "epoch 82: loss=0.8918871879577637\n",
      "epoch 83: loss=0.8895998597145081\n",
      "epoch 84: loss=0.8880131244659424\n",
      "epoch 85: loss=0.8894367218017578\n",
      "epoch 86: loss=0.8863188624382019\n",
      "epoch 87: loss=0.8848211169242859\n",
      "epoch 88: loss=0.8847204446792603\n",
      "epoch 89: loss=0.8846582770347595\n",
      "epoch 90: loss=0.8828954696655273\n",
      "epoch 91: loss=0.8805949687957764\n",
      "epoch 92: loss=0.8801801204681396\n",
      "epoch 93: loss=0.8809613585472107\n",
      "epoch 94: loss=0.8792497515678406\n",
      "epoch 95: loss=0.8777864575386047\n",
      "epoch 96: loss=0.8764383792877197\n",
      "epoch 97: loss=0.8754667043685913\n",
      "epoch 98: loss=0.875791072845459\n",
      "epoch 99: loss=0.8748618960380554\n",
      "epoch 100: loss=0.8730639815330505\n",
      "epoch 101: loss=0.8727403283119202\n",
      "epoch 102: loss=0.871563732624054\n",
      "epoch 103: loss=0.8708282113075256\n",
      "epoch 104: loss=0.870336651802063\n",
      "epoch 105: loss=0.8691332936286926\n",
      "epoch 106: loss=0.8686990141868591\n",
      "epoch 107: loss=0.8678226470947266\n",
      "epoch 108: loss=0.865821361541748\n",
      "epoch 109: loss=0.8672344088554382\n",
      "epoch 110: loss=0.8646618723869324\n",
      "epoch 111: loss=0.864372193813324\n",
      "epoch 112: loss=0.8626261353492737\n",
      "epoch 113: loss=0.8624921441078186\n",
      "epoch 114: loss=0.8616002202033997\n",
      "epoch 115: loss=0.8628216981887817\n",
      "epoch 116: loss=0.8599486947059631\n",
      "epoch 117: loss=0.8597601652145386\n",
      "epoch 118: loss=0.8599681258201599\n",
      "epoch 119: loss=0.8583925366401672\n",
      "epoch 120: loss=0.8571844100952148\n",
      "epoch 121: loss=0.8564898371696472\n",
      "epoch 122: loss=0.8567925691604614\n",
      "epoch 123: loss=0.8549532890319824\n",
      "epoch 124: loss=0.8544050455093384\n",
      "epoch 125: loss=0.8534536361694336\n",
      "epoch 126: loss=0.8524158596992493\n",
      "epoch 127: loss=0.8516885638237\n",
      "epoch 128: loss=0.8504639267921448\n",
      "epoch 129: loss=0.8500217795372009\n",
      "epoch 130: loss=0.8510640859603882\n",
      "epoch 131: loss=0.8492640256881714\n",
      "epoch 132: loss=0.8481346368789673\n",
      "epoch 133: loss=0.8483926653862\n",
      "epoch 134: loss=0.8484334349632263\n",
      "epoch 135: loss=0.8484224081039429\n",
      "epoch 136: loss=0.8485588431358337\n",
      "epoch 137: loss=0.8461731672286987\n",
      "epoch 138: loss=0.8462691903114319\n",
      "epoch 139: loss=0.845548689365387\n",
      "epoch 140: loss=0.8444628715515137\n",
      "epoch 141: loss=0.8452361822128296\n",
      "epoch 142: loss=0.8441212177276611\n",
      "epoch 143: loss=0.8444871306419373\n",
      "epoch 144: loss=0.8435450792312622\n",
      "epoch 145: loss=0.843542218208313\n",
      "epoch 146: loss=0.8429269790649414\n",
      "epoch 147: loss=0.8420067429542542\n",
      "epoch 148: loss=0.8415882587432861\n",
      "epoch 149: loss=0.8417906761169434\n",
      "epoch 150: loss=0.8416681885719299\n",
      "epoch 151: loss=0.8410205841064453\n",
      "epoch 152: loss=0.8410775065422058\n",
      "epoch 153: loss=0.8408057689666748\n",
      "epoch 154: loss=0.8392065167427063\n",
      "epoch 155: loss=0.8400906920433044\n",
      "epoch 156: loss=0.8402378559112549\n",
      "epoch 157: loss=0.8389014005661011\n",
      "epoch 158: loss=0.8396772146224976\n",
      "epoch 159: loss=0.8392360210418701\n",
      "epoch 160: loss=0.8370948433876038\n",
      "epoch 161: loss=0.838546633720398\n",
      "epoch 162: loss=0.8375706076622009\n",
      "epoch 163: loss=0.837275505065918\n",
      "epoch 164: loss=0.8355270624160767\n",
      "epoch 165: loss=0.8383840322494507\n",
      "epoch 166: loss=0.8373246788978577\n",
      "epoch 167: loss=0.8358938694000244\n",
      "epoch 168: loss=0.8367680311203003\n",
      "epoch 169: loss=0.8349505066871643\n",
      "epoch 170: loss=0.8345122933387756\n",
      "epoch 171: loss=0.8361301422119141\n",
      "epoch 172: loss=0.8361673951148987\n",
      "epoch 173: loss=0.8349688053131104\n",
      "epoch 174: loss=0.8337057828903198\n",
      "epoch 175: loss=0.8335829377174377\n",
      "epoch 176: loss=0.8333348631858826\n",
      "epoch 177: loss=0.835599422454834\n",
      "epoch 178: loss=0.8325483202934265\n",
      "epoch 179: loss=0.8321223855018616\n",
      "epoch 180: loss=0.8325340151786804\n",
      "epoch 181: loss=0.8326463103294373\n",
      "epoch 182: loss=0.8330498337745667\n",
      "epoch 183: loss=0.8315531015396118\n",
      "epoch 184: loss=0.8312710523605347\n",
      "epoch 185: loss=0.8321953415870667\n",
      "epoch 186: loss=0.8315778970718384\n",
      "epoch 187: loss=0.8297343850135803\n",
      "epoch 188: loss=0.8312098979949951\n",
      "epoch 189: loss=0.8305990695953369\n",
      "epoch 190: loss=0.8300837874412537\n",
      "epoch 191: loss=0.8287073969841003\n",
      "epoch 192: loss=0.8305221199989319\n",
      "epoch 193: loss=0.829272985458374\n",
      "epoch 194: loss=0.8284721970558167\n",
      "epoch 195: loss=0.8293473124504089\n",
      "epoch 196: loss=0.8305044770240784\n",
      "epoch 197: loss=0.8289204239845276\n",
      "epoch 198: loss=0.82828688621521\n",
      "epoch 199: loss=0.8292821645736694\n",
      "training patch with 227552 edges\n",
      "epoch 0: loss=18.204113006591797\n",
      "epoch 1: loss=17.813722610473633\n",
      "epoch 2: loss=17.23150634765625\n",
      "epoch 3: loss=16.267759323120117\n",
      "epoch 4: loss=14.696885108947754\n",
      "epoch 5: loss=13.080341339111328\n",
      "epoch 6: loss=12.629634857177734\n",
      "epoch 7: loss=13.524341583251953\n",
      "epoch 8: loss=14.12956428527832\n",
      "epoch 9: loss=13.934006690979004\n",
      "epoch 10: loss=13.07271957397461\n",
      "epoch 11: loss=11.736154556274414\n",
      "epoch 12: loss=10.473846435546875\n",
      "epoch 13: loss=9.42935562133789\n",
      "epoch 14: loss=8.632691383361816\n",
      "epoch 15: loss=8.20339584350586\n",
      "epoch 16: loss=7.865563869476318\n",
      "epoch 17: loss=7.575501441955566\n",
      "epoch 18: loss=7.326038837432861\n",
      "epoch 19: loss=6.951361656188965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss=6.629589080810547\n",
      "epoch 21: loss=6.251727104187012\n",
      "epoch 22: loss=5.904007434844971\n",
      "epoch 23: loss=5.532655715942383\n",
      "epoch 24: loss=5.127653121948242\n",
      "epoch 25: loss=4.715644836425781\n",
      "epoch 26: loss=4.295774459838867\n",
      "epoch 27: loss=3.946220874786377\n",
      "epoch 28: loss=3.571915626525879\n",
      "epoch 29: loss=3.298992395401001\n",
      "epoch 30: loss=3.006748676300049\n",
      "epoch 31: loss=2.796579360961914\n",
      "epoch 32: loss=2.567919969558716\n",
      "epoch 33: loss=2.345266580581665\n",
      "epoch 34: loss=2.139686346054077\n",
      "epoch 35: loss=1.9446535110473633\n",
      "epoch 36: loss=1.7758933305740356\n",
      "epoch 37: loss=1.6536202430725098\n",
      "epoch 38: loss=1.5494587421417236\n",
      "epoch 39: loss=1.4410268068313599\n",
      "epoch 40: loss=1.3529835939407349\n",
      "epoch 41: loss=1.2754535675048828\n",
      "epoch 42: loss=1.2211976051330566\n",
      "epoch 43: loss=1.1827282905578613\n",
      "epoch 44: loss=1.1469532251358032\n",
      "epoch 45: loss=1.112436294555664\n",
      "epoch 46: loss=1.0837252140045166\n",
      "epoch 47: loss=1.0610439777374268\n",
      "epoch 48: loss=1.0408600568771362\n",
      "epoch 49: loss=1.028350830078125\n",
      "epoch 50: loss=1.0157074928283691\n",
      "epoch 51: loss=1.0079758167266846\n",
      "epoch 52: loss=0.9994097948074341\n",
      "epoch 53: loss=0.9904438853263855\n",
      "epoch 54: loss=0.9835130572319031\n",
      "epoch 55: loss=0.9776941537857056\n",
      "epoch 56: loss=0.9755330085754395\n",
      "epoch 57: loss=0.968991756439209\n",
      "epoch 58: loss=0.9640701413154602\n",
      "epoch 59: loss=0.9631144404411316\n",
      "epoch 60: loss=0.9615002274513245\n",
      "epoch 61: loss=0.9596409201622009\n",
      "epoch 62: loss=0.9557671546936035\n",
      "epoch 63: loss=0.9539970755577087\n",
      "epoch 64: loss=0.9541648626327515\n",
      "epoch 65: loss=0.9527370929718018\n",
      "epoch 66: loss=0.9525041580200195\n",
      "epoch 67: loss=0.9512749910354614\n",
      "epoch 68: loss=0.9486657977104187\n",
      "epoch 69: loss=0.948054850101471\n",
      "epoch 70: loss=0.9470246434211731\n",
      "epoch 71: loss=0.9455441832542419\n",
      "epoch 72: loss=0.9440654516220093\n",
      "epoch 73: loss=0.9446624517440796\n",
      "epoch 74: loss=0.9438908696174622\n",
      "epoch 75: loss=0.9423460364341736\n",
      "epoch 76: loss=0.942696750164032\n",
      "epoch 77: loss=0.9416629672050476\n",
      "epoch 78: loss=0.9417151808738708\n",
      "epoch 79: loss=0.940200924873352\n",
      "epoch 80: loss=0.9402930736541748\n",
      "epoch 81: loss=0.9397051334381104\n",
      "epoch 82: loss=0.939835250377655\n",
      "epoch 83: loss=0.9371643662452698\n",
      "epoch 84: loss=0.9369422197341919\n",
      "epoch 85: loss=0.937650740146637\n",
      "epoch 86: loss=0.9369398355484009\n",
      "epoch 87: loss=0.9366434216499329\n",
      "epoch 88: loss=0.9351550340652466\n",
      "epoch 89: loss=0.9360402226448059\n",
      "epoch 90: loss=0.934349536895752\n",
      "epoch 91: loss=0.9342235922813416\n",
      "epoch 92: loss=0.9324553608894348\n",
      "epoch 93: loss=0.9333173632621765\n",
      "epoch 94: loss=0.9321949481964111\n",
      "epoch 95: loss=0.9334731101989746\n",
      "epoch 96: loss=0.9304278492927551\n",
      "epoch 97: loss=0.9312000870704651\n",
      "epoch 98: loss=0.930537223815918\n",
      "epoch 99: loss=0.9289023280143738\n",
      "epoch 100: loss=0.9293297529220581\n",
      "epoch 101: loss=0.9276361465454102\n",
      "epoch 102: loss=0.9272466897964478\n",
      "epoch 103: loss=0.9267091155052185\n",
      "epoch 104: loss=0.9252578616142273\n",
      "epoch 105: loss=0.9235662221908569\n",
      "epoch 106: loss=0.9234569072723389\n",
      "epoch 107: loss=0.9243461489677429\n",
      "epoch 108: loss=0.9231902360916138\n",
      "epoch 109: loss=0.9207586646080017\n",
      "epoch 110: loss=0.9203706979751587\n",
      "epoch 111: loss=0.9205160737037659\n",
      "epoch 112: loss=0.9178548455238342\n",
      "epoch 113: loss=0.9193687438964844\n",
      "epoch 114: loss=0.9171308875083923\n",
      "epoch 115: loss=0.9177171587944031\n",
      "epoch 116: loss=0.9159016013145447\n",
      "epoch 117: loss=0.9138035774230957\n",
      "epoch 118: loss=0.9130902886390686\n",
      "epoch 119: loss=0.912034273147583\n",
      "epoch 120: loss=0.9112232327461243\n",
      "epoch 121: loss=0.9110243916511536\n",
      "epoch 122: loss=0.9106404185295105\n",
      "epoch 123: loss=0.9076365232467651\n",
      "epoch 124: loss=0.9069547057151794\n",
      "epoch 125: loss=0.9061641097068787\n",
      "epoch 126: loss=0.9042653441429138\n",
      "epoch 127: loss=0.9031933546066284\n",
      "epoch 128: loss=0.9033082723617554\n",
      "epoch 129: loss=0.9005905985832214\n",
      "epoch 130: loss=0.9000324010848999\n",
      "epoch 131: loss=0.8991260528564453\n",
      "epoch 132: loss=0.8971947431564331\n",
      "epoch 133: loss=0.8964570760726929\n",
      "epoch 134: loss=0.897325873374939\n",
      "epoch 135: loss=0.8949182629585266\n",
      "epoch 136: loss=0.8937516212463379\n",
      "epoch 137: loss=0.8942463994026184\n",
      "epoch 138: loss=0.8940356969833374\n",
      "epoch 139: loss=0.8924791812896729\n",
      "epoch 140: loss=0.8909865617752075\n",
      "epoch 141: loss=0.8920528888702393\n",
      "epoch 142: loss=0.8917289972305298\n",
      "epoch 143: loss=0.8949275612831116\n",
      "epoch 144: loss=0.8943334817886353\n",
      "epoch 145: loss=0.8931914567947388\n",
      "epoch 146: loss=0.8897665143013\n",
      "epoch 147: loss=0.8904104828834534\n",
      "epoch 148: loss=0.8906344175338745\n",
      "epoch 149: loss=0.8893565535545349\n",
      "epoch 150: loss=0.887864351272583\n",
      "epoch 151: loss=0.8877636790275574\n",
      "epoch 152: loss=0.8879901766777039\n",
      "epoch 153: loss=0.8875880241394043\n",
      "epoch 154: loss=0.885765552520752\n",
      "epoch 155: loss=0.8861785531044006\n",
      "epoch 156: loss=0.8855593204498291\n",
      "epoch 157: loss=0.884644091129303\n",
      "epoch 158: loss=0.885531485080719\n",
      "epoch 159: loss=0.8841046094894409\n",
      "epoch 160: loss=0.8848528265953064\n",
      "epoch 161: loss=0.8818054795265198\n",
      "epoch 162: loss=0.8822557330131531\n",
      "epoch 163: loss=0.8823134899139404\n",
      "epoch 164: loss=0.8810713887214661\n",
      "epoch 165: loss=0.881767749786377\n",
      "epoch 166: loss=0.8811396360397339\n",
      "epoch 167: loss=0.8829259872436523\n",
      "epoch 168: loss=0.8803849220275879\n",
      "epoch 169: loss=0.8812388181686401\n",
      "epoch 170: loss=0.8793647885322571\n",
      "epoch 171: loss=0.8798794746398926\n",
      "epoch 172: loss=0.8806217908859253\n",
      "epoch 173: loss=0.8779416084289551\n",
      "epoch 174: loss=0.878608763217926\n",
      "epoch 175: loss=0.8792317509651184\n",
      "epoch 176: loss=0.8779923319816589\n",
      "epoch 177: loss=0.880224347114563\n",
      "epoch 178: loss=0.8770034313201904\n",
      "epoch 179: loss=0.8774238228797913\n",
      "epoch 180: loss=0.8767594695091248\n",
      "epoch 181: loss=0.8773044347763062\n",
      "epoch 182: loss=0.880244791507721\n",
      "epoch 183: loss=0.8792046308517456\n",
      "epoch 184: loss=0.8780079483985901\n",
      "epoch 185: loss=0.8745790123939514\n",
      "epoch 186: loss=0.8764496445655823\n",
      "epoch 187: loss=0.8756736516952515\n",
      "epoch 188: loss=0.87713623046875\n",
      "epoch 189: loss=0.8758875131607056\n",
      "epoch 190: loss=0.8762720823287964\n",
      "epoch 191: loss=0.8756241798400879\n",
      "epoch 192: loss=0.8754212856292725\n",
      "epoch 193: loss=0.8760713338851929\n",
      "epoch 194: loss=0.8752857446670532\n",
      "epoch 195: loss=0.876466691493988\n",
      "epoch 196: loss=0.874793529510498\n",
      "epoch 197: loss=0.8760020732879639\n",
      "epoch 198: loss=0.8753729462623596\n",
      "epoch 199: loss=0.8742393255233765\n",
      "training patch with 82345 edges\n",
      "epoch 0: loss=18.17661476135254\n",
      "epoch 1: loss=17.954727172851562\n",
      "epoch 2: loss=17.267436981201172\n",
      "epoch 3: loss=16.33503532409668\n",
      "epoch 4: loss=15.21201229095459\n",
      "epoch 5: loss=13.709596633911133\n",
      "epoch 6: loss=13.037762641906738\n",
      "epoch 7: loss=13.958876609802246\n",
      "epoch 8: loss=14.653257369995117\n",
      "epoch 9: loss=14.501523971557617\n",
      "epoch 10: loss=13.334120750427246\n",
      "epoch 11: loss=11.826353073120117\n",
      "epoch 12: loss=10.229602813720703\n",
      "epoch 13: loss=9.077147483825684\n",
      "epoch 14: loss=8.465824127197266\n",
      "epoch 15: loss=8.030844688415527\n",
      "epoch 16: loss=7.731945514678955\n",
      "epoch 17: loss=7.301051616668701\n",
      "epoch 18: loss=6.867746829986572\n",
      "epoch 19: loss=6.351590156555176\n",
      "epoch 20: loss=5.955804824829102\n",
      "epoch 21: loss=5.567045211791992\n",
      "epoch 22: loss=5.150339126586914\n",
      "epoch 23: loss=4.792296409606934\n",
      "epoch 24: loss=4.313773155212402\n",
      "epoch 25: loss=3.9293270111083984\n",
      "epoch 26: loss=3.539813756942749\n",
      "epoch 27: loss=3.2683422565460205\n",
      "epoch 28: loss=2.9605281352996826\n",
      "epoch 29: loss=2.8089981079101562\n",
      "epoch 30: loss=2.624262571334839\n",
      "epoch 31: loss=2.42112398147583\n",
      "epoch 32: loss=2.169713258743286\n",
      "epoch 33: loss=1.9938021898269653\n",
      "epoch 34: loss=1.839988350868225\n",
      "epoch 35: loss=1.7416801452636719\n",
      "epoch 36: loss=1.6497584581375122\n",
      "epoch 37: loss=1.5939815044403076\n",
      "epoch 38: loss=1.5182147026062012\n",
      "epoch 39: loss=1.4332506656646729\n",
      "epoch 40: loss=1.3591866493225098\n",
      "epoch 41: loss=1.3157716989517212\n",
      "epoch 42: loss=1.2839488983154297\n",
      "epoch 43: loss=1.2618072032928467\n",
      "epoch 44: loss=1.2283681631088257\n",
      "epoch 45: loss=1.2030260562896729\n",
      "epoch 46: loss=1.1871904134750366\n",
      "epoch 47: loss=1.1699622869491577\n",
      "epoch 48: loss=1.1541283130645752\n",
      "epoch 49: loss=1.1479090452194214\n",
      "epoch 50: loss=1.130306601524353\n",
      "epoch 51: loss=1.1228160858154297\n",
      "epoch 52: loss=1.123906135559082\n",
      "epoch 53: loss=1.1185505390167236\n",
      "epoch 54: loss=1.1134995222091675\n",
      "epoch 55: loss=1.1016205549240112\n",
      "epoch 56: loss=1.100066900253296\n",
      "epoch 57: loss=1.0987554788589478\n",
      "epoch 58: loss=1.0912652015686035\n",
      "epoch 59: loss=1.089512825012207\n",
      "epoch 60: loss=1.0889095067977905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61: loss=1.0850619077682495\n",
      "epoch 62: loss=1.0869355201721191\n",
      "epoch 63: loss=1.0822466611862183\n",
      "epoch 64: loss=1.078847050666809\n",
      "epoch 65: loss=1.0820269584655762\n",
      "epoch 66: loss=1.0783097743988037\n",
      "epoch 67: loss=1.0781906843185425\n",
      "epoch 68: loss=1.076481819152832\n",
      "epoch 69: loss=1.0752593278884888\n",
      "epoch 70: loss=1.0733009576797485\n",
      "epoch 71: loss=1.070915699005127\n",
      "epoch 72: loss=1.0713567733764648\n",
      "epoch 73: loss=1.0700535774230957\n",
      "epoch 74: loss=1.067845344543457\n",
      "epoch 75: loss=1.0657130479812622\n",
      "epoch 76: loss=1.0616319179534912\n",
      "epoch 77: loss=1.0617583990097046\n",
      "epoch 78: loss=1.061748743057251\n",
      "epoch 79: loss=1.0600197315216064\n",
      "epoch 80: loss=1.0582654476165771\n",
      "epoch 81: loss=1.0578465461730957\n",
      "epoch 82: loss=1.0558533668518066\n",
      "epoch 83: loss=1.0556305646896362\n",
      "epoch 84: loss=1.0540285110473633\n",
      "epoch 85: loss=1.0510255098342896\n",
      "epoch 86: loss=1.0513794422149658\n",
      "epoch 87: loss=1.0476064682006836\n",
      "epoch 88: loss=1.0471563339233398\n",
      "epoch 89: loss=1.0451362133026123\n",
      "epoch 90: loss=1.0448015928268433\n",
      "epoch 91: loss=1.0420695543289185\n",
      "epoch 92: loss=1.0439504384994507\n",
      "epoch 93: loss=1.0409178733825684\n",
      "epoch 94: loss=1.0390444993972778\n",
      "epoch 95: loss=1.0407466888427734\n",
      "epoch 96: loss=1.0413703918457031\n",
      "epoch 97: loss=1.037300705909729\n",
      "epoch 98: loss=1.0360344648361206\n",
      "epoch 99: loss=1.0362881422042847\n",
      "epoch 100: loss=1.0312700271606445\n",
      "epoch 101: loss=1.0287940502166748\n",
      "epoch 102: loss=1.0311206579208374\n",
      "epoch 103: loss=1.026002287864685\n",
      "epoch 104: loss=1.0241094827651978\n",
      "epoch 105: loss=1.0248165130615234\n",
      "epoch 106: loss=1.0220023393630981\n",
      "epoch 107: loss=1.021166443824768\n",
      "epoch 108: loss=1.021552562713623\n",
      "epoch 109: loss=1.0198745727539062\n",
      "epoch 110: loss=1.0194638967514038\n",
      "epoch 111: loss=1.0230001211166382\n",
      "epoch 112: loss=1.018146276473999\n",
      "epoch 113: loss=1.0169235467910767\n",
      "epoch 114: loss=1.0121040344238281\n",
      "epoch 115: loss=1.0168368816375732\n",
      "epoch 116: loss=1.010677695274353\n",
      "epoch 117: loss=1.0096192359924316\n",
      "epoch 118: loss=1.0103567838668823\n",
      "epoch 119: loss=1.002756118774414\n",
      "epoch 120: loss=1.0082433223724365\n",
      "epoch 121: loss=1.0055959224700928\n",
      "epoch 122: loss=1.0067698955535889\n",
      "epoch 123: loss=1.0033214092254639\n",
      "epoch 124: loss=1.0023554563522339\n",
      "epoch 125: loss=1.0030567646026611\n",
      "epoch 126: loss=1.000720739364624\n",
      "epoch 127: loss=0.9962669610977173\n",
      "epoch 128: loss=0.9956148266792297\n",
      "epoch 129: loss=0.9982663989067078\n",
      "epoch 130: loss=0.9974662065505981\n",
      "epoch 131: loss=0.9937127828598022\n",
      "epoch 132: loss=0.9984642267227173\n",
      "epoch 133: loss=0.9929152727127075\n",
      "epoch 134: loss=0.9945062398910522\n",
      "epoch 135: loss=0.9921917915344238\n",
      "epoch 136: loss=0.9932301044464111\n",
      "epoch 137: loss=0.9927900433540344\n",
      "epoch 138: loss=0.9915893077850342\n",
      "epoch 139: loss=0.9887904524803162\n",
      "epoch 140: loss=0.9914649724960327\n",
      "epoch 141: loss=0.9876055717468262\n",
      "epoch 142: loss=0.9882929921150208\n",
      "epoch 143: loss=0.9887230396270752\n",
      "epoch 144: loss=0.990549623966217\n",
      "epoch 145: loss=0.9863468408584595\n",
      "epoch 146: loss=0.9870551228523254\n",
      "epoch 147: loss=0.9833569526672363\n",
      "epoch 148: loss=0.9836032390594482\n",
      "epoch 149: loss=0.9846457839012146\n",
      "epoch 150: loss=0.9836685061454773\n",
      "epoch 151: loss=0.9835753440856934\n",
      "epoch 152: loss=0.9809833765029907\n",
      "epoch 153: loss=0.9857300519943237\n",
      "epoch 154: loss=0.9875500202178955\n",
      "epoch 155: loss=0.9941783547401428\n",
      "epoch 156: loss=0.9881776571273804\n",
      "epoch 157: loss=0.9844958782196045\n",
      "epoch 158: loss=0.9792062044143677\n",
      "epoch 159: loss=0.9831595420837402\n",
      "epoch 160: loss=0.9821032285690308\n",
      "epoch 161: loss=0.9789425730705261\n",
      "epoch 162: loss=0.9817677140235901\n",
      "epoch 163: loss=0.9843570590019226\n",
      "epoch 164: loss=0.9788867831230164\n",
      "epoch 165: loss=0.9759231805801392\n",
      "epoch 166: loss=0.9820883870124817\n",
      "epoch 167: loss=0.9763988256454468\n",
      "epoch 168: loss=0.9760807752609253\n",
      "epoch 169: loss=0.9775142073631287\n",
      "epoch 170: loss=0.975330114364624\n",
      "epoch 171: loss=0.976422131061554\n",
      "epoch 172: loss=0.976795494556427\n",
      "epoch 173: loss=0.9732475876808167\n",
      "epoch 174: loss=0.975443422794342\n",
      "epoch 175: loss=0.9724684357643127\n",
      "epoch 176: loss=0.97600257396698\n",
      "epoch 177: loss=0.9724704027175903\n",
      "epoch 178: loss=0.9741445779800415\n",
      "epoch 179: loss=0.9735326766967773\n",
      "epoch 180: loss=0.9724487066268921\n",
      "epoch 181: loss=0.9704039096832275\n",
      "epoch 182: loss=0.9718146324157715\n",
      "epoch 183: loss=0.9700065851211548\n",
      "epoch 184: loss=0.9706354737281799\n",
      "epoch 185: loss=0.9709565043449402\n",
      "epoch 186: loss=0.9724138975143433\n",
      "epoch 187: loss=0.9701406955718994\n",
      "epoch 188: loss=0.9685069918632507\n",
      "epoch 189: loss=0.9704262018203735\n",
      "epoch 190: loss=0.9679109454154968\n",
      "epoch 191: loss=0.9664034247398376\n",
      "epoch 192: loss=0.9674452543258667\n",
      "epoch 193: loss=0.9711351990699768\n",
      "epoch 194: loss=0.9716452956199646\n",
      "epoch 195: loss=0.9667338132858276\n",
      "epoch 196: loss=0.9669123888015747\n",
      "epoch 197: loss=0.9646077752113342\n",
      "epoch 198: loss=0.9648646712303162\n",
      "epoch 199: loss=0.9677154421806335\n",
      "training patch with 4117 edges\n",
      "epoch 0: loss=18.48216438293457\n",
      "epoch 1: loss=19.213600158691406\n",
      "epoch 2: loss=17.937273025512695\n",
      "epoch 3: loss=19.02305030822754\n",
      "epoch 4: loss=17.1264591217041\n",
      "epoch 5: loss=17.04599952697754\n",
      "epoch 6: loss=17.147966384887695\n",
      "epoch 7: loss=17.51869010925293\n",
      "epoch 8: loss=16.189844131469727\n",
      "epoch 9: loss=14.818788528442383\n",
      "epoch 10: loss=15.589676856994629\n",
      "epoch 11: loss=15.558977127075195\n",
      "epoch 12: loss=15.468811988830566\n",
      "epoch 13: loss=17.42053985595703\n",
      "epoch 14: loss=17.045467376708984\n",
      "epoch 15: loss=17.136796951293945\n",
      "epoch 16: loss=15.815656661987305\n",
      "epoch 17: loss=15.334065437316895\n",
      "epoch 18: loss=15.017657279968262\n",
      "epoch 19: loss=13.573962211608887\n",
      "epoch 20: loss=14.944205284118652\n",
      "epoch 21: loss=13.20311164855957\n",
      "epoch 22: loss=12.729960441589355\n",
      "epoch 23: loss=12.623652458190918\n",
      "epoch 24: loss=12.398941993713379\n",
      "epoch 25: loss=13.04406452178955\n",
      "epoch 26: loss=11.192914009094238\n",
      "epoch 27: loss=10.993690490722656\n",
      "epoch 28: loss=10.348051071166992\n",
      "epoch 29: loss=11.09009838104248\n",
      "epoch 30: loss=9.480344772338867\n",
      "epoch 31: loss=9.618773460388184\n",
      "epoch 32: loss=9.593172073364258\n",
      "epoch 33: loss=9.377395629882812\n",
      "epoch 34: loss=8.672304153442383\n",
      "epoch 35: loss=8.542680740356445\n",
      "epoch 36: loss=8.132139205932617\n",
      "epoch 37: loss=7.063243389129639\n",
      "epoch 38: loss=7.343830585479736\n",
      "epoch 39: loss=6.143606662750244\n",
      "epoch 40: loss=5.463131904602051\n",
      "epoch 41: loss=5.342983245849609\n",
      "epoch 42: loss=5.15525484085083\n",
      "epoch 43: loss=4.4152936935424805\n",
      "epoch 44: loss=3.9689908027648926\n",
      "epoch 45: loss=3.685779333114624\n",
      "epoch 46: loss=3.6070237159729004\n",
      "epoch 47: loss=3.511312484741211\n",
      "epoch 48: loss=2.956010341644287\n",
      "epoch 49: loss=3.0181994438171387\n",
      "epoch 50: loss=2.795440435409546\n",
      "epoch 51: loss=2.7813913822174072\n",
      "epoch 52: loss=2.6194443702697754\n",
      "epoch 53: loss=2.4283390045166016\n",
      "epoch 54: loss=2.372328519821167\n",
      "epoch 55: loss=2.248891830444336\n",
      "epoch 56: loss=2.1152896881103516\n",
      "epoch 57: loss=2.222012996673584\n",
      "epoch 58: loss=2.133671522140503\n",
      "epoch 59: loss=2.085505962371826\n",
      "epoch 60: loss=2.1615164279937744\n",
      "epoch 61: loss=2.016934394836426\n",
      "epoch 62: loss=1.9095851182937622\n",
      "epoch 63: loss=2.134577512741089\n",
      "epoch 64: loss=1.9871666431427002\n",
      "epoch 65: loss=2.01867938041687\n",
      "epoch 66: loss=1.8406099081039429\n",
      "epoch 67: loss=1.9369481801986694\n",
      "epoch 68: loss=2.0193862915039062\n",
      "epoch 69: loss=1.8679258823394775\n",
      "epoch 70: loss=1.9839116334915161\n",
      "epoch 71: loss=1.924496054649353\n",
      "epoch 72: loss=1.9468255043029785\n",
      "epoch 73: loss=1.8419160842895508\n",
      "epoch 74: loss=1.890960693359375\n",
      "epoch 75: loss=1.9295613765716553\n",
      "epoch 76: loss=1.914453148841858\n",
      "epoch 77: loss=1.866911768913269\n",
      "epoch 78: loss=1.8177738189697266\n",
      "epoch 79: loss=1.9424686431884766\n",
      "epoch 80: loss=1.7637724876403809\n",
      "epoch 81: loss=1.8830960988998413\n",
      "epoch 82: loss=1.8180053234100342\n",
      "epoch 83: loss=1.871936321258545\n",
      "epoch 84: loss=1.8256272077560425\n",
      "epoch 85: loss=1.9245843887329102\n",
      "epoch 86: loss=1.9492466449737549\n",
      "epoch 87: loss=1.8599536418914795\n",
      "epoch 88: loss=1.876882553100586\n",
      "epoch 89: loss=1.8554826974868774\n",
      "epoch 90: loss=1.8490238189697266\n",
      "epoch 91: loss=1.8872923851013184\n",
      "epoch 92: loss=1.8205349445343018\n",
      "epoch 93: loss=1.8933653831481934\n",
      "epoch 94: loss=1.815754771232605\n",
      "epoch 95: loss=1.8213393688201904\n",
      "epoch 96: loss=1.7958707809448242\n",
      "epoch 97: loss=1.855224609375\n",
      "epoch 98: loss=1.8257246017456055\n",
      "epoch 99: loss=1.8145047426223755\n",
      "epoch 100: loss=1.8185855150222778\n",
      "epoch 101: loss=1.8210171461105347\n",
      "epoch 102: loss=1.802225947380066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103: loss=1.8332432508468628\n",
      "epoch 104: loss=1.7890818119049072\n",
      "epoch 105: loss=1.7537957429885864\n",
      "epoch 106: loss=1.810968041419983\n",
      "epoch 107: loss=1.7837218046188354\n",
      "epoch 108: loss=1.8274943828582764\n",
      "epoch 109: loss=1.841336727142334\n",
      "epoch 110: loss=1.7417949438095093\n",
      "epoch 111: loss=1.7931896448135376\n",
      "epoch 112: loss=1.8753905296325684\n",
      "epoch 113: loss=1.7779048681259155\n",
      "epoch 114: loss=1.7760021686553955\n",
      "epoch 115: loss=1.855629324913025\n",
      "epoch 116: loss=1.8563055992126465\n",
      "epoch 117: loss=1.7550404071807861\n",
      "epoch 118: loss=1.804048776626587\n",
      "epoch 119: loss=1.776323914527893\n",
      "epoch 120: loss=1.8522089719772339\n",
      "epoch 121: loss=1.8200262784957886\n",
      "epoch 122: loss=1.8503557443618774\n",
      "epoch 123: loss=1.785846471786499\n",
      "epoch 124: loss=1.8170087337493896\n",
      "epoch 125: loss=1.8979854583740234\n",
      "epoch 126: loss=1.7779923677444458\n",
      "epoch 127: loss=1.8175420761108398\n",
      "epoch 128: loss=1.7721917629241943\n",
      "epoch 129: loss=1.8143839836120605\n",
      "epoch 130: loss=1.7889702320098877\n",
      "epoch 131: loss=1.7878767251968384\n",
      "epoch 132: loss=1.7673343420028687\n",
      "epoch 133: loss=1.8124449253082275\n",
      "epoch 134: loss=1.6875869035720825\n",
      "epoch 135: loss=1.7650824785232544\n",
      "epoch 136: loss=1.735430121421814\n",
      "epoch 137: loss=1.7290019989013672\n",
      "epoch 138: loss=1.7377257347106934\n",
      "epoch 139: loss=1.8279507160186768\n",
      "epoch 140: loss=1.739518404006958\n",
      "epoch 141: loss=1.745017409324646\n",
      "epoch 142: loss=1.681158185005188\n",
      "epoch 143: loss=1.817412257194519\n",
      "epoch 144: loss=1.867097020149231\n",
      "epoch 145: loss=1.748939871788025\n",
      "epoch 146: loss=1.7848563194274902\n",
      "epoch 147: loss=1.7255117893218994\n",
      "epoch 148: loss=1.721035361289978\n",
      "epoch 149: loss=1.793824315071106\n",
      "epoch 150: loss=1.7859567403793335\n",
      "epoch 151: loss=1.7972462177276611\n",
      "epoch 152: loss=1.7207635641098022\n",
      "epoch 153: loss=1.7260860204696655\n",
      "epoch 154: loss=1.7983568906784058\n",
      "epoch 155: loss=1.8039748668670654\n",
      "epoch 156: loss=1.8019704818725586\n",
      "epoch 157: loss=1.7359274625778198\n",
      "epoch 158: loss=1.7444865703582764\n",
      "epoch 159: loss=1.767885684967041\n",
      "epoch 160: loss=1.7723537683486938\n",
      "epoch 161: loss=1.7670406103134155\n",
      "epoch 162: loss=1.776673436164856\n",
      "epoch 163: loss=1.6936501264572144\n",
      "epoch 164: loss=1.8174899816513062\n",
      "epoch 165: loss=1.7422677278518677\n",
      "epoch 166: loss=1.7571808099746704\n",
      "epoch 167: loss=1.7079582214355469\n",
      "epoch 168: loss=1.7299695014953613\n",
      "epoch 169: loss=1.826774001121521\n",
      "epoch 170: loss=1.7602369785308838\n",
      "epoch 171: loss=1.8306877613067627\n",
      "epoch 172: loss=1.673234224319458\n",
      "epoch 173: loss=1.7612736225128174\n",
      "epoch 174: loss=1.813753366470337\n",
      "epoch 175: loss=1.7569849491119385\n",
      "epoch 176: loss=1.7493642568588257\n",
      "epoch 177: loss=1.7240684032440186\n",
      "epoch 178: loss=1.7056809663772583\n",
      "epoch 179: loss=1.8502280712127686\n",
      "epoch 180: loss=1.7632818222045898\n",
      "epoch 181: loss=1.746293306350708\n",
      "epoch 182: loss=1.7304106950759888\n",
      "epoch 183: loss=1.743560552597046\n",
      "epoch 184: loss=1.7003008127212524\n",
      "epoch 185: loss=1.7738748788833618\n",
      "epoch 186: loss=1.7287006378173828\n",
      "epoch 187: loss=1.7521904706954956\n",
      "epoch 188: loss=1.6891422271728516\n",
      "epoch 189: loss=1.8017727136611938\n",
      "epoch 190: loss=1.7510178089141846\n",
      "epoch 191: loss=1.728354573249817\n",
      "epoch 192: loss=1.7648004293441772\n",
      "epoch 193: loss=1.7931029796600342\n",
      "epoch 194: loss=1.8204519748687744\n",
      "epoch 195: loss=1.7551634311676025\n",
      "epoch 196: loss=1.7109805345535278\n",
      "epoch 197: loss=1.8052319288253784\n",
      "epoch 198: loss=1.7591407299041748\n",
      "epoch 199: loss=1.7824139595031738\n",
      "training patch with 313818 edges\n",
      "epoch 0: loss=18.3771915435791\n",
      "epoch 1: loss=18.003328323364258\n",
      "epoch 2: loss=17.425466537475586\n",
      "epoch 3: loss=16.392986297607422\n",
      "epoch 4: loss=14.715166091918945\n",
      "epoch 5: loss=13.062703132629395\n",
      "epoch 6: loss=12.503031730651855\n",
      "epoch 7: loss=13.218910217285156\n",
      "epoch 8: loss=13.876608848571777\n",
      "epoch 9: loss=13.823687553405762\n",
      "epoch 10: loss=13.144608497619629\n",
      "epoch 11: loss=12.143515586853027\n",
      "epoch 12: loss=11.036537170410156\n",
      "epoch 13: loss=9.894856452941895\n",
      "epoch 14: loss=9.083052635192871\n",
      "epoch 15: loss=8.41043472290039\n",
      "epoch 16: loss=8.023957252502441\n",
      "epoch 17: loss=7.740423679351807\n",
      "epoch 18: loss=7.562952995300293\n",
      "epoch 19: loss=7.278261184692383\n",
      "epoch 20: loss=7.064921855926514\n",
      "epoch 21: loss=6.719943523406982\n",
      "epoch 22: loss=6.355473518371582\n",
      "epoch 23: loss=6.1052632331848145\n",
      "epoch 24: loss=5.77717924118042\n",
      "epoch 25: loss=5.429836273193359\n",
      "epoch 26: loss=5.099503993988037\n",
      "epoch 27: loss=4.735901355743408\n",
      "epoch 28: loss=4.379636764526367\n",
      "epoch 29: loss=4.041049957275391\n",
      "epoch 30: loss=3.720804214477539\n",
      "epoch 31: loss=3.477433681488037\n",
      "epoch 32: loss=3.236543893814087\n",
      "epoch 33: loss=3.0211522579193115\n",
      "epoch 34: loss=2.7738592624664307\n",
      "epoch 35: loss=2.5341291427612305\n",
      "epoch 36: loss=2.2939021587371826\n",
      "epoch 37: loss=2.0893166065216064\n",
      "epoch 38: loss=1.9117765426635742\n",
      "epoch 39: loss=1.7708224058151245\n",
      "epoch 40: loss=1.6571422815322876\n",
      "epoch 41: loss=1.5424948930740356\n",
      "epoch 42: loss=1.442469596862793\n",
      "epoch 43: loss=1.35404634475708\n",
      "epoch 44: loss=1.2903674840927124\n",
      "epoch 45: loss=1.2285267114639282\n",
      "epoch 46: loss=1.1860456466674805\n",
      "epoch 47: loss=1.1427682638168335\n",
      "epoch 48: loss=1.0995463132858276\n",
      "epoch 49: loss=1.073028564453125\n",
      "epoch 50: loss=1.0542658567428589\n",
      "epoch 51: loss=1.0328428745269775\n",
      "epoch 52: loss=1.01512610912323\n",
      "epoch 53: loss=0.9979344010353088\n",
      "epoch 54: loss=0.988213062286377\n",
      "epoch 55: loss=0.973942756652832\n",
      "epoch 56: loss=0.9651402831077576\n",
      "epoch 57: loss=0.9571901559829712\n",
      "epoch 58: loss=0.9506319761276245\n",
      "epoch 59: loss=0.9457811713218689\n",
      "epoch 60: loss=0.937698483467102\n",
      "epoch 61: loss=0.9345616102218628\n",
      "epoch 62: loss=0.9319936037063599\n",
      "epoch 63: loss=0.9314600825309753\n",
      "epoch 64: loss=0.9288244247436523\n",
      "epoch 65: loss=0.9271297454833984\n",
      "epoch 66: loss=0.9251957535743713\n",
      "epoch 67: loss=0.9207082986831665\n",
      "epoch 68: loss=0.9205588102340698\n",
      "epoch 69: loss=0.9176038503646851\n",
      "epoch 70: loss=0.9171772003173828\n",
      "epoch 71: loss=0.9154351949691772\n",
      "epoch 72: loss=0.9152917265892029\n",
      "epoch 73: loss=0.9129352569580078\n",
      "epoch 74: loss=0.9126700758934021\n",
      "epoch 75: loss=0.9111737012863159\n",
      "epoch 76: loss=0.9112004041671753\n",
      "epoch 77: loss=0.9091960787773132\n",
      "epoch 78: loss=0.9087490439414978\n",
      "epoch 79: loss=0.9075161218643188\n",
      "epoch 80: loss=0.9084132313728333\n",
      "epoch 81: loss=0.9078951478004456\n",
      "epoch 82: loss=0.9060819149017334\n",
      "epoch 83: loss=0.905804455280304\n",
      "epoch 84: loss=0.9059851169586182\n",
      "epoch 85: loss=0.9036459922790527\n",
      "epoch 86: loss=0.9033672213554382\n",
      "epoch 87: loss=0.9043941497802734\n",
      "epoch 88: loss=0.9023268222808838\n",
      "epoch 89: loss=0.9028937220573425\n",
      "epoch 90: loss=0.9019129872322083\n",
      "epoch 91: loss=0.9018089771270752\n",
      "epoch 92: loss=0.9005693197250366\n",
      "epoch 93: loss=0.9008004665374756\n",
      "epoch 94: loss=0.9005730152130127\n",
      "epoch 95: loss=0.8990663886070251\n",
      "epoch 96: loss=0.8983946442604065\n",
      "epoch 97: loss=0.8996383547782898\n",
      "epoch 98: loss=0.8981369733810425\n",
      "epoch 99: loss=0.8979519009590149\n",
      "epoch 100: loss=0.8973080515861511\n",
      "epoch 101: loss=0.897846519947052\n",
      "epoch 102: loss=0.8964943289756775\n",
      "epoch 103: loss=0.8968654870986938\n",
      "epoch 104: loss=0.8968564867973328\n",
      "epoch 105: loss=0.8965105414390564\n",
      "epoch 106: loss=0.8947694897651672\n",
      "epoch 107: loss=0.89397132396698\n",
      "epoch 108: loss=0.8941276669502258\n",
      "epoch 109: loss=0.895351767539978\n",
      "epoch 110: loss=0.8944391012191772\n",
      "epoch 111: loss=0.8932390213012695\n",
      "epoch 112: loss=0.8943688273429871\n",
      "epoch 113: loss=0.8912522792816162\n",
      "epoch 114: loss=0.8941032290458679\n",
      "epoch 115: loss=0.8922788500785828\n",
      "epoch 116: loss=0.8927597999572754\n",
      "epoch 117: loss=0.8930093050003052\n",
      "epoch 118: loss=0.893031895160675\n",
      "epoch 119: loss=0.891952395439148\n",
      "epoch 120: loss=0.8919901251792908\n",
      "epoch 121: loss=0.890688955783844\n",
      "epoch 122: loss=0.8905771970748901\n",
      "epoch 123: loss=0.8881287574768066\n",
      "epoch 124: loss=0.889047384262085\n",
      "epoch 125: loss=0.888635516166687\n",
      "epoch 126: loss=0.890335738658905\n",
      "epoch 127: loss=0.8888381719589233\n",
      "epoch 128: loss=0.888614296913147\n",
      "epoch 129: loss=0.8874486684799194\n",
      "epoch 130: loss=0.8871456980705261\n",
      "epoch 131: loss=0.8872104287147522\n",
      "epoch 132: loss=0.8878882527351379\n",
      "epoch 133: loss=0.8858062028884888\n",
      "epoch 134: loss=0.8858059644699097\n",
      "epoch 135: loss=0.8842172622680664\n",
      "epoch 136: loss=0.8848491907119751\n",
      "epoch 137: loss=0.8845425248146057\n",
      "epoch 138: loss=0.8842669725418091\n",
      "epoch 139: loss=0.8839855194091797\n",
      "epoch 140: loss=0.8823785781860352\n",
      "epoch 141: loss=0.8833073973655701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 142: loss=0.8824689388275146\n",
      "epoch 143: loss=0.8817823529243469\n",
      "epoch 144: loss=0.8819003105163574\n",
      "epoch 145: loss=0.879869282245636\n",
      "epoch 146: loss=0.8790915608406067\n",
      "epoch 147: loss=0.878854513168335\n",
      "epoch 148: loss=0.8796056509017944\n",
      "epoch 149: loss=0.880013108253479\n",
      "epoch 150: loss=0.8787721395492554\n",
      "epoch 151: loss=0.8798834681510925\n",
      "epoch 152: loss=0.8778255581855774\n",
      "epoch 153: loss=0.8773062229156494\n",
      "epoch 154: loss=0.8766587972640991\n",
      "epoch 155: loss=0.8761302828788757\n",
      "epoch 156: loss=0.8774183392524719\n",
      "epoch 157: loss=0.8765671253204346\n",
      "epoch 158: loss=0.8756293654441833\n",
      "epoch 159: loss=0.8747625350952148\n",
      "epoch 160: loss=0.8747905492782593\n",
      "epoch 161: loss=0.874722957611084\n",
      "epoch 162: loss=0.8735686540603638\n",
      "epoch 163: loss=0.8744795322418213\n",
      "epoch 164: loss=0.8741768598556519\n",
      "epoch 165: loss=0.8741112947463989\n",
      "epoch 166: loss=0.8713508248329163\n",
      "epoch 167: loss=0.8721816539764404\n",
      "epoch 168: loss=0.8723974823951721\n",
      "epoch 169: loss=0.8699759840965271\n",
      "epoch 170: loss=0.8714748620986938\n",
      "epoch 171: loss=0.8721939325332642\n",
      "epoch 172: loss=0.8715066313743591\n",
      "epoch 173: loss=0.8700249791145325\n",
      "epoch 174: loss=0.8709192872047424\n",
      "epoch 175: loss=0.8689922094345093\n",
      "epoch 176: loss=0.8685644865036011\n",
      "epoch 177: loss=0.869441032409668\n",
      "epoch 178: loss=0.8700202107429504\n",
      "epoch 179: loss=0.8699625730514526\n",
      "epoch 180: loss=0.8682390451431274\n",
      "epoch 181: loss=0.8682838082313538\n",
      "epoch 182: loss=0.8682419657707214\n",
      "epoch 183: loss=0.867694616317749\n",
      "epoch 184: loss=0.868180513381958\n",
      "epoch 185: loss=0.866956353187561\n",
      "epoch 186: loss=0.8662865161895752\n",
      "epoch 187: loss=0.8669067621231079\n",
      "epoch 188: loss=0.8658182621002197\n",
      "epoch 189: loss=0.8665142059326172\n",
      "epoch 190: loss=0.8661922216415405\n",
      "epoch 191: loss=0.8656406998634338\n",
      "epoch 192: loss=0.8652536273002625\n",
      "epoch 193: loss=0.8662310242652893\n",
      "epoch 194: loss=0.8640092015266418\n",
      "epoch 195: loss=0.8639358282089233\n",
      "epoch 196: loss=0.8653698563575745\n",
      "epoch 197: loss=0.8639414310455322\n",
      "epoch 198: loss=0.8623930215835571\n",
      "epoch 199: loss=0.8630931377410889\n",
      "training patch with 218347 edges\n",
      "epoch 0: loss=18.281291961669922\n",
      "epoch 1: loss=17.93329429626465\n",
      "epoch 2: loss=17.284570693969727\n",
      "epoch 3: loss=16.30586051940918\n",
      "epoch 4: loss=14.675987243652344\n",
      "epoch 5: loss=12.970874786376953\n",
      "epoch 6: loss=12.413044929504395\n",
      "epoch 7: loss=13.020971298217773\n",
      "epoch 8: loss=13.467144966125488\n",
      "epoch 9: loss=13.124835968017578\n",
      "epoch 10: loss=12.190996170043945\n",
      "epoch 11: loss=10.980175971984863\n",
      "epoch 12: loss=9.776185989379883\n",
      "epoch 13: loss=8.780292510986328\n",
      "epoch 14: loss=7.976990699768066\n",
      "epoch 15: loss=7.464983940124512\n",
      "epoch 16: loss=7.155380725860596\n",
      "epoch 17: loss=6.905999183654785\n",
      "epoch 18: loss=6.687004089355469\n",
      "epoch 19: loss=6.4513349533081055\n",
      "epoch 20: loss=6.098926544189453\n",
      "epoch 21: loss=5.794339656829834\n",
      "epoch 22: loss=5.376968860626221\n",
      "epoch 23: loss=5.001254081726074\n",
      "epoch 24: loss=4.570777416229248\n",
      "epoch 25: loss=4.111398220062256\n",
      "epoch 26: loss=3.721292495727539\n",
      "epoch 27: loss=3.3744421005249023\n",
      "epoch 28: loss=3.0703842639923096\n",
      "epoch 29: loss=2.8055171966552734\n",
      "epoch 30: loss=2.582885265350342\n",
      "epoch 31: loss=2.3687632083892822\n",
      "epoch 32: loss=2.138162612915039\n",
      "epoch 33: loss=1.9550021886825562\n",
      "epoch 34: loss=1.7871066331863403\n",
      "epoch 35: loss=1.665751576423645\n",
      "epoch 36: loss=1.5406687259674072\n",
      "epoch 37: loss=1.4350385665893555\n",
      "epoch 38: loss=1.3438016176223755\n",
      "epoch 39: loss=1.2770648002624512\n",
      "epoch 40: loss=1.2096775770187378\n",
      "epoch 41: loss=1.1697611808776855\n",
      "epoch 42: loss=1.1301302909851074\n",
      "epoch 43: loss=1.1124869585037231\n",
      "epoch 44: loss=1.0869359970092773\n",
      "epoch 45: loss=1.059181809425354\n",
      "epoch 46: loss=1.0320476293563843\n",
      "epoch 47: loss=1.0148545503616333\n",
      "epoch 48: loss=1.0063971281051636\n",
      "epoch 49: loss=1.0052525997161865\n",
      "epoch 50: loss=1.0002644062042236\n",
      "epoch 51: loss=0.9924293160438538\n",
      "epoch 52: loss=0.9807204008102417\n",
      "epoch 53: loss=0.9729319214820862\n",
      "epoch 54: loss=0.9659599661827087\n",
      "epoch 55: loss=0.9610593318939209\n",
      "epoch 56: loss=0.9605746269226074\n",
      "epoch 57: loss=0.9598654508590698\n",
      "epoch 58: loss=0.9509716629981995\n",
      "epoch 59: loss=0.9441508650779724\n",
      "epoch 60: loss=0.9386733770370483\n",
      "epoch 61: loss=0.937561571598053\n",
      "epoch 62: loss=0.937451183795929\n",
      "epoch 63: loss=0.9324483871459961\n",
      "epoch 64: loss=0.9285452365875244\n",
      "epoch 65: loss=0.9282405972480774\n",
      "epoch 66: loss=0.9256407618522644\n",
      "epoch 67: loss=0.9216081500053406\n",
      "epoch 68: loss=0.9218950867652893\n",
      "epoch 69: loss=0.9200895428657532\n",
      "epoch 70: loss=0.9199054837226868\n",
      "epoch 71: loss=0.9174172878265381\n",
      "epoch 72: loss=0.9162430763244629\n",
      "epoch 73: loss=0.9136326313018799\n",
      "epoch 74: loss=0.9142070412635803\n",
      "epoch 75: loss=0.9115039706230164\n",
      "epoch 76: loss=0.9109525680541992\n",
      "epoch 77: loss=0.910239577293396\n",
      "epoch 78: loss=0.9087471961975098\n",
      "epoch 79: loss=0.9082701206207275\n",
      "epoch 80: loss=0.9069321751594543\n",
      "epoch 81: loss=0.9066890478134155\n",
      "epoch 82: loss=0.9056365489959717\n",
      "epoch 83: loss=0.906152606010437\n",
      "epoch 84: loss=0.9026678800582886\n",
      "epoch 85: loss=0.903731644153595\n",
      "epoch 86: loss=0.9028875827789307\n",
      "epoch 87: loss=0.9025977849960327\n",
      "epoch 88: loss=0.903016984462738\n",
      "epoch 89: loss=0.9010069370269775\n",
      "epoch 90: loss=0.899887204170227\n",
      "epoch 91: loss=0.9009643793106079\n",
      "epoch 92: loss=0.8972441554069519\n",
      "epoch 93: loss=0.8972795009613037\n",
      "epoch 94: loss=0.8970502614974976\n",
      "epoch 95: loss=0.8965270519256592\n",
      "epoch 96: loss=0.897716760635376\n",
      "epoch 97: loss=0.8948023319244385\n",
      "epoch 98: loss=0.8966267108917236\n",
      "epoch 99: loss=0.8952215313911438\n",
      "epoch 100: loss=0.8944278359413147\n",
      "epoch 101: loss=0.8946481347084045\n",
      "epoch 102: loss=0.8932149410247803\n",
      "epoch 103: loss=0.8937879800796509\n",
      "epoch 104: loss=0.8913782238960266\n",
      "epoch 105: loss=0.8919693231582642\n",
      "epoch 106: loss=0.8911718130111694\n",
      "epoch 107: loss=0.8922805190086365\n",
      "epoch 108: loss=0.8901042342185974\n",
      "epoch 109: loss=0.8900796175003052\n",
      "epoch 110: loss=0.8887624740600586\n",
      "epoch 111: loss=0.88936448097229\n",
      "epoch 112: loss=0.8887777328491211\n",
      "epoch 113: loss=0.8890414237976074\n",
      "epoch 114: loss=0.8889951109886169\n",
      "epoch 115: loss=0.8890081644058228\n",
      "epoch 116: loss=0.8868118524551392\n",
      "epoch 117: loss=0.8870229125022888\n",
      "epoch 118: loss=0.888353705406189\n",
      "epoch 119: loss=0.8849847316741943\n",
      "epoch 120: loss=0.8855419754981995\n",
      "epoch 121: loss=0.8838508725166321\n",
      "epoch 122: loss=0.8851400017738342\n",
      "epoch 123: loss=0.885508120059967\n",
      "epoch 124: loss=0.8850222229957581\n",
      "epoch 125: loss=0.8841472268104553\n",
      "epoch 126: loss=0.8858919739723206\n",
      "epoch 127: loss=0.8837798237800598\n",
      "epoch 128: loss=0.8816108107566833\n",
      "epoch 129: loss=0.8828569650650024\n",
      "epoch 130: loss=0.8832467794418335\n",
      "epoch 131: loss=0.8809435963630676\n",
      "epoch 132: loss=0.8812820315361023\n",
      "epoch 133: loss=0.8798219561576843\n",
      "epoch 134: loss=0.8812358975410461\n",
      "epoch 135: loss=0.8816153407096863\n",
      "epoch 136: loss=0.8787238597869873\n",
      "epoch 137: loss=0.8777098655700684\n",
      "epoch 138: loss=0.8791731595993042\n",
      "epoch 139: loss=0.8788919448852539\n",
      "epoch 140: loss=0.8765696287155151\n",
      "epoch 141: loss=0.8782373666763306\n",
      "epoch 142: loss=0.8756459355354309\n",
      "epoch 143: loss=0.8767855167388916\n",
      "epoch 144: loss=0.8761436939239502\n",
      "epoch 145: loss=0.8752692341804504\n",
      "epoch 146: loss=0.8769296407699585\n",
      "epoch 147: loss=0.8765522837638855\n",
      "epoch 148: loss=0.8746852874755859\n",
      "epoch 149: loss=0.8758164644241333\n",
      "epoch 150: loss=0.8741176724433899\n",
      "epoch 151: loss=0.8744203448295593\n",
      "epoch 152: loss=0.8731482625007629\n",
      "epoch 153: loss=0.8740116357803345\n",
      "epoch 154: loss=0.8745654225349426\n",
      "epoch 155: loss=0.8730866312980652\n",
      "epoch 156: loss=0.8711438775062561\n",
      "epoch 157: loss=0.8726210594177246\n",
      "epoch 158: loss=0.8703590631484985\n",
      "epoch 159: loss=0.8719925880432129\n",
      "epoch 160: loss=0.8717576265335083\n",
      "epoch 161: loss=0.8717552423477173\n",
      "epoch 162: loss=0.869221568107605\n",
      "epoch 163: loss=0.8703958988189697\n",
      "epoch 164: loss=0.8706878423690796\n",
      "epoch 165: loss=0.8703734874725342\n",
      "epoch 166: loss=0.8684543371200562\n",
      "epoch 167: loss=0.868135392665863\n",
      "epoch 168: loss=0.8680136799812317\n",
      "epoch 169: loss=0.8686783313751221\n",
      "epoch 170: loss=0.8674702048301697\n",
      "epoch 171: loss=0.8659690022468567\n",
      "epoch 172: loss=0.8663977384567261\n",
      "epoch 173: loss=0.8682408928871155\n",
      "epoch 174: loss=0.8660138845443726\n",
      "epoch 175: loss=0.8685158491134644\n",
      "epoch 176: loss=0.866137683391571\n",
      "epoch 177: loss=0.8658890128135681\n",
      "epoch 178: loss=0.8633950352668762\n",
      "epoch 179: loss=0.8650135397911072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 180: loss=0.8640416264533997\n",
      "epoch 181: loss=0.8664530515670776\n",
      "epoch 182: loss=0.8644198775291443\n",
      "epoch 183: loss=0.8640026450157166\n",
      "epoch 184: loss=0.8642069101333618\n",
      "epoch 185: loss=0.8647116422653198\n",
      "epoch 186: loss=0.8625513315200806\n",
      "epoch 187: loss=0.8649718761444092\n",
      "epoch 188: loss=0.8631384968757629\n",
      "epoch 189: loss=0.8624340891838074\n",
      "epoch 190: loss=0.8637607097625732\n",
      "epoch 191: loss=0.8621153235435486\n",
      "epoch 192: loss=0.8626225590705872\n",
      "epoch 193: loss=0.8622834086418152\n",
      "epoch 194: loss=0.8637691140174866\n",
      "epoch 195: loss=0.8613892197608948\n",
      "epoch 196: loss=0.8605601787567139\n",
      "epoch 197: loss=0.8607237339019775\n",
      "epoch 198: loss=0.8596939444541931\n",
      "epoch 199: loss=0.8615379333496094\n",
      "training patch with 138796 edges\n",
      "epoch 0: loss=18.220314025878906\n",
      "epoch 1: loss=17.87268829345703\n",
      "epoch 2: loss=17.266563415527344\n",
      "epoch 3: loss=16.479236602783203\n",
      "epoch 4: loss=15.051641464233398\n",
      "epoch 5: loss=13.311851501464844\n",
      "epoch 6: loss=12.533525466918945\n",
      "epoch 7: loss=13.235837936401367\n",
      "epoch 8: loss=13.960027694702148\n",
      "epoch 9: loss=13.906706809997559\n",
      "epoch 10: loss=13.054727554321289\n",
      "epoch 11: loss=11.807177543640137\n",
      "epoch 12: loss=10.373056411743164\n",
      "epoch 13: loss=9.160948753356934\n",
      "epoch 14: loss=8.141159057617188\n",
      "epoch 15: loss=7.645809173583984\n",
      "epoch 16: loss=7.333375453948975\n",
      "epoch 17: loss=7.1408772468566895\n",
      "epoch 18: loss=6.767292499542236\n",
      "epoch 19: loss=6.472684383392334\n",
      "epoch 20: loss=6.134309768676758\n",
      "epoch 21: loss=5.838349342346191\n",
      "epoch 22: loss=5.505359172821045\n",
      "epoch 23: loss=4.9685540199279785\n",
      "epoch 24: loss=4.56025505065918\n",
      "epoch 25: loss=4.173311233520508\n",
      "epoch 26: loss=3.7852730751037598\n",
      "epoch 27: loss=3.4916555881500244\n",
      "epoch 28: loss=3.225241184234619\n",
      "epoch 29: loss=3.0098140239715576\n",
      "epoch 30: loss=2.8440144062042236\n",
      "epoch 31: loss=2.631800651550293\n",
      "epoch 32: loss=2.461355686187744\n",
      "epoch 33: loss=2.262188196182251\n",
      "epoch 34: loss=2.09384822845459\n",
      "epoch 35: loss=1.942439317703247\n",
      "epoch 36: loss=1.8024547100067139\n",
      "epoch 37: loss=1.7014892101287842\n",
      "epoch 38: loss=1.5819580554962158\n",
      "epoch 39: loss=1.490715742111206\n",
      "epoch 40: loss=1.4076260328292847\n",
      "epoch 41: loss=1.3469047546386719\n",
      "epoch 42: loss=1.2847976684570312\n",
      "epoch 43: loss=1.2479567527770996\n",
      "epoch 44: loss=1.2127896547317505\n",
      "epoch 45: loss=1.1741334199905396\n",
      "epoch 46: loss=1.1398911476135254\n",
      "epoch 47: loss=1.1107158660888672\n",
      "epoch 48: loss=1.0875710248947144\n",
      "epoch 49: loss=1.0693360567092896\n",
      "epoch 50: loss=1.0622135400772095\n",
      "epoch 51: loss=1.0556538105010986\n",
      "epoch 52: loss=1.0421842336654663\n",
      "epoch 53: loss=1.0254497528076172\n",
      "epoch 54: loss=1.0190516710281372\n",
      "epoch 55: loss=1.0134565830230713\n",
      "epoch 56: loss=1.0099753141403198\n",
      "epoch 57: loss=1.0035719871520996\n",
      "epoch 58: loss=1.0040334463119507\n",
      "epoch 59: loss=0.9978224039077759\n",
      "epoch 60: loss=0.9933971762657166\n",
      "epoch 61: loss=0.9918479919433594\n",
      "epoch 62: loss=0.9912355542182922\n",
      "epoch 63: loss=0.9895342588424683\n",
      "epoch 64: loss=0.9872347116470337\n",
      "epoch 65: loss=0.9845266938209534\n",
      "epoch 66: loss=0.9841165542602539\n",
      "epoch 67: loss=0.9846396446228027\n",
      "epoch 68: loss=0.9826897382736206\n",
      "epoch 69: loss=0.9826453924179077\n",
      "epoch 70: loss=0.981755793094635\n",
      "epoch 71: loss=0.9812434911727905\n",
      "epoch 72: loss=0.9793738126754761\n",
      "epoch 73: loss=0.9778997898101807\n",
      "epoch 74: loss=0.9768375158309937\n",
      "epoch 75: loss=0.9780199527740479\n",
      "epoch 76: loss=0.9778095483779907\n",
      "epoch 77: loss=0.9756444692611694\n",
      "epoch 78: loss=0.9773301482200623\n",
      "epoch 79: loss=0.975468635559082\n",
      "epoch 80: loss=0.9758122563362122\n",
      "epoch 81: loss=0.9757763743400574\n",
      "epoch 82: loss=0.9721235632896423\n",
      "epoch 83: loss=0.9739190936088562\n",
      "epoch 84: loss=0.9749795198440552\n",
      "epoch 85: loss=0.9737907648086548\n",
      "epoch 86: loss=0.9715496897697449\n",
      "epoch 87: loss=0.9709093570709229\n",
      "epoch 88: loss=0.9742329716682434\n",
      "epoch 89: loss=0.9730291962623596\n",
      "epoch 90: loss=0.9708518981933594\n",
      "epoch 91: loss=0.9728458523750305\n",
      "epoch 92: loss=0.9706143140792847\n",
      "epoch 93: loss=0.9717316031455994\n",
      "epoch 94: loss=0.9705474376678467\n",
      "epoch 95: loss=0.9701264500617981\n",
      "epoch 96: loss=0.9692681431770325\n",
      "epoch 97: loss=0.9705840349197388\n",
      "epoch 98: loss=0.9681355953216553\n",
      "epoch 99: loss=0.9681603312492371\n",
      "epoch 100: loss=0.967185378074646\n",
      "epoch 101: loss=0.9678168892860413\n",
      "epoch 102: loss=0.9674158692359924\n",
      "epoch 103: loss=0.9652879238128662\n",
      "epoch 104: loss=0.9660931825637817\n",
      "epoch 105: loss=0.9670608639717102\n",
      "epoch 106: loss=0.9672941565513611\n",
      "epoch 107: loss=0.9673628211021423\n",
      "epoch 108: loss=0.9676072001457214\n",
      "epoch 109: loss=0.9654802083969116\n",
      "epoch 110: loss=0.9653213024139404\n",
      "epoch 111: loss=0.9670365452766418\n",
      "epoch 112: loss=0.963407576084137\n",
      "epoch 113: loss=0.9628040790557861\n",
      "epoch 114: loss=0.9630328416824341\n",
      "epoch 115: loss=0.9636552333831787\n",
      "epoch 116: loss=0.9625304341316223\n",
      "epoch 117: loss=0.9633329510688782\n",
      "epoch 118: loss=0.9615561962127686\n",
      "epoch 119: loss=0.9626438617706299\n",
      "epoch 120: loss=0.9599621295928955\n",
      "epoch 121: loss=0.9616274833679199\n",
      "epoch 122: loss=0.9613537788391113\n",
      "epoch 123: loss=0.960716962814331\n",
      "epoch 124: loss=0.9613246321678162\n",
      "epoch 125: loss=0.9588503241539001\n",
      "epoch 126: loss=0.9595465064048767\n",
      "epoch 127: loss=0.959110677242279\n",
      "epoch 128: loss=0.9604434967041016\n",
      "epoch 129: loss=0.9586756229400635\n",
      "epoch 130: loss=0.9591612219810486\n",
      "epoch 131: loss=0.9560142755508423\n",
      "epoch 132: loss=0.9587090611457825\n",
      "epoch 133: loss=0.9604316353797913\n",
      "epoch 134: loss=0.960192859172821\n",
      "epoch 135: loss=0.9580221176147461\n",
      "epoch 136: loss=0.9593624472618103\n",
      "epoch 137: loss=0.9568523168563843\n",
      "epoch 138: loss=0.9568650126457214\n",
      "epoch 139: loss=0.9569031596183777\n",
      "epoch 140: loss=0.9568934440612793\n",
      "epoch 141: loss=0.9540871381759644\n",
      "epoch 142: loss=0.9565648436546326\n",
      "epoch 143: loss=0.9538965821266174\n",
      "epoch 144: loss=0.9563199877738953\n",
      "epoch 145: loss=0.9570024609565735\n",
      "epoch 146: loss=0.9538524746894836\n",
      "epoch 147: loss=0.953816294670105\n",
      "epoch 148: loss=0.954307496547699\n",
      "epoch 149: loss=0.9543449282646179\n",
      "epoch 150: loss=0.9511052966117859\n",
      "epoch 151: loss=0.9555313587188721\n",
      "epoch 152: loss=0.9523895978927612\n",
      "epoch 153: loss=0.9508926272392273\n",
      "epoch 154: loss=0.9520618915557861\n",
      "epoch 155: loss=0.9507723450660706\n",
      "epoch 156: loss=0.9515820145606995\n",
      "epoch 157: loss=0.9513793587684631\n",
      "epoch 158: loss=0.9503138661384583\n",
      "epoch 159: loss=0.950107753276825\n",
      "epoch 160: loss=0.9493579268455505\n",
      "epoch 161: loss=0.9504221081733704\n",
      "epoch 162: loss=0.9474689960479736\n",
      "epoch 163: loss=0.9496796131134033\n",
      "epoch 164: loss=0.9466172456741333\n",
      "epoch 165: loss=0.9465286731719971\n",
      "epoch 166: loss=0.9482551217079163\n",
      "epoch 167: loss=0.945163369178772\n",
      "epoch 168: loss=0.945652961730957\n",
      "epoch 169: loss=0.9472635388374329\n",
      "epoch 170: loss=0.9443896412849426\n",
      "epoch 171: loss=0.9454136490821838\n",
      "epoch 172: loss=0.9456913471221924\n",
      "epoch 173: loss=0.9451575875282288\n",
      "epoch 174: loss=0.9442033767700195\n",
      "epoch 175: loss=0.9449247717857361\n",
      "epoch 176: loss=0.9431922435760498\n",
      "epoch 177: loss=0.9430273175239563\n",
      "epoch 178: loss=0.9430617094039917\n",
      "epoch 179: loss=0.9408562779426575\n",
      "epoch 180: loss=0.9423999190330505\n",
      "epoch 181: loss=0.942183792591095\n",
      "epoch 182: loss=0.9428585767745972\n",
      "epoch 183: loss=0.9406217932701111\n",
      "epoch 184: loss=0.9426712989807129\n",
      "epoch 185: loss=0.9397369623184204\n",
      "epoch 186: loss=0.9404581189155579\n",
      "epoch 187: loss=0.9434522390365601\n",
      "epoch 188: loss=0.9402320384979248\n",
      "epoch 189: loss=0.9402260780334473\n",
      "epoch 190: loss=0.9419828653335571\n",
      "epoch 191: loss=0.9394631385803223\n",
      "epoch 192: loss=0.9411587119102478\n",
      "epoch 193: loss=0.9395184516906738\n",
      "epoch 194: loss=0.9394445419311523\n",
      "epoch 195: loss=0.9395188093185425\n",
      "epoch 196: loss=0.9397056698799133\n",
      "epoch 197: loss=0.9372431039810181\n",
      "epoch 198: loss=0.9384781718254089\n",
      "epoch 199: loss=0.9371390342712402\n",
      "training patch with 132877 edges\n",
      "epoch 0: loss=18.424781799316406\n",
      "epoch 1: loss=17.990760803222656\n",
      "epoch 2: loss=17.358905792236328\n",
      "epoch 3: loss=16.157711029052734\n",
      "epoch 4: loss=14.711416244506836\n",
      "epoch 5: loss=13.202213287353516\n",
      "epoch 6: loss=12.887126922607422\n",
      "epoch 7: loss=13.849868774414062\n",
      "epoch 8: loss=14.516959190368652\n",
      "epoch 9: loss=14.436832427978516\n",
      "epoch 10: loss=13.502974510192871\n",
      "epoch 11: loss=12.187674522399902\n",
      "epoch 12: loss=10.766122817993164\n",
      "epoch 13: loss=9.71112060546875\n",
      "epoch 14: loss=8.663372993469238\n",
      "epoch 15: loss=8.103498458862305\n",
      "epoch 16: loss=7.79551362991333\n",
      "epoch 17: loss=7.546614646911621\n",
      "epoch 18: loss=7.189114093780518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19: loss=6.713892936706543\n",
      "epoch 20: loss=6.330645561218262\n",
      "epoch 21: loss=6.006564140319824\n",
      "epoch 22: loss=5.595001220703125\n",
      "epoch 23: loss=5.196930408477783\n",
      "epoch 24: loss=4.796202182769775\n",
      "epoch 25: loss=4.332270622253418\n",
      "epoch 26: loss=3.937411069869995\n",
      "epoch 27: loss=3.5744688510894775\n",
      "epoch 28: loss=3.2733964920043945\n",
      "epoch 29: loss=3.0365209579467773\n",
      "epoch 30: loss=2.8121886253356934\n",
      "epoch 31: loss=2.651026487350464\n",
      "epoch 32: loss=2.4560651779174805\n",
      "epoch 33: loss=2.233560085296631\n",
      "epoch 34: loss=2.033320665359497\n",
      "epoch 35: loss=1.8675566911697388\n",
      "epoch 36: loss=1.725999116897583\n",
      "epoch 37: loss=1.6498382091522217\n",
      "epoch 38: loss=1.5666364431381226\n",
      "epoch 39: loss=1.480513095855713\n",
      "epoch 40: loss=1.3960756063461304\n",
      "epoch 41: loss=1.3317979574203491\n",
      "epoch 42: loss=1.2741823196411133\n",
      "epoch 43: loss=1.2423006296157837\n",
      "epoch 44: loss=1.214709997177124\n",
      "epoch 45: loss=1.1840837001800537\n",
      "epoch 46: loss=1.1548970937728882\n",
      "epoch 47: loss=1.1345241069793701\n",
      "epoch 48: loss=1.1194591522216797\n",
      "epoch 49: loss=1.1126946210861206\n",
      "epoch 50: loss=1.1010291576385498\n",
      "epoch 51: loss=1.0863571166992188\n",
      "epoch 52: loss=1.0857185125350952\n",
      "epoch 53: loss=1.0761168003082275\n",
      "epoch 54: loss=1.0735533237457275\n",
      "epoch 55: loss=1.0667591094970703\n",
      "epoch 56: loss=1.0621800422668457\n",
      "epoch 57: loss=1.057822346687317\n",
      "epoch 58: loss=1.056807041168213\n",
      "epoch 59: loss=1.0527864694595337\n",
      "epoch 60: loss=1.0490046739578247\n",
      "epoch 61: loss=1.0458735227584839\n",
      "epoch 62: loss=1.0451189279556274\n",
      "epoch 63: loss=1.043714165687561\n",
      "epoch 64: loss=1.0385160446166992\n",
      "epoch 65: loss=1.0380207300186157\n",
      "epoch 66: loss=1.0369160175323486\n",
      "epoch 67: loss=1.0358983278274536\n",
      "epoch 68: loss=1.0317126512527466\n",
      "epoch 69: loss=1.0316170454025269\n",
      "epoch 70: loss=1.029800295829773\n",
      "epoch 71: loss=1.030970811843872\n",
      "epoch 72: loss=1.0304564237594604\n",
      "epoch 73: loss=1.0284600257873535\n",
      "epoch 74: loss=1.0312602519989014\n",
      "epoch 75: loss=1.028558611869812\n",
      "epoch 76: loss=1.0273017883300781\n",
      "epoch 77: loss=1.028407096862793\n",
      "epoch 78: loss=1.0275086164474487\n",
      "epoch 79: loss=1.027376651763916\n",
      "epoch 80: loss=1.0240848064422607\n",
      "epoch 81: loss=1.0249382257461548\n",
      "epoch 82: loss=1.0257655382156372\n",
      "epoch 83: loss=1.0232208967208862\n",
      "epoch 84: loss=1.0232336521148682\n",
      "epoch 85: loss=1.0243408679962158\n",
      "epoch 86: loss=1.0206046104431152\n",
      "epoch 87: loss=1.0209920406341553\n",
      "epoch 88: loss=1.0206117630004883\n",
      "epoch 89: loss=1.020658016204834\n",
      "epoch 90: loss=1.019775390625\n",
      "epoch 91: loss=1.0198274850845337\n",
      "epoch 92: loss=1.0168733596801758\n",
      "epoch 93: loss=1.0190883874893188\n",
      "epoch 94: loss=1.0194653272628784\n",
      "epoch 95: loss=1.0191819667816162\n",
      "epoch 96: loss=1.017203688621521\n",
      "epoch 97: loss=1.0171167850494385\n",
      "epoch 98: loss=1.0175988674163818\n",
      "epoch 99: loss=1.0141918659210205\n",
      "epoch 100: loss=1.0144433975219727\n",
      "epoch 101: loss=1.0140066146850586\n",
      "epoch 102: loss=1.0176801681518555\n",
      "epoch 103: loss=1.0103119611740112\n",
      "epoch 104: loss=1.0139950513839722\n",
      "epoch 105: loss=1.0127681493759155\n",
      "epoch 106: loss=1.0123445987701416\n",
      "epoch 107: loss=1.013552188873291\n",
      "epoch 108: loss=1.0118080377578735\n",
      "epoch 109: loss=1.0116034746170044\n",
      "epoch 110: loss=1.0123549699783325\n",
      "epoch 111: loss=1.0104777812957764\n",
      "epoch 112: loss=1.0108036994934082\n",
      "epoch 113: loss=1.008582353591919\n",
      "epoch 114: loss=1.0087001323699951\n",
      "epoch 115: loss=1.0086407661437988\n",
      "epoch 116: loss=1.0076605081558228\n",
      "epoch 117: loss=1.0089737176895142\n",
      "epoch 118: loss=1.0058369636535645\n",
      "epoch 119: loss=1.0061742067337036\n",
      "epoch 120: loss=1.005854606628418\n",
      "epoch 121: loss=1.0028071403503418\n",
      "epoch 122: loss=1.0039516687393188\n",
      "epoch 123: loss=1.0050994157791138\n",
      "epoch 124: loss=1.0034257173538208\n",
      "epoch 125: loss=1.001672387123108\n",
      "epoch 126: loss=0.9997813701629639\n",
      "epoch 127: loss=1.0005335807800293\n",
      "epoch 128: loss=1.0015819072723389\n",
      "epoch 129: loss=0.9991858005523682\n",
      "epoch 130: loss=0.9997742772102356\n",
      "epoch 131: loss=1.0000252723693848\n",
      "epoch 132: loss=0.9961276650428772\n",
      "epoch 133: loss=0.9961162805557251\n",
      "epoch 134: loss=0.9969722628593445\n",
      "epoch 135: loss=0.9963216185569763\n",
      "epoch 136: loss=0.9959319829940796\n",
      "epoch 137: loss=0.9956634640693665\n",
      "epoch 138: loss=0.9939981698989868\n",
      "epoch 139: loss=0.9956103563308716\n",
      "epoch 140: loss=0.9945772290229797\n",
      "epoch 141: loss=0.9950422048568726\n",
      "epoch 142: loss=0.9953238368034363\n",
      "epoch 143: loss=0.9950066804885864\n",
      "epoch 144: loss=0.9907063245773315\n",
      "epoch 145: loss=0.9907808899879456\n",
      "epoch 146: loss=0.9924212694168091\n",
      "epoch 147: loss=0.9923966526985168\n",
      "epoch 148: loss=0.9955733418464661\n",
      "epoch 149: loss=0.9927863478660583\n",
      "epoch 150: loss=0.9917138814926147\n",
      "epoch 151: loss=0.9919437170028687\n",
      "epoch 152: loss=0.990821361541748\n",
      "epoch 153: loss=0.9901382327079773\n",
      "epoch 154: loss=0.9928563833236694\n",
      "epoch 155: loss=0.9889571070671082\n",
      "epoch 156: loss=0.9876664876937866\n",
      "epoch 157: loss=0.9901093244552612\n",
      "epoch 158: loss=0.9889324307441711\n",
      "epoch 159: loss=0.9878221750259399\n",
      "epoch 160: loss=0.9915318489074707\n",
      "epoch 161: loss=0.9882773160934448\n",
      "epoch 162: loss=0.9887105226516724\n",
      "epoch 163: loss=0.9883006811141968\n",
      "epoch 164: loss=0.9894898533821106\n",
      "epoch 165: loss=0.9883072376251221\n",
      "epoch 166: loss=0.9876285195350647\n",
      "epoch 167: loss=0.9866511821746826\n",
      "epoch 168: loss=0.9864291548728943\n",
      "epoch 169: loss=0.9837950468063354\n",
      "epoch 170: loss=0.9867497086524963\n",
      "epoch 171: loss=0.9856541156768799\n",
      "epoch 172: loss=0.9862948656082153\n",
      "epoch 173: loss=0.9850371479988098\n",
      "epoch 174: loss=0.9859926700592041\n",
      "epoch 175: loss=0.9838746786117554\n",
      "epoch 176: loss=0.9834809303283691\n",
      "epoch 177: loss=0.9850496649742126\n",
      "epoch 178: loss=0.9838466644287109\n",
      "epoch 179: loss=0.9848896861076355\n",
      "epoch 180: loss=0.9841987490653992\n",
      "epoch 181: loss=0.9866206049919128\n",
      "epoch 182: loss=0.9852942228317261\n",
      "epoch 183: loss=0.9848073124885559\n",
      "epoch 184: loss=0.9837180376052856\n",
      "epoch 185: loss=0.98463374376297\n",
      "epoch 186: loss=0.9825066328048706\n",
      "epoch 187: loss=0.9839552044868469\n",
      "epoch 188: loss=0.9836796522140503\n",
      "epoch 189: loss=0.9822953343391418\n",
      "epoch 190: loss=0.9827377200126648\n",
      "epoch 191: loss=0.9818679690361023\n",
      "epoch 192: loss=0.9838166832923889\n",
      "epoch 193: loss=0.9807845950126648\n",
      "epoch 194: loss=0.9835760593414307\n",
      "epoch 195: loss=0.981904923915863\n",
      "epoch 196: loss=0.9829608201980591\n",
      "epoch 197: loss=0.9790400266647339\n",
      "epoch 198: loss=0.9832897186279297\n",
      "epoch 199: loss=0.9820615649223328\n",
      "training patch with 104609 edges\n",
      "epoch 0: loss=18.141864776611328\n",
      "epoch 1: loss=17.816625595092773\n",
      "epoch 2: loss=17.15228843688965\n",
      "epoch 3: loss=16.275619506835938\n",
      "epoch 4: loss=14.652482032775879\n",
      "epoch 5: loss=13.027127265930176\n",
      "epoch 6: loss=12.443817138671875\n",
      "epoch 7: loss=12.981561660766602\n",
      "epoch 8: loss=13.517411231994629\n",
      "epoch 9: loss=13.381591796875\n",
      "epoch 10: loss=12.679072380065918\n",
      "epoch 11: loss=11.559677124023438\n",
      "epoch 12: loss=10.354039192199707\n",
      "epoch 13: loss=9.297380447387695\n",
      "epoch 14: loss=8.439584732055664\n",
      "epoch 15: loss=7.862984657287598\n",
      "epoch 16: loss=7.384555816650391\n",
      "epoch 17: loss=7.162191390991211\n",
      "epoch 18: loss=6.899553298950195\n",
      "epoch 19: loss=6.661609649658203\n",
      "epoch 20: loss=6.361669063568115\n",
      "epoch 21: loss=6.070679187774658\n",
      "epoch 22: loss=5.738497257232666\n",
      "epoch 23: loss=5.270401954650879\n",
      "epoch 24: loss=4.845940113067627\n",
      "epoch 25: loss=4.292769908905029\n",
      "epoch 26: loss=3.8960561752319336\n",
      "epoch 27: loss=3.45963978767395\n",
      "epoch 28: loss=3.12798810005188\n",
      "epoch 29: loss=2.83858060836792\n",
      "epoch 30: loss=2.593900203704834\n",
      "epoch 31: loss=2.3768928050994873\n",
      "epoch 32: loss=2.1974291801452637\n",
      "epoch 33: loss=1.9873019456863403\n",
      "epoch 34: loss=1.7840207815170288\n",
      "epoch 35: loss=1.6406750679016113\n",
      "epoch 36: loss=1.5478731393814087\n",
      "epoch 37: loss=1.463273286819458\n",
      "epoch 38: loss=1.3961067199707031\n",
      "epoch 39: loss=1.3154798746109009\n",
      "epoch 40: loss=1.236292839050293\n",
      "epoch 41: loss=1.186676025390625\n",
      "epoch 42: loss=1.149038314819336\n",
      "epoch 43: loss=1.1311191320419312\n",
      "epoch 44: loss=1.1114081144332886\n",
      "epoch 45: loss=1.0867362022399902\n",
      "epoch 46: loss=1.061596393585205\n",
      "epoch 47: loss=1.0451854467391968\n",
      "epoch 48: loss=1.0406602621078491\n",
      "epoch 49: loss=1.0256632566452026\n",
      "epoch 50: loss=1.0214308500289917\n",
      "epoch 51: loss=1.0197021961212158\n",
      "epoch 52: loss=1.0102119445800781\n",
      "epoch 53: loss=1.002962350845337\n",
      "epoch 54: loss=0.9992029666900635\n",
      "epoch 55: loss=0.9974632859230042\n",
      "epoch 56: loss=0.997164785861969\n",
      "epoch 57: loss=0.9939748644828796\n",
      "epoch 58: loss=0.9916896820068359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59: loss=0.9876396059989929\n",
      "epoch 60: loss=0.988228976726532\n",
      "epoch 61: loss=0.9869676828384399\n",
      "epoch 62: loss=0.9859360456466675\n",
      "epoch 63: loss=0.9848693609237671\n",
      "epoch 64: loss=0.9805524945259094\n",
      "epoch 65: loss=0.9799030423164368\n",
      "epoch 66: loss=0.9796339273452759\n",
      "epoch 67: loss=0.9799520969390869\n",
      "epoch 68: loss=0.976691722869873\n",
      "epoch 69: loss=0.9776821136474609\n",
      "epoch 70: loss=0.9755471348762512\n",
      "epoch 71: loss=0.9760152101516724\n",
      "epoch 72: loss=0.9759411215782166\n",
      "epoch 73: loss=0.9742892980575562\n",
      "epoch 74: loss=0.9748203158378601\n",
      "epoch 75: loss=0.9728660583496094\n",
      "epoch 76: loss=0.973253607749939\n",
      "epoch 77: loss=0.971630871295929\n",
      "epoch 78: loss=0.9723219871520996\n",
      "epoch 79: loss=0.9708551168441772\n",
      "epoch 80: loss=0.9701100587844849\n",
      "epoch 81: loss=0.9711410999298096\n",
      "epoch 82: loss=0.9694344997406006\n",
      "epoch 83: loss=0.9687235355377197\n",
      "epoch 84: loss=0.968393087387085\n",
      "epoch 85: loss=0.9686321020126343\n",
      "epoch 86: loss=0.9684416055679321\n",
      "epoch 87: loss=0.9690948128700256\n",
      "epoch 88: loss=0.9672960638999939\n",
      "epoch 89: loss=0.9679507613182068\n",
      "epoch 90: loss=0.9657624959945679\n",
      "epoch 91: loss=0.9658829569816589\n",
      "epoch 92: loss=0.9657832384109497\n",
      "epoch 93: loss=0.9647902250289917\n",
      "epoch 94: loss=0.9647076725959778\n",
      "epoch 95: loss=0.965628981590271\n",
      "epoch 96: loss=0.9651394486427307\n",
      "epoch 97: loss=0.964303195476532\n",
      "epoch 98: loss=0.963751494884491\n",
      "epoch 99: loss=0.9634304642677307\n",
      "epoch 100: loss=0.9624307751655579\n",
      "epoch 101: loss=0.9621297121047974\n",
      "epoch 102: loss=0.9615074396133423\n",
      "epoch 103: loss=0.9617618322372437\n",
      "epoch 104: loss=0.9606937766075134\n",
      "epoch 105: loss=0.9624254107475281\n",
      "epoch 106: loss=0.9606336355209351\n",
      "epoch 107: loss=0.9592632055282593\n",
      "epoch 108: loss=0.9599813222885132\n",
      "epoch 109: loss=0.9604450464248657\n",
      "epoch 110: loss=0.9590743780136108\n",
      "epoch 111: loss=0.9589020013809204\n",
      "epoch 112: loss=0.9591161012649536\n",
      "epoch 113: loss=0.9589544534683228\n",
      "epoch 114: loss=0.9594921469688416\n",
      "epoch 115: loss=0.95733243227005\n",
      "epoch 116: loss=0.9577208757400513\n",
      "epoch 117: loss=0.9580121040344238\n",
      "epoch 118: loss=0.956342339515686\n",
      "epoch 119: loss=0.9567989706993103\n",
      "epoch 120: loss=0.9563725590705872\n",
      "epoch 121: loss=0.9575109481811523\n",
      "epoch 122: loss=0.9556260108947754\n",
      "epoch 123: loss=0.9539052248001099\n",
      "epoch 124: loss=0.9543235301971436\n",
      "epoch 125: loss=0.9529290795326233\n",
      "epoch 126: loss=0.9533214569091797\n",
      "epoch 127: loss=0.9532725811004639\n",
      "epoch 128: loss=0.9520466923713684\n",
      "epoch 129: loss=0.953230619430542\n",
      "epoch 130: loss=0.9514110088348389\n",
      "epoch 131: loss=0.9505736231803894\n",
      "epoch 132: loss=0.9502618908882141\n",
      "epoch 133: loss=0.9506086111068726\n",
      "epoch 134: loss=0.9507670402526855\n",
      "epoch 135: loss=0.9486433267593384\n",
      "epoch 136: loss=0.9471059441566467\n",
      "epoch 137: loss=0.9502339959144592\n",
      "epoch 138: loss=0.9488137364387512\n",
      "epoch 139: loss=0.9477748870849609\n",
      "epoch 140: loss=0.9501604437828064\n",
      "epoch 141: loss=0.9477264881134033\n",
      "epoch 142: loss=0.9479627013206482\n",
      "epoch 143: loss=0.9475169777870178\n",
      "epoch 144: loss=0.9472929835319519\n",
      "epoch 145: loss=0.9451565146446228\n",
      "epoch 146: loss=0.9453727602958679\n",
      "epoch 147: loss=0.9435378909111023\n",
      "epoch 148: loss=0.945659875869751\n",
      "epoch 149: loss=0.944007158279419\n",
      "epoch 150: loss=0.9407410025596619\n",
      "epoch 151: loss=0.9422041773796082\n",
      "epoch 152: loss=0.9391565918922424\n",
      "epoch 153: loss=0.9416018724441528\n",
      "epoch 154: loss=0.941868245601654\n",
      "epoch 155: loss=0.9387299418449402\n",
      "epoch 156: loss=0.9378966093063354\n",
      "epoch 157: loss=0.9399566650390625\n",
      "epoch 158: loss=0.9356557130813599\n",
      "epoch 159: loss=0.9369287490844727\n",
      "epoch 160: loss=0.9349163174629211\n",
      "epoch 161: loss=0.934756875038147\n",
      "epoch 162: loss=0.934134840965271\n",
      "epoch 163: loss=0.9351568818092346\n",
      "epoch 164: loss=0.9346426129341125\n",
      "epoch 165: loss=0.9333252310752869\n",
      "epoch 166: loss=0.9305537343025208\n",
      "epoch 167: loss=0.9315184354782104\n",
      "epoch 168: loss=0.9325780272483826\n",
      "epoch 169: loss=0.9299708604812622\n",
      "epoch 170: loss=0.931042492389679\n",
      "epoch 171: loss=0.9311020374298096\n",
      "epoch 172: loss=0.9317507743835449\n",
      "epoch 173: loss=0.9292188286781311\n",
      "epoch 174: loss=0.9299099445343018\n",
      "epoch 175: loss=0.9286821484565735\n",
      "epoch 176: loss=0.9282342195510864\n",
      "epoch 177: loss=0.9296785593032837\n",
      "epoch 178: loss=0.9264041185379028\n",
      "epoch 179: loss=0.9263947606086731\n",
      "epoch 180: loss=0.9259211421012878\n",
      "epoch 181: loss=0.9263976812362671\n",
      "epoch 182: loss=0.925311267375946\n",
      "epoch 183: loss=0.9257688522338867\n",
      "epoch 184: loss=0.9255313277244568\n",
      "epoch 185: loss=0.9258971810340881\n",
      "epoch 186: loss=0.9275379180908203\n",
      "epoch 187: loss=0.9251393675804138\n",
      "epoch 188: loss=0.9256548285484314\n",
      "epoch 189: loss=0.9245138764381409\n",
      "epoch 190: loss=0.9265280365943909\n",
      "epoch 191: loss=0.9239375591278076\n",
      "epoch 192: loss=0.9217102527618408\n",
      "epoch 193: loss=0.9242799282073975\n",
      "epoch 194: loss=0.9237664341926575\n",
      "epoch 195: loss=0.9245404601097107\n",
      "epoch 196: loss=0.9258744716644287\n",
      "epoch 197: loss=0.9220980405807495\n",
      "epoch 198: loss=0.9204009175300598\n",
      "epoch 199: loss=0.9223212599754333\n",
      "training patch with 106870 edges\n",
      "epoch 0: loss=18.298795700073242\n",
      "epoch 1: loss=17.876768112182617\n",
      "epoch 2: loss=17.351415634155273\n",
      "epoch 3: loss=16.352033615112305\n",
      "epoch 4: loss=14.900829315185547\n",
      "epoch 5: loss=13.42029857635498\n",
      "epoch 6: loss=12.88671588897705\n",
      "epoch 7: loss=13.699173927307129\n",
      "epoch 8: loss=14.238502502441406\n",
      "epoch 9: loss=13.958243370056152\n",
      "epoch 10: loss=12.782551765441895\n",
      "epoch 11: loss=11.286103248596191\n",
      "epoch 12: loss=9.76885986328125\n",
      "epoch 13: loss=8.623197555541992\n",
      "epoch 14: loss=7.855048179626465\n",
      "epoch 15: loss=7.432801246643066\n",
      "epoch 16: loss=7.123510360717773\n",
      "epoch 17: loss=6.8097310066223145\n",
      "epoch 18: loss=6.556632041931152\n",
      "epoch 19: loss=6.129429340362549\n",
      "epoch 20: loss=5.7111310958862305\n",
      "epoch 21: loss=5.279299259185791\n",
      "epoch 22: loss=4.7834320068359375\n",
      "epoch 23: loss=4.353998184204102\n",
      "epoch 24: loss=3.9663162231445312\n",
      "epoch 25: loss=3.5964269638061523\n",
      "epoch 26: loss=3.303154706954956\n",
      "epoch 27: loss=3.029696226119995\n",
      "epoch 28: loss=2.8331000804901123\n",
      "epoch 29: loss=2.6366281509399414\n",
      "epoch 30: loss=2.4158921241760254\n",
      "epoch 31: loss=2.1984424591064453\n",
      "epoch 32: loss=2.0088376998901367\n",
      "epoch 33: loss=1.810354232788086\n",
      "epoch 34: loss=1.657610297203064\n",
      "epoch 35: loss=1.5515211820602417\n",
      "epoch 36: loss=1.4612033367156982\n",
      "epoch 37: loss=1.38775634765625\n",
      "epoch 38: loss=1.3108593225479126\n",
      "epoch 39: loss=1.2634238004684448\n",
      "epoch 40: loss=1.2392332553863525\n",
      "epoch 41: loss=1.2151120901107788\n",
      "epoch 42: loss=1.1779015064239502\n",
      "epoch 43: loss=1.1465809345245361\n",
      "epoch 44: loss=1.1175320148468018\n",
      "epoch 45: loss=1.1069467067718506\n",
      "epoch 46: loss=1.1006923913955688\n",
      "epoch 47: loss=1.0936089754104614\n",
      "epoch 48: loss=1.0847609043121338\n",
      "epoch 49: loss=1.0650749206542969\n",
      "epoch 50: loss=1.0560287237167358\n",
      "epoch 51: loss=1.050788164138794\n",
      "epoch 52: loss=1.0510832071304321\n",
      "epoch 53: loss=1.0475248098373413\n",
      "epoch 54: loss=1.0480798482894897\n",
      "epoch 55: loss=1.044882893562317\n",
      "epoch 56: loss=1.0379612445831299\n",
      "epoch 57: loss=1.0389645099639893\n",
      "epoch 58: loss=1.0358632802963257\n",
      "epoch 59: loss=1.0340259075164795\n",
      "epoch 60: loss=1.029165267944336\n",
      "epoch 61: loss=1.025938868522644\n",
      "epoch 62: loss=1.025553584098816\n",
      "epoch 63: loss=1.0280439853668213\n",
      "epoch 64: loss=1.0241341590881348\n",
      "epoch 65: loss=1.023751139640808\n",
      "epoch 66: loss=1.0232535600662231\n",
      "epoch 67: loss=1.0235751867294312\n",
      "epoch 68: loss=1.0230965614318848\n",
      "epoch 69: loss=1.0223054885864258\n",
      "epoch 70: loss=1.0208162069320679\n",
      "epoch 71: loss=1.0191930532455444\n",
      "epoch 72: loss=1.0184571743011475\n",
      "epoch 73: loss=1.0174334049224854\n",
      "epoch 74: loss=1.017478346824646\n",
      "epoch 75: loss=1.0168572664260864\n",
      "epoch 76: loss=1.0180513858795166\n",
      "epoch 77: loss=1.0145279169082642\n",
      "epoch 78: loss=1.0145843029022217\n",
      "epoch 79: loss=1.014305591583252\n",
      "epoch 80: loss=1.0141950845718384\n",
      "epoch 81: loss=1.0136467218399048\n",
      "epoch 82: loss=1.0126672983169556\n",
      "epoch 83: loss=1.010262131690979\n",
      "epoch 84: loss=1.0102639198303223\n",
      "epoch 85: loss=1.0081679821014404\n",
      "epoch 86: loss=1.0103578567504883\n",
      "epoch 87: loss=1.0100575685501099\n",
      "epoch 88: loss=1.0088553428649902\n",
      "epoch 89: loss=1.0120686292648315\n",
      "epoch 90: loss=1.0083767175674438\n",
      "epoch 91: loss=1.0071837902069092\n",
      "epoch 92: loss=1.0089845657348633\n",
      "epoch 93: loss=1.0074760913848877\n",
      "epoch 94: loss=1.00801682472229\n",
      "epoch 95: loss=1.0073089599609375\n",
      "epoch 96: loss=1.0038936138153076\n",
      "epoch 97: loss=1.0027230978012085\n",
      "epoch 98: loss=1.0057936906814575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99: loss=1.0056581497192383\n",
      "epoch 100: loss=1.0032846927642822\n",
      "epoch 101: loss=1.0057311058044434\n",
      "epoch 102: loss=1.0037356615066528\n",
      "epoch 103: loss=1.0046042203903198\n",
      "epoch 104: loss=1.0013246536254883\n",
      "epoch 105: loss=1.0018906593322754\n",
      "epoch 106: loss=1.003663182258606\n",
      "epoch 107: loss=1.0020453929901123\n",
      "epoch 108: loss=1.0027344226837158\n",
      "epoch 109: loss=1.0002762079238892\n",
      "epoch 110: loss=1.0019007921218872\n",
      "epoch 111: loss=0.9999289512634277\n",
      "epoch 112: loss=1.0004090070724487\n",
      "epoch 113: loss=0.9999430775642395\n",
      "epoch 114: loss=1.0023356676101685\n",
      "epoch 115: loss=0.997527539730072\n",
      "epoch 116: loss=1.0000617504119873\n",
      "epoch 117: loss=0.9988705515861511\n",
      "epoch 118: loss=0.9963976144790649\n",
      "epoch 119: loss=0.9970651268959045\n",
      "epoch 120: loss=0.995965301990509\n",
      "epoch 121: loss=0.9954835176467896\n",
      "epoch 122: loss=0.9978578090667725\n",
      "epoch 123: loss=0.9958763718605042\n",
      "epoch 124: loss=0.9960976243019104\n",
      "epoch 125: loss=0.9984627366065979\n",
      "epoch 126: loss=0.9962950348854065\n",
      "epoch 127: loss=0.999365508556366\n",
      "epoch 128: loss=0.9913047552108765\n",
      "epoch 129: loss=0.9964646100997925\n",
      "epoch 130: loss=0.9937089085578918\n",
      "epoch 131: loss=0.9951197504997253\n",
      "epoch 132: loss=0.9935848116874695\n",
      "epoch 133: loss=0.9947041273117065\n",
      "epoch 134: loss=0.9907296895980835\n",
      "epoch 135: loss=0.9929882287979126\n",
      "epoch 136: loss=0.9937084913253784\n",
      "epoch 137: loss=0.9914130568504333\n",
      "epoch 138: loss=0.9908915758132935\n",
      "epoch 139: loss=0.9912780523300171\n",
      "epoch 140: loss=0.991669237613678\n",
      "epoch 141: loss=0.9885742664337158\n",
      "epoch 142: loss=0.9889700412750244\n",
      "epoch 143: loss=0.9899615049362183\n",
      "epoch 144: loss=0.9879005551338196\n",
      "epoch 145: loss=0.988831639289856\n",
      "epoch 146: loss=0.9857181906700134\n",
      "epoch 147: loss=0.9831627607345581\n",
      "epoch 148: loss=0.9852324724197388\n",
      "epoch 149: loss=0.9822672009468079\n",
      "epoch 150: loss=0.9811305999755859\n",
      "epoch 151: loss=0.9814271926879883\n",
      "epoch 152: loss=0.9816492199897766\n",
      "epoch 153: loss=0.9810222387313843\n",
      "epoch 154: loss=0.9806438088417053\n",
      "epoch 155: loss=0.977347731590271\n",
      "epoch 156: loss=0.9761974215507507\n",
      "epoch 157: loss=0.9775182604789734\n",
      "epoch 158: loss=0.9751459360122681\n",
      "epoch 159: loss=0.9742778539657593\n",
      "epoch 160: loss=0.9756691455841064\n",
      "epoch 161: loss=0.972783088684082\n",
      "epoch 162: loss=0.9726612567901611\n",
      "epoch 163: loss=0.9725115299224854\n",
      "epoch 164: loss=0.9726201891899109\n",
      "epoch 165: loss=0.9725822806358337\n",
      "epoch 166: loss=0.9715291261672974\n",
      "epoch 167: loss=0.9692880511283875\n",
      "epoch 168: loss=0.9678441882133484\n",
      "epoch 169: loss=0.9677476286888123\n",
      "epoch 170: loss=0.968189001083374\n",
      "epoch 171: loss=0.9659258127212524\n",
      "epoch 172: loss=0.9637815356254578\n",
      "epoch 173: loss=0.9678237438201904\n",
      "epoch 174: loss=0.9648430943489075\n",
      "epoch 175: loss=0.9659587144851685\n",
      "epoch 176: loss=0.9676692485809326\n",
      "epoch 177: loss=0.9632114768028259\n",
      "epoch 178: loss=0.9646247625350952\n",
      "epoch 179: loss=0.9625583291053772\n",
      "epoch 180: loss=0.9624236822128296\n",
      "epoch 181: loss=0.9658753871917725\n",
      "epoch 182: loss=0.963421106338501\n",
      "epoch 183: loss=0.9603610634803772\n",
      "epoch 184: loss=0.9653851389884949\n",
      "epoch 185: loss=0.9597028493881226\n",
      "epoch 186: loss=0.9587149620056152\n",
      "epoch 187: loss=0.9614949822425842\n",
      "epoch 188: loss=0.958949625492096\n",
      "epoch 189: loss=0.9578261971473694\n",
      "epoch 190: loss=0.958527147769928\n",
      "epoch 191: loss=0.9618995785713196\n",
      "epoch 192: loss=0.9561267495155334\n",
      "epoch 193: loss=0.9576890468597412\n",
      "epoch 194: loss=0.9571831226348877\n",
      "epoch 195: loss=0.9553937911987305\n",
      "epoch 196: loss=0.9578807950019836\n",
      "epoch 197: loss=0.956586480140686\n",
      "epoch 198: loss=0.9563777446746826\n",
      "epoch 199: loss=0.954997718334198\n",
      "training patch with 121510 edges\n",
      "epoch 0: loss=18.261913299560547\n",
      "epoch 1: loss=17.881397247314453\n",
      "epoch 2: loss=17.2210750579834\n",
      "epoch 3: loss=16.119081497192383\n",
      "epoch 4: loss=14.483451843261719\n",
      "epoch 5: loss=12.94997787475586\n",
      "epoch 6: loss=13.084273338317871\n",
      "epoch 7: loss=14.183965682983398\n",
      "epoch 8: loss=14.790807723999023\n",
      "epoch 9: loss=14.141432762145996\n",
      "epoch 10: loss=12.924551010131836\n",
      "epoch 11: loss=11.434500694274902\n",
      "epoch 12: loss=9.953056335449219\n",
      "epoch 13: loss=8.835723876953125\n",
      "epoch 14: loss=8.167980194091797\n",
      "epoch 15: loss=7.818051338195801\n",
      "epoch 16: loss=7.516142845153809\n",
      "epoch 17: loss=7.067664623260498\n",
      "epoch 18: loss=6.6092915534973145\n",
      "epoch 19: loss=6.179176330566406\n",
      "epoch 20: loss=5.774667739868164\n",
      "epoch 21: loss=5.386916637420654\n",
      "epoch 22: loss=4.887957572937012\n",
      "epoch 23: loss=4.395644187927246\n",
      "epoch 24: loss=3.917850971221924\n",
      "epoch 25: loss=3.517509698867798\n",
      "epoch 26: loss=3.2011466026306152\n",
      "epoch 27: loss=2.9490578174591064\n",
      "epoch 28: loss=2.741690158843994\n",
      "epoch 29: loss=2.586050510406494\n",
      "epoch 30: loss=2.3911168575286865\n",
      "epoch 31: loss=2.1501553058624268\n",
      "epoch 32: loss=1.9572796821594238\n",
      "epoch 33: loss=1.800279140472412\n",
      "epoch 34: loss=1.6795836687088013\n",
      "epoch 35: loss=1.6145788431167603\n",
      "epoch 36: loss=1.5289649963378906\n",
      "epoch 37: loss=1.4270949363708496\n",
      "epoch 38: loss=1.3553717136383057\n",
      "epoch 39: loss=1.3036214113235474\n",
      "epoch 40: loss=1.2829829454421997\n",
      "epoch 41: loss=1.2459410429000854\n",
      "epoch 42: loss=1.211706280708313\n",
      "epoch 43: loss=1.1817996501922607\n",
      "epoch 44: loss=1.160081148147583\n",
      "epoch 45: loss=1.1453989744186401\n",
      "epoch 46: loss=1.1297792196273804\n",
      "epoch 47: loss=1.1145739555358887\n",
      "epoch 48: loss=1.1058731079101562\n",
      "epoch 49: loss=1.1029083728790283\n",
      "epoch 50: loss=1.0982857942581177\n",
      "epoch 51: loss=1.0902608633041382\n",
      "epoch 52: loss=1.0809820890426636\n",
      "epoch 53: loss=1.079944372177124\n",
      "epoch 54: loss=1.0792007446289062\n",
      "epoch 55: loss=1.0748270750045776\n",
      "epoch 56: loss=1.0745723247528076\n",
      "epoch 57: loss=1.0697530508041382\n",
      "epoch 58: loss=1.069969892501831\n",
      "epoch 59: loss=1.066300630569458\n",
      "epoch 60: loss=1.0638957023620605\n",
      "epoch 61: loss=1.062281608581543\n",
      "epoch 62: loss=1.0604792833328247\n",
      "epoch 63: loss=1.0568625926971436\n",
      "epoch 64: loss=1.0558085441589355\n",
      "epoch 65: loss=1.057797908782959\n",
      "epoch 66: loss=1.054699182510376\n",
      "epoch 67: loss=1.0544297695159912\n",
      "epoch 68: loss=1.0538804531097412\n",
      "epoch 69: loss=1.0500805377960205\n",
      "epoch 70: loss=1.0513955354690552\n",
      "epoch 71: loss=1.0495543479919434\n",
      "epoch 72: loss=1.0484505891799927\n",
      "epoch 73: loss=1.0466278791427612\n",
      "epoch 74: loss=1.0489426851272583\n",
      "epoch 75: loss=1.0462721586227417\n",
      "epoch 76: loss=1.0452303886413574\n",
      "epoch 77: loss=1.0484275817871094\n",
      "epoch 78: loss=1.0471980571746826\n",
      "epoch 79: loss=1.0417516231536865\n",
      "epoch 80: loss=1.047566533088684\n",
      "epoch 81: loss=1.0429127216339111\n",
      "epoch 82: loss=1.047171950340271\n",
      "epoch 83: loss=1.0443216562271118\n",
      "epoch 84: loss=1.0431535243988037\n",
      "epoch 85: loss=1.040720820426941\n",
      "epoch 86: loss=1.040489673614502\n",
      "epoch 87: loss=1.0418719053268433\n",
      "epoch 88: loss=1.0395350456237793\n",
      "epoch 89: loss=1.0376148223876953\n",
      "epoch 90: loss=1.0377843379974365\n",
      "epoch 91: loss=1.0388180017471313\n",
      "epoch 92: loss=1.0384753942489624\n",
      "epoch 93: loss=1.037400484085083\n",
      "epoch 94: loss=1.0363454818725586\n",
      "epoch 95: loss=1.0357308387756348\n",
      "epoch 96: loss=1.0382959842681885\n",
      "epoch 97: loss=1.0346770286560059\n",
      "epoch 98: loss=1.035904884338379\n",
      "epoch 99: loss=1.0353355407714844\n",
      "epoch 100: loss=1.03318452835083\n",
      "epoch 101: loss=1.0340543985366821\n",
      "epoch 102: loss=1.0349256992340088\n",
      "epoch 103: loss=1.0327821969985962\n",
      "epoch 104: loss=1.0334547758102417\n",
      "epoch 105: loss=1.0332343578338623\n",
      "epoch 106: loss=1.0317219495773315\n",
      "epoch 107: loss=1.0310912132263184\n",
      "epoch 108: loss=1.0321812629699707\n",
      "epoch 109: loss=1.02985680103302\n",
      "epoch 110: loss=1.02886962890625\n",
      "epoch 111: loss=1.0315139293670654\n",
      "epoch 112: loss=1.0301657915115356\n",
      "epoch 113: loss=1.0304621458053589\n",
      "epoch 114: loss=1.0279967784881592\n",
      "epoch 115: loss=1.0282628536224365\n",
      "epoch 116: loss=1.027299404144287\n",
      "epoch 117: loss=1.027475357055664\n",
      "epoch 118: loss=1.0267002582550049\n",
      "epoch 119: loss=1.0261212587356567\n",
      "epoch 120: loss=1.0242737531661987\n",
      "epoch 121: loss=1.0237061977386475\n",
      "epoch 122: loss=1.024700403213501\n",
      "epoch 123: loss=1.0219897031784058\n",
      "epoch 124: loss=1.0214675664901733\n",
      "epoch 125: loss=1.0254125595092773\n",
      "epoch 126: loss=1.0189604759216309\n",
      "epoch 127: loss=1.0196229219436646\n",
      "epoch 128: loss=1.018953800201416\n",
      "epoch 129: loss=1.0188875198364258\n",
      "epoch 130: loss=1.0214654207229614\n",
      "epoch 131: loss=1.019665002822876\n",
      "epoch 132: loss=1.0177805423736572\n",
      "epoch 133: loss=1.0186671018600464\n",
      "epoch 134: loss=1.017194390296936\n",
      "epoch 135: loss=1.0173966884613037\n",
      "epoch 136: loss=1.0136560201644897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 137: loss=1.013358235359192\n",
      "epoch 138: loss=1.0131611824035645\n",
      "epoch 139: loss=1.0143121480941772\n",
      "epoch 140: loss=1.0128542184829712\n",
      "epoch 141: loss=1.0112777948379517\n",
      "epoch 142: loss=1.0144153833389282\n",
      "epoch 143: loss=1.0091980695724487\n",
      "epoch 144: loss=1.008947491645813\n",
      "epoch 145: loss=1.0100319385528564\n",
      "epoch 146: loss=1.008283257484436\n",
      "epoch 147: loss=1.0097477436065674\n",
      "epoch 148: loss=1.0092353820800781\n",
      "epoch 149: loss=1.0061842203140259\n",
      "epoch 150: loss=1.0072165727615356\n",
      "epoch 151: loss=1.0044865608215332\n",
      "epoch 152: loss=1.0038011074066162\n",
      "epoch 153: loss=1.004912257194519\n",
      "epoch 154: loss=1.005908727645874\n",
      "epoch 155: loss=1.0052859783172607\n",
      "epoch 156: loss=1.0004905462265015\n",
      "epoch 157: loss=1.002387523651123\n",
      "epoch 158: loss=1.003057599067688\n",
      "epoch 159: loss=1.0001652240753174\n",
      "epoch 160: loss=1.0036739110946655\n",
      "epoch 161: loss=1.0012333393096924\n",
      "epoch 162: loss=1.0017470121383667\n",
      "epoch 163: loss=1.002123475074768\n",
      "epoch 164: loss=1.000826120376587\n",
      "epoch 165: loss=1.0018835067749023\n",
      "epoch 166: loss=1.0021432638168335\n",
      "epoch 167: loss=0.998396098613739\n",
      "epoch 168: loss=0.9974210858345032\n",
      "epoch 169: loss=1.000910758972168\n",
      "epoch 170: loss=0.9985611438751221\n",
      "epoch 171: loss=1.0017991065979004\n",
      "epoch 172: loss=0.998414933681488\n",
      "epoch 173: loss=0.9974310398101807\n",
      "epoch 174: loss=0.9959262609481812\n",
      "epoch 175: loss=0.9950693845748901\n",
      "epoch 176: loss=0.9978168606758118\n",
      "epoch 177: loss=0.9942907094955444\n",
      "epoch 178: loss=0.9952404499053955\n",
      "epoch 179: loss=0.9963957071304321\n",
      "epoch 180: loss=0.9940662384033203\n",
      "epoch 181: loss=0.9959890842437744\n",
      "epoch 182: loss=0.9956812858581543\n",
      "epoch 183: loss=0.9966639280319214\n",
      "epoch 184: loss=0.9937993884086609\n",
      "epoch 185: loss=0.9947881698608398\n",
      "epoch 186: loss=0.9930320978164673\n",
      "epoch 187: loss=0.9955220222473145\n",
      "epoch 188: loss=0.9952166080474854\n",
      "epoch 189: loss=0.9939039349555969\n",
      "epoch 190: loss=0.9921454787254333\n",
      "epoch 191: loss=0.9922168254852295\n",
      "epoch 192: loss=0.9924193024635315\n",
      "epoch 193: loss=0.9908758401870728\n",
      "epoch 194: loss=0.9912636280059814\n",
      "epoch 195: loss=0.9924378395080566\n",
      "epoch 196: loss=0.9917346239089966\n",
      "epoch 197: loss=0.9902175068855286\n",
      "epoch 198: loss=0.9884522557258606\n",
      "epoch 199: loss=0.9911264777183533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e09144b2024d4bb7d287a8362e078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 131991.046875\n",
      "Epoch 10, Loss: 126345.9921875\n",
      "Epoch 20, Loss: 95837.90625\n",
      "Epoch 30, Loss: 73747.7578125\n",
      "Epoch 40, Loss: 58188.265625\n",
      "Epoch 50, Loss: 39053.875\n",
      "Epoch 60, Loss: 40844.9296875\n",
      "Epoch 70, Loss: 35980.8671875\n",
      "Epoch 80, Loss: 49415.87890625\n",
      "Epoch 90, Loss: 35067.52734375\n",
      "Epoch 100, Loss: 29618.673828125\n",
      "Epoch 110, Loss: 24994.91015625\n",
      "Epoch 120, Loss: 30307.5546875\n",
      "Epoch 130, Loss: 32703.08984375\n",
      "Epoch 140, Loss: 31341.37890625\n",
      "Epoch 150, Loss: 26645.673828125\n",
      "Epoch 160, Loss: 25507.078125\n",
      "Epoch 170, Loss: 29626.490234375\n",
      "Epoch 180, Loss: 24447.23046875\n",
      "Epoch 190, Loss: 21772.185546875\n"
     ]
    }
   ],
   "source": [
    "dimensions=[16, 32, 64, 128, 256, 512]\n",
    "AUC=[]\n",
    "AP=[]\n",
    "Loss=[]\n",
    "PATCHES=[]\n",
    "new_emb=[]\n",
    "for dim in dimensions:\n",
    "    patches_ip, models_ip =VGAE_patch_embeddings(patch_data, dim=dim, num_epochs=200, device=device)\n",
    "    PATCHES.append(patches_ip)\n",
    "    emb_patches = preprocess_graphs(patches_ip, nodes)\n",
    "    res, loss_hist= train_model(emb_patches, dim, n_patches , num_epochs=200, learning_rate=0.05)\n",
    "    emb=get_embedding(patches_ip, res)\n",
    "    new_emb.append(emb)\n",
    "    full_model_ip = tg.nn.VGAE(encoder=VGAEconv(dim, test_data.num_node_features))\n",
    "    auc, ap = full_model_ip.test(torch.tensor(emb), test_data.edge_index, neg_edges)\n",
    "    AUC.append(auc)\n",
    "    AP.append(ap)\n",
    "    Loss.append(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fa7640be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3ccc8d27d0>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(AUC)), AUC, linestyle='-', marker='o', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c89a05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3ccb8f7ee0>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(AUC)), AP, linestyle='-', marker='o', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a4c68c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD5CAYAAADSiMnIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbh0lEQVR4nO3de3Rc5Xnv8e8zM7raYNmyYoxlxwa75ABJwKhgcJo2cQqGpDFtWkrSU9zUq7QrJCGn6Wmdk7NCVy6rSdoTAuskdNHCwaScACHJsduaOi4haQK1sQw2tjHE4mIkxReBfMM33Z7zx35H2nOTLI1nRrZ+n7W0Zs87797zaGs8P7/73XvG3B0REZF8EpUuQERExi+FhIiIFKSQEBGRghQSIiJSkEJCREQKUkiIiEhBqZE6mNn9wIeA/e5+aWibBjwCzAVeA25y9wNmZsBdwA3AMeCP3P3ZsM5y4H+GzX7Z3VeF9iuAB4A6YC1wu7t7oecYqd7p06f73LlzR/7NRURk0ObNm99w96bsdhvpOgkzey/wFvBgLCS+DnS7+1fNbCUw1d3/ysxuAD5FFBJXAXe5+1XhDb8VaAEc2AxcEYLlGeDTwEaikLjb3R8v9Bwj/aItLS3e2tp6antFREQAMLPN7t6S3T7i4SZ3/w+gO6t5GbAqLK8Cboy1P+iRDUCDmc0ErgPWu3t3GA2sB5aGx8519w0epdWDWdvK9xwiIlImY52TmOHue8LyXmBGWJ4FtMf6dYS24do78rQP9xwiIlImRU9chxFAST/bY6TnMLNbzazVzFq7urpKWYqIyIQy1pDYFw4VEW73h/ZOYHasX3NoG669OU/7cM+Rw93vdfcWd29pasqZdxERkTEaa0isAZaH5eXA6lj7LRZZBBwKh4zWAdea2VQzmwpcC6wLjx02s0XhzKhbsraV7zlERKRMTuUU2O8CvwFMN7MO4A7gq8CjZrYC2A3cFLqvJTqzqY3oFNiPA7h7t5l9CdgU+n3R3dOT4Z9g6BTYx8MPwzyHiIiUyYinwJ5pdAqsiMjojfkU2Inih8918E8bdle6DBGRcUUhEfzz1j08sql95I4iIhOIQiJIJoy+gbPr0JuISLEUEkEqYfQPDFS6DBGRcUUhEWgkISKSSyERRCMJhYSISJxCIkgmEvT1KyREROIUEoFGEiIiuRQSQTJp9GniWkQkg0IiSGniWkQkh0IiSCUS9GtOQkQkg0IiSCU1khARyaaQCJKauBYRyaGQCKI5CU1ci4jEKSSCZMIYcBjQaEJEZJBCIkglDID+s+z7NUREiqGQCJKJaFdoXkJEZIhCIkiPJHr7NS8hIpKmkAiS6cNNGkmIiAxSSARVySgkdK2EiMgQhUSgOQkRkVwKiSA9J6GRhIjIEIVEMDgnoc9vEhEZpJAIUoNzEjq7SUQkTSER6OwmEZFcColAcxIiIrkUEkH67CZ9z7WIyBCFRDA0ktCchIhImkIiSE9ca05CRGSIQiJIak5CRCSHQiJI6YprEZEcColAIwkRkVwKiWDwS4c0cS0iMkghEQyOJHQKrIjIIIVEoLObRERyKSSCwW+mU0iIiAxSSARD3yehOQkRkbSiQsLM/puZ7TCz7Wb2XTOrNbN5ZrbRzNrM7BEzqw59a8L9tvD43Nh2PhfaXzKz62LtS0Nbm5mtLKbWkaQ0JyEikmPMIWFms4BPAy3ufimQBG4Gvgbc6e7zgQPAirDKCuBAaL8z9MPMLg7rXQIsBb5tZkkzSwLfAq4HLgY+GvqWhOYkRERyFXu4KQXUmVkKqAf2AO8HHguPrwJuDMvLwn3C40vMzEL7w+5+0t1fBdqAK8NPm7u/4u49wMOhb0noOgkRkVxjDgl37wT+DnidKBwOAZuBg+7eF7p1ALPC8iygPazbF/o3xtuz1inUXhK64lpEJFcxh5umEv3Pfh5wPjCJ6HBR2ZnZrWbWamatXV1dY9qGRhIiIrmKOdz0AeBVd+9y917gB8BioCEcfgJoBjrDcicwGyA8PgV4M96etU6h9hzufq+7t7h7S1NT05h+GV1xLSKSq5iQeB1YZGb1YW5hCfAC8CTwu6HPcmB1WF4T7hMe/7G7e2i/OZz9NA9YADwDbAIWhLOlqokmt9cUUe+wNJIQEcmVGrlLfu6+0cweA54F+oDngHuBfwUeNrMvh7b7wir3Ad8xszagm+hNH3ffYWaPEgVMH3Cbu/cDmNkngXVEZ07d7+47xlrvSHQKrIhIrjGHBIC73wHckdX8CtGZSdl9TwC/V2A7XwG+kqd9LbC2mBpPlUYSIiK5dMV1YGakEqY5CRGRGIVETDJhGkmIiMQoJGJSCaNfcxIiIoMUEjEaSYiIZFJIxKSSCV1xLSISo5CI0UhCRCSTQiImlTD6+nV2k4hImkIiJpkwHW4SEYlRSMSkdLhJRCSDQiJGE9ciIpkUEjHRSEJzEiIiaQqJGM1JiIhkUkjEaE5CRCSTQiJGIwkRkUwKiZhUIqHvkxARiVFIxCQ1cS0ikkEhEZNKak5CRCROIRGT0pyEiEgGhURMUnMSIiIZFBIxGkmIiGRSSMQkk5q4FhGJU0jEaCQhIpJJIRGjLx0SEcmkkIjRSEJEJJNCIiaZSNCrs5tERAYpJGKikYQmrkVE0hQSMbriWkQkk0IiRnMSIiKZFBIxyURCIwkRkRiFRIxGEiIimRQSMekvHXJXUIiIgEIiQyphABpNiIgEComYZDIKCc1LiIhEFBIx6ZGEQkJEJKKQiEkmot3Rr6uuRUQAhUSGqsHDTbrqWkQEFBIZkpq4FhHJoJCI0ZyEiEimokLCzBrM7DEze9HMdprZ1WY2zczWm9mucDs19DUzu9vM2szseTNbGNvO8tB/l5ktj7VfYWbbwjp3m5kVU+9IBuckFBIiIkDxI4m7gH9z93cA7wZ2AiuBJ9x9AfBEuA9wPbAg/NwK3ANgZtOAO4CrgCuBO9LBEvr8SWy9pUXWOyyNJEREMo05JMxsCvBe4D4Ad+9x94PAMmBV6LYKuDEsLwMe9MgGoMHMZgLXAevdvdvdDwDrgaXhsXPdfYNHl0A/GNtWSQzNSWjiWkQEihtJzAO6gP9jZs+Z2T+a2SRghrvvCX32AjPC8iygPbZ+R2gbrr0jT3sOM7vVzFrNrLWrq2vMv5BGEiIimYoJiRSwELjH3S8HjjJ0aAmAMAIo+Tuuu9/r7i3u3tLU1DTm7aRHEn26TkJEBCguJDqADnffGO4/RhQa+8KhIsLt/vB4JzA7tn5zaBuuvTlPe8mk9LEcIiIZxhwS7r4XaDezi0LTEuAFYA2QPkNpObA6LK8BbglnOS0CDoXDUuuAa81sapiwvhZYFx47bGaLwllNt8S2VRKpwbObNCchIgLRIaNifAp4yMyqgVeAjxMFz6NmtgLYDdwU+q4FbgDagGOhL+7ebWZfAjaFfl909+6w/AngAaAOeDz8lExKh5tERDIUFRLuvgVoyfPQkjx9HbitwHbuB+7P094KXFpMjaOhK65FRDLpiusYzUmIiGRSSMToimsRkUwKiRhdJyEikkkhEZM+3NTbr7ObRERAIZGhJpUE4GRff4UrEREZHxQSMXVVUUgc79FIQkQEFBIZ0iFxolcjCRERUEhkqKmKdsdxhYSICKCQyFCTSmCmkYSISJpCIsbMqE0lFRIiIoFCIktddVKHm0REAoVEltpUghO9OrtJRAQUEjlqNZIQERmkkMhSV5XkpEJCRARQSOSordJIQkQkTSGRpa4qyfEehYSICCgkctRWaeJaRCRNIZGltkrXSYiIpCkksigkRESGKCSy1GniWkRkkEIii664FhEZopDIkr7i2l1fYSoiopDIUlud/nY6neEkIqKQyFKb0hcPiYikKSSy1IWRhOYlREQUEjmGvudaISEiopDIUhu+wlRXXYuIKCRy1FbpcJOISJpCIks6JPRx4SIiCokcdRpJiIgMUkhk0eEmEZEhCoks6ZGEJq5FRBQSOWqro12ikYSIiEIihyauRUSGKCSy6GI6EZEhCoksVckEyYTpcJOICAqJvOqqkpq4FhHhNISEmSXN7Dkz+5dwf56ZbTSzNjN7xMyqQ3tNuN8WHp8b28bnQvtLZnZdrH1paGszs5XF1nqqaqsSGkmIiHB6RhK3Aztj978G3Onu84EDwIrQvgI4ENrvDP0ws4uBm4FLgKXAt0PwJIFvAdcDFwMfDX1LrrYqqYlrERGKDAkzawY+CPxjuG/A+4HHQpdVwI1heVm4T3h8Sei/DHjY3U+6+6tAG3Bl+Glz91fcvQd4OPQtOX3PtYhIpNiRxDeBvwTSB/AbgYPu3hfudwCzwvIsoB0gPH4o9B9sz1qnUHvJ1VYl9aVDIiIUERJm9iFgv7tvPo31jLWWW82s1cxau7q6it6eRhIiIpFiRhKLgQ+b2WtEh4LeD9wFNJhZKvRpBjrDcicwGyA8PgV4M96etU6h9hzufq+7t7h7S1NTUxG/UqSmKsFxnd0kIjL2kHD3z7l7s7vPJZp4/rG7/wHwJPC7odtyYHVYXhPuEx7/sbt7aL85nP00D1gAPANsAhaEs6Wqw3OsGWu9o1GniWsREQBSI3cZtb8CHjazLwPPAfeF9vuA75hZG9BN9KaPu+8ws0eBF4A+4DZ37wcws08C64AkcL+77yhBvTkm16Y4fLy3HE8lIjKunZaQcPefAD8Jy68QnZmU3ecE8HsF1v8K8JU87WuBtaejxtFobqhj7+ET9PYPUJXU9YYiMnHpHTCP5mn1DDj88uDxSpciIlJRCok8Zk+tB6C9WyEhIhObQiKP2dPqAGg/cKzClYiIVJZCIo+ZU+pIJYz2boWEiExsCok8kgnj/IY62g/ocJOITGwKiQKap9ZpJCEiE55CooDZU+vp0JyEiExwCokCZk+r4423ejjW0zdyZxGRs5RCooDZ06LTYDs0LyEiE5hCooDmwWsldMhJRCYuhUQBFzZNImHw3OsHK12KiEjFKCQKaKiv5qp5jazdvofow2pFRCYehcQwPviumbzSdZSX9h2pdCkiIhWhkBjG0kvPI2Gw9vk9lS5FRKQiFBLDmD65Jhxy2lvpUkREKkIhMYIl/+VttO1/iz2HdCqsiEw8CokRLJ4/HYCn2t6scCUiIuWnkBjBRTPOoXFSNU+3vVHpUkREyk4hMYJEwrj6wkaeevkNnQorIhOOQuIULJ4/nX2HT/Jy19FKlyIiUlYKiVOw+MJoXuLpl3XISUQmFoXEKZjTWE/z1Dqe0ryEiEwwColTtPjC6fzny2/SP6B5CRGZOBQSp+ia+Y0cPtHH9s5DlS5FRKRsFBKn6JowL/GU5iVEZAJRSJyipnNquGjGOTyti+pEZAJRSIzC4vnTeea1bg4d6610KSIiZaGQGIWPXDGLnr4Bvre5vdKliIiUhUJiFC45fwq/OncqD/7nbgZ0lpOITAAKiVG65eq5vN59jCdf2l/pUkRESk4hMUpLLz2POdPq+cLqHRw6rrkJETm7KSRGqSqZ4Js3X8bewyf4/A+36UP/ROSsppAYg4VzpvLnv/kr/Mvze/hea0elyxERKRmFxBj92a9fyDUXNnLHmh207X+r0uWIiJSEQmKMkgnjzt+/jJqqBF9YvV2HnUTkrKSQKMKMc2u5fckCnn75TX76i65KlyMictopJIr0savmMHtaHV99/EVdOyEiZ50xh4SZzTazJ83sBTPbYWa3h/ZpZrbezHaF26mh3czsbjNrM7PnzWxhbFvLQ/9dZrY81n6FmW0L69xtZlbML1sKNakkn/3Ni3hx7xF+9MLeSpcjInJaFTOS6AM+6+4XA4uA28zsYmAl8IS7LwCeCPcBrgcWhJ9bgXsgChXgDuAq4ErgjnSwhD5/EltvaRH1lsxvvft85jbW860nX9bchIicVcYcEu6+x92fDctHgJ3ALGAZsCp0WwXcGJaXAQ96ZAPQYGYzgeuA9e7e7e4HgPXA0vDYue6+waN33gdj2xpXkgnjT3/9QrZ1HuJnu/RR4iJy9jgtcxJmNhe4HNgIzHD3PeGhvcCMsDwLiH8yXkdoG669I0/7uPQ7C2dx3rm13PXELo0mROSsUXRImNlk4PvAZ9z9cPyxMAIo+Tummd1qZq1m1trVVZmzjGpSST61ZD6bdx/gxy/qc51E5OxQVEiYWRVRQDzk7j8IzfvCoSLCbfodsxOYHVu9ObQN196cpz2Hu9/r7i3u3tLU1FTMr1SUm1pmM7exnr9d95K+C1tEzgrFnN1kwH3ATnf/RuyhNUD6DKXlwOpY+y3hLKdFwKFwWGodcK2ZTQ0T1tcC68Jjh81sUXiuW2LbGpeqkgn+4rroTKd7ftJW6XJERIqWKmLdxcAfAtvMbEto+x/AV4FHzWwFsBu4KTy2FrgBaAOOAR8HcPduM/sSsCn0+6K7d4flTwAPAHXA4+FnXPvgO2fy75ft4xvrf8HCOVO5Zv70SpckIjJmdrZNsra0tHhra2tFazh6so8P/++f037gOH/z2+/kI1c0j7ySiEgFmdlmd2/JbtcV1yUwqSbFo396NQvnNPDZ723ltoeeZf+RE5UuS0Rk1BQSJdI4uYZ/WnEV//26i1i/cx/Xf/NnPN2mayhE5MyikCihVDLBbe+bz9pPv4epk6r5r/dt5Ns/adNnPInIGUMhUQbz33YOq29bzAffdT5f/7eX+PgDm2jvPlbpskRERqSQKJNJNSnuvvkyvrTsElpf6+YD3/gpK7//PFvbD+oKbREZt4o5BVZGycz4w6vn8oGLZ/DN9btYveWXPLypnUvOP5ePXTWHZZfNYnKN/iQiMn7oFNgKOnyil9XPdfLQxtd5ce8RzqlJ8bFFc/jjxfOYcW5tpcsTkQmk0CmwColxwN3Z0n6Q+37+Kmu37SGVSPChd81k6aXn8Z4F06mv1uhCREpLIXGGeP3NY/zDz17h/23p5MiJPmpSCRZd0Mjlcxp49+wG3t3cwLRJ1ZUuU0TOMgqJM0xv/wCbXu1m/c59PNX2Brv2v0X6TzVnWn0IjCm8c9YULpk1RXMZIlKUQiGhd5ZxqiqZ4Jr50wc/++mtk31s7zzE1vaDbO04yLO7D/DPW38JgBnMa5zEBU2TeHvjJOY21jN3+iTOb6hjWn01U+qqSCTG3Te/isgZQCFxhphck2LRBY0suqBxsK3ryEm2dx5iW+chdvzyELvfPMbP297gRO9AxroJg4b6ahrqq5hWX83USdVDt5OqaKiP34+Wz6lNKVhERCFxJms6p4b3veNtvO8dbxtsc3f2HT7Jq28cZf+RE3Qf7eHA0R66j/Vw4GgvB4710N59jOc7DnLgaC89/QN5t20GNakENakkNakEtVWZtzVVCWpTSWqqoj614bY6lSCZMJJmJBNGKmEkk0P3M37MSKT7JIyERcuJ9PphvcG22HrpZTMw0rdR3WYWlm2oLfQh1ieRtS452xrqg5HRnr3d3DqG+gztU4WunHkUEmcZM+O8KbWcN2XkU2jdnaM9/Rw42sOBYz1RoBzroftoL4eO93Kyt5+TfQOcCLcn+/o50Tt0e+h47+D9k71Rv57+AQYGoG9gAH36yKnJCJKMdsvTFu+bf8VT6T/Sc2a3F95+7jZPZXtWYOOF64q3j+73yHgmy1028vfNrTP/c+WuU+C5C64wzLZG+Rz3L/9V5jTWF97gGCgkJjAzY3JNisk1KWZPO70vLIhCqH/A6RuIbvvd6e+Pbgdi7QMeLQ+EPn39UVv/QNZPnjYH3MHxcBs9b0ZbvB0gPDbguevisW2G5YGwTN7nij9PbvvQvojtl8ydlLc93Xwq28hsz98ps//Yt1noPJfB/TPKbRTqT6H+RWwz374Z7v8xhU7qGX6dAu2jfI5hn2eYAqpTp/9DNBQSUjJmRipppJKVrkRExkqf3SQiIgUpJEREpCCFhIiIFKSQEBGRghQSIiJSkEJCREQKUkiIiEhBCgkRESnorPuocDPrAnaPcfXpwBunsZzTZbzWBeO3NtU1Oqpr9MZrbWOt6+3u3pTdeNaFRDHMrDXf56lX2nitC8ZvbaprdFTX6I3X2k53XTrcJCIiBSkkRESkIIVEpnsrXUAB47UuGL+1qa7RUV2jN15rO611aU5CREQK0khCREQKUkgEZrbUzF4yszYzW1nBOmab2ZNm9oKZ7TCz20P7X5tZp5ltCT83VKC218xsW3j+1tA2zczWm9mucDu1zDVdFNsnW8zssJl9plL7y8zuN7P9ZrY91pZ3H1nk7vCae97MFpa5rr81sxfDc//QzBpC+1wzOx7bd39f5roK/u3M7HNhf71kZteVua5HYjW9ZmZbQns591eh94fSvcaib9Oa2D9AEngZuACoBrYCF1eolpnAwrB8DvAL4GLgr4G/qPB+eg2YntX2dWBlWF4JfK3Cf8e9wNsrtb+A9wILge0j7SPgBuBxom+pXARsLHNd1wKpsPy1WF1z4/0qsL/y/u3Cv4OtQA0wL/ybTZarrqzH/xfwhQrsr0LvDyV7jWkkEbkSaHP3V9y9B3gYWFaJQtx9j7s/G5aPADuBWZWo5RQtA1aF5VXAjZUrhSXAy+4+1ospi+bu/wF0ZzUX2kfLgAc9sgFoMLOZ5arL3X/k7n3h7gaguRTPPdq6hrEMeNjdT7r7q0Ab0b/dstZl0RdM3wR8txTPPZxh3h9K9hpTSERmAe2x+x2MgzdmM5sLXA5sDE2fDEPG+8t9WCdw4EdmttnMbg1tM9x9T1jeC8yoQF1pN5P5D7fS+yut0D4aT6+7Pyb6H2faPDN7zsx+ama/VoF68v3txsv++jVgn7vvirWVfX9lvT+U7DWmkBinzGwy8H3gM+5+GLgHuBC4DNhDNNwtt/e4+0LgeuA2M3tv/EGPxrcVOV3OzKqBDwPfC03jYX/lqOQ+KsTMPg/0AQ+Fpj3AHHe/HPhz4P+a2bllLGlc/u1iPkrmf0bKvr/yvD8MOt2vMYVEpBOYHbvfHNoqwsyqiF4AD7n7DwDcfZ+797v7APAPlGiYPRx37wy3+4Efhhr2pYev4XZ/uesKrgeedfd9ocaK76+YQvuo4q87M/sj4EPAH4Q3F8LhnDfD8maiY/+/Uq6ahvnbjYf9lQJ+B3gk3Vbu/ZXv/YESvsYUEpFNwAIzmxf+R3ozsKYShYTjnfcBO939G7H2+HHE3wa2Z69b4rommdk56WWiSc/tRPtpeei2HFhdzrpiMv53V+n9laXQPloD3BLOQFkEHIodMig5M1sK/CXwYXc/FmtvMrNkWL4AWAC8Usa6Cv3t1gA3m1mNmc0LdT1TrrqCDwAvuntHuqGc+6vQ+wOlfI2VY0b+TPghOgvgF0T/C/h8Bet4D9FQ8XlgS/i5AfgOsC20rwFmlrmuC4jOLNkK7EjvI6AReALYBfw7MK0C+2wS8CYwJdZWkf1FFFR7gF6i478rCu0jojNOvhVec9uAljLX1UZ0vDr9Ovv70Pcj4W+8BXgW+K0y11Xwbwd8Puyvl4Dry1lXaH8A+LOsvuXcX4XeH0r2GtMV1yIiUpAON4mISEEKCRERKUghISIiBSkkRESkIIWEiIgUpJAQEZGCFBIiIlKQQkJERAr6/4vDqM38Yew2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for l in Loss:\n",
    "    plt.plot(l)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1182db08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f3c578c-6df9-4437-9121-ef9d1e675235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e4b01573534a63a47a7847a3209cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b190117bd74cb7a777009accffe2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d066635afa004f1aa06140f1d6cdfbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4529d4d1ac48bb971d695a010e42fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5486897baec4d07954ac6f74b0b5c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67060bedddc446418ef6a76b6cd9315c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd36f3d6617a4414a6ed09537ef4f6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de76c69c2124084bb998aa7affaad8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4291f34c28a24c0ba77536ddb6aa80c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12f5256885646639867c2d5b45b1859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ef1efedd2f4f76a844aa97645d8ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811bbc8ce1bf499d89ee42c1d905d672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OLD_AUC=[]\n",
    "OLD_AP=[]\n",
    "old_emb=[]\n",
    "for i in range(len(PATCHES)):\n",
    "    prob=l2g.AlignmentProblem(PATCHES[i])\n",
    "    e=prob.get_aligned_embedding()\n",
    "    old_emb.append(e)\n",
    "    full_model_ip = tg.nn.VGAE(encoder=VGAEconv(dimensions[i], test_data.num_node_features))\n",
    "    auc, ap= full_model_ip.test(torch.tensor(e), test_data.edge_index, neg_edges)\n",
    "    OLD_AUC.append(auc)\n",
    "    OLD_AP.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5688910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dimensions, AUC, linestyle='-', marker='o', color='b', label='new')\n",
    "plt.plot(dimensions, OLD_AUC, linestyle='-', marker='o', color='r', label='old')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c80a796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(AUC)), AP, linestyle='-', marker='o', color='b', label='new')\n",
    "plt.plot(np.arange(len(AUC)), OLD_AP, linestyle='-', marker='o', color='r', label='old')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9ac8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "ERR=[]\n",
    "for i in range(len(dimensions)):\n",
    "    _, _, error = procrustes(new_emb[i], old_emb[i])\n",
    "    ERR.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8cdc035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3ccc92f100>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(AUC)), ERR, linestyle='-', marker='o', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d7280-9de2-4f2d-8e5d-daf738cf928f",
   "metadata": {},
   "source": [
    "###  <a id='chapter2'> <font color=\"grey\">2. Embedding </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab9f1c-2fd1-4f6f-bd65-8d8ce24185ab",
   "metadata": {},
   "source": [
    "For the embedding, we use the architecture of a Variational Graph Autoencoder. Given a graph $G=(V,E)$ with $|V|=n$ nodes and node features ${x}_i\\in \\mathbb{R}^d$, $i\\in [n]$, denote by ${X}=[{x}_1,\\dots,{x}_n]^T\\in \\mathbb{R}^{n\\times d}$ the features matrix and by $A=(a_{ij})\\in \\{0,1\\}^{n\\times n}$ the adjacency matrix of the graph. The **encoder** produces latent representations ${z}_i\\in \\mathbb{R}^k$ for $i\\in [n]$, which are sampled from the inference model\n",
    "\\begin{equation*}\n",
    "  q({z}_i \\ | \\ {X},{A}) = \\mathcal{N}({z}_i \\ | \\ {\\mu}_i,\\mathrm{diag}({\\sigma}_i)).\n",
    "\\end{equation*}\n",
    "The means $\\mu_i$ and variances $\\mathrm{diag}({\\sigma}_i)$ are parametrized using an encoder network, for example, a graph convolutional neural network (GCN). Denoting by ${Z}=[{z}_1,\\dots,{z}_n]^T$ the matrix of latent represenations and by ${\\mu}$ and ${\\sigma}$ the matrices representing the means and variances, we have\n",
    "\\begin{equation*}\n",
    "  {\\mu} = \\mathrm{GCN}_{\\mu}({X},{A}), \\quad \\quad \\log {\\sigma} = \\mathrm{GCN}_{\\sigma}({X},{A}).\n",
    "\\end{equation*}\n",
    "The **generative model** is a distribution on the adjacency matrix,\n",
    "\\begin{equation*}\n",
    "  p({A}\\ | \\ {Z}) = \\prod_{i,j} p(a_{ij} \\ | \\ {z}_i,{z}_j).\n",
    "\\end{equation*}\n",
    "It is convenient to use\n",
    "\\begin{equation*}\n",
    "  p(a_{ij}=1 \\ | \\ {z}_i,{z}_j) = \\sigma({z}_i^T{z}_j),\n",
    "\\end{equation*}\n",
    "where $\\sigma$ is the logistic sigmoid. In order to train the model, we optimize the evidence lower bound\n",
    "\\begin{equation*}\n",
    "  \\mathcal{L} = \\mathbb{E}_{q({Z}\\ | \\ {X},{A})}[\\log p({A}\\ | \\ {Z})]-\\mathrm{D}_{\\mathrm{KL}}(q({Z}\\ | \\ {X},{A}) \\ \\| \\ p({Z})).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03bdf91f-2d14-45e4-999b-1202d5ac45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAE(dim=256, hidden_dim=128, num_features=data.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5952dc3e-2d6d-4070-8e68-09e827820147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(data, model, loss_fun, num_epochs=100, verbose=True, lr=0.01, logger=lambda loss: None):\n",
    "  #  losses = []\n",
    " #   optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "  #  # schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "  #  for e in tqdm(range(num_epochs)):\n",
    "  #      optimizer.zero_grad()\n",
    "    #    loss = loss_fun(model, data)\n",
    "    #    loss.backward()\n",
    "    #    optimizer.step()\n",
    "    #    losses.append(float(loss))\n",
    "    #    if verbose:\n",
    "     #       if not e % 20:\n",
    "     #           print(f'epoch {e}: loss={loss.item()}')\n",
    "        # schedule.step()\n",
    "    #return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "613527ab-a063-4283-bd62-5ee3b58121a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, losses = train(data, model,GAE_loss, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ab6bc1b-7f0b-440e-bb1e-193136cecac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa00692-b04c-45fc-b44f-f8db54d5b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.encode(data).detach().numpy()\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c64b9-04a9-48c8-90b1-41bff00a7de5",
   "metadata": {},
   "source": [
    "In the original L2G code, there is a Patch class handling patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad01511-5965-4676-8b56-dd304d2ec538",
   "metadata": {},
   "source": [
    "###  <a id='chapter3'> <font color=\"grey\">3. Visualisation </font></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd75dab-3105-454f-a615-df326670871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = ['AU', 'BR', 'CN']\n",
    "countries = dl.get_nodes(ts=dates[0])['country'].to_list()\n",
    "indices = [i for i in range(len(countries)) if countries[i] in most_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961ad0e-e22e-4160-a742-4e461bfa4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = embedding[indices, :]\n",
    "labels = [most_common.index(countries[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c57c17-115a-4aee-8d9f-cd69d89901e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to visualise the graph embeddings for different days\n",
    "reducer = umap.UMAP(n_neighbors=10, min_dist=0.00, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede66a-8b07-4a10-8121-9645d8f5dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = StandardScaler().fit_transform(points)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b68f64-9fd7-4ad5-8aad-d0939325a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding = reducer.fit_transform(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba61ce-a601-44ad-af31-488757c060b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    points[:, 0],\n",
    "    points[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fcea7-f8e9-49d6-b371-ec3128dac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(\n",
    "    umap_embedding[:, 0],\n",
    "    umap_embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in labels],\n",
    "    lw=1\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Embedding', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15f9b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2\n",
    "n_patches=30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9776106",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches= [ tg_graphs[d] for d in dates[:n_patches]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3ad13b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss=1.8183332681655884\n",
      "epoch 1: loss=1.7286288738250732\n",
      "epoch 2: loss=1.6538267135620117\n",
      "epoch 3: loss=1.6065369844436646\n",
      "epoch 4: loss=1.5601023435592651\n",
      "epoch 5: loss=1.5246920585632324\n",
      "epoch 6: loss=1.4916727542877197\n",
      "epoch 7: loss=1.4552620649337769\n",
      "epoch 8: loss=1.4145761728286743\n",
      "epoch 9: loss=1.3801597356796265\n",
      "epoch 10: loss=1.3303579092025757\n",
      "epoch 11: loss=1.2849384546279907\n",
      "epoch 12: loss=1.2360200881958008\n",
      "epoch 13: loss=1.1962257623672485\n",
      "epoch 14: loss=1.167494773864746\n",
      "epoch 15: loss=1.1542855501174927\n",
      "epoch 16: loss=1.1506705284118652\n",
      "epoch 17: loss=1.1479732990264893\n",
      "epoch 18: loss=1.1423649787902832\n",
      "epoch 19: loss=1.130204677581787\n",
      "epoch 20: loss=1.1152303218841553\n",
      "epoch 21: loss=1.0954545736312866\n",
      "epoch 22: loss=1.0799859762191772\n",
      "epoch 23: loss=1.064706563949585\n",
      "epoch 24: loss=1.0500530004501343\n",
      "epoch 25: loss=1.0417213439941406\n",
      "epoch 26: loss=1.036286473274231\n",
      "epoch 27: loss=1.0319632291793823\n",
      "epoch 28: loss=1.028126835823059\n",
      "epoch 29: loss=1.0229547023773193\n",
      "epoch 30: loss=1.0171953439712524\n",
      "epoch 31: loss=1.0096797943115234\n",
      "epoch 32: loss=1.0023105144500732\n",
      "epoch 33: loss=0.9932257533073425\n",
      "epoch 34: loss=0.9851431846618652\n",
      "epoch 35: loss=0.9777398705482483\n",
      "epoch 36: loss=0.9703960418701172\n",
      "epoch 37: loss=0.9660504460334778\n",
      "epoch 38: loss=0.9604564309120178\n",
      "epoch 39: loss=0.95662921667099\n",
      "epoch 40: loss=0.9532769918441772\n",
      "epoch 41: loss=0.9494003653526306\n",
      "epoch 42: loss=0.9465682506561279\n",
      "epoch 43: loss=0.9425159692764282\n",
      "epoch 44: loss=0.9402985572814941\n",
      "epoch 45: loss=0.9378054141998291\n",
      "epoch 46: loss=0.9347885847091675\n",
      "epoch 47: loss=0.9335220456123352\n",
      "epoch 48: loss=0.9321727156639099\n",
      "epoch 49: loss=0.9310123920440674\n",
      "epoch 0: loss=1.719434380531311\n",
      "epoch 1: loss=1.6278302669525146\n",
      "epoch 2: loss=1.5658926963806152\n",
      "epoch 3: loss=1.5141984224319458\n",
      "epoch 4: loss=1.4783698320388794\n",
      "epoch 5: loss=1.4406534433364868\n",
      "epoch 6: loss=1.413591980934143\n",
      "epoch 7: loss=1.376975655555725\n",
      "epoch 8: loss=1.3331528902053833\n",
      "epoch 9: loss=1.2727142572402954\n",
      "epoch 10: loss=1.2116217613220215\n",
      "epoch 11: loss=1.1659760475158691\n",
      "epoch 12: loss=1.13405179977417\n",
      "epoch 13: loss=1.1284343004226685\n",
      "epoch 14: loss=1.1328471899032593\n",
      "epoch 15: loss=1.1347987651824951\n",
      "epoch 16: loss=1.1280895471572876\n",
      "epoch 17: loss=1.1113308668136597\n",
      "epoch 18: loss=1.087524175643921\n",
      "epoch 19: loss=1.0649502277374268\n",
      "epoch 20: loss=1.0450812578201294\n",
      "epoch 21: loss=1.0294338464736938\n",
      "epoch 22: loss=1.0210944414138794\n",
      "epoch 23: loss=1.0160152912139893\n",
      "epoch 24: loss=1.0141515731811523\n",
      "epoch 25: loss=1.0101414918899536\n",
      "epoch 26: loss=1.0042505264282227\n",
      "epoch 27: loss=0.9963073134422302\n",
      "epoch 28: loss=0.9875320196151733\n",
      "epoch 29: loss=0.9789395332336426\n",
      "epoch 30: loss=0.9717439413070679\n",
      "epoch 31: loss=0.9651325941085815\n",
      "epoch 32: loss=0.961736261844635\n",
      "epoch 33: loss=0.9575480222702026\n",
      "epoch 34: loss=0.9550553560256958\n",
      "epoch 35: loss=0.9520140886306763\n",
      "epoch 36: loss=0.9487408995628357\n",
      "epoch 37: loss=0.9452006220817566\n",
      "epoch 38: loss=0.940738320350647\n",
      "epoch 39: loss=0.9384181499481201\n",
      "epoch 40: loss=0.935257077217102\n",
      "epoch 41: loss=0.9325315952301025\n",
      "epoch 42: loss=0.9307473301887512\n",
      "epoch 43: loss=0.9305784106254578\n",
      "epoch 44: loss=0.9288801550865173\n",
      "epoch 45: loss=0.9288336634635925\n",
      "epoch 46: loss=0.9285315871238708\n",
      "epoch 47: loss=0.9282169342041016\n",
      "epoch 48: loss=0.9279016256332397\n",
      "epoch 49: loss=0.9262293577194214\n",
      "epoch 0: loss=1.745256781578064\n",
      "epoch 1: loss=1.677864909172058\n",
      "epoch 2: loss=1.6082148551940918\n",
      "epoch 3: loss=1.5544393062591553\n",
      "epoch 4: loss=1.5156278610229492\n",
      "epoch 5: loss=1.463465690612793\n",
      "epoch 6: loss=1.412430763244629\n",
      "epoch 7: loss=1.3727301359176636\n",
      "epoch 8: loss=1.3058162927627563\n",
      "epoch 9: loss=1.2419841289520264\n",
      "epoch 10: loss=1.1979488134384155\n",
      "epoch 11: loss=1.1702380180358887\n",
      "epoch 12: loss=1.151413083076477\n",
      "epoch 13: loss=1.1471891403198242\n",
      "epoch 14: loss=1.1390610933303833\n",
      "epoch 15: loss=1.1232082843780518\n",
      "epoch 16: loss=1.1020671129226685\n",
      "epoch 17: loss=1.0784908533096313\n",
      "epoch 18: loss=1.054765224456787\n",
      "epoch 19: loss=1.034062147140503\n",
      "epoch 20: loss=1.0191471576690674\n",
      "epoch 21: loss=1.0131992101669312\n",
      "epoch 22: loss=1.007859706878662\n",
      "epoch 23: loss=1.0020538568496704\n",
      "epoch 24: loss=0.999509334564209\n",
      "epoch 25: loss=0.9913873672485352\n",
      "epoch 26: loss=0.9821845293045044\n",
      "epoch 27: loss=0.9722804427146912\n",
      "epoch 28: loss=0.9634181261062622\n",
      "epoch 29: loss=0.9547455906867981\n",
      "epoch 30: loss=0.952343761920929\n",
      "epoch 31: loss=0.9497115015983582\n",
      "epoch 32: loss=0.9471419453620911\n",
      "epoch 33: loss=0.9448506236076355\n",
      "epoch 34: loss=0.9425636529922485\n",
      "epoch 35: loss=0.9412515163421631\n",
      "epoch 36: loss=0.9385600686073303\n",
      "epoch 37: loss=0.9379445910453796\n",
      "epoch 38: loss=0.9369944334030151\n",
      "epoch 39: loss=0.9379364848136902\n",
      "epoch 40: loss=0.9379335045814514\n",
      "epoch 41: loss=0.9380722641944885\n",
      "epoch 42: loss=0.9354845285415649\n",
      "epoch 43: loss=0.9342659115791321\n",
      "epoch 44: loss=0.9329964518547058\n",
      "epoch 45: loss=0.9325369000434875\n",
      "epoch 46: loss=0.9301795363426208\n",
      "epoch 47: loss=0.9300205111503601\n",
      "epoch 48: loss=0.9302477836608887\n",
      "epoch 49: loss=0.9283108115196228\n",
      "epoch 0: loss=1.781896710395813\n",
      "epoch 1: loss=1.6752899885177612\n",
      "epoch 2: loss=1.6153900623321533\n",
      "epoch 3: loss=1.5456644296646118\n",
      "epoch 4: loss=1.512025237083435\n",
      "epoch 5: loss=1.475785493850708\n",
      "epoch 6: loss=1.4536703824996948\n",
      "epoch 7: loss=1.4343432188034058\n",
      "epoch 8: loss=1.4030522108078003\n",
      "epoch 9: loss=1.3692927360534668\n",
      "epoch 10: loss=1.3349652290344238\n",
      "epoch 11: loss=1.2912952899932861\n",
      "epoch 12: loss=1.2496116161346436\n",
      "epoch 13: loss=1.2004550695419312\n",
      "epoch 14: loss=1.161678671836853\n",
      "epoch 15: loss=1.1392935514450073\n",
      "epoch 16: loss=1.1313217878341675\n",
      "epoch 17: loss=1.13222336769104\n",
      "epoch 18: loss=1.1346012353897095\n",
      "epoch 19: loss=1.1322954893112183\n",
      "epoch 20: loss=1.1224855184555054\n",
      "epoch 21: loss=1.1039631366729736\n",
      "epoch 22: loss=1.0837433338165283\n",
      "epoch 23: loss=1.066891074180603\n",
      "epoch 24: loss=1.050665020942688\n",
      "epoch 25: loss=1.038611888885498\n",
      "epoch 26: loss=1.0322219133377075\n",
      "epoch 27: loss=1.029678463935852\n",
      "epoch 28: loss=1.026459813117981\n",
      "epoch 29: loss=1.023533821105957\n",
      "epoch 30: loss=1.0183037519454956\n",
      "epoch 31: loss=1.0103727579116821\n",
      "epoch 32: loss=1.0023537874221802\n",
      "epoch 33: loss=0.993863582611084\n",
      "epoch 34: loss=0.9855190515518188\n",
      "epoch 35: loss=0.9810776710510254\n",
      "epoch 36: loss=0.9766746163368225\n",
      "epoch 37: loss=0.9732146859169006\n",
      "epoch 38: loss=0.9696533679962158\n",
      "epoch 39: loss=0.9670805931091309\n",
      "epoch 40: loss=0.9631709456443787\n",
      "epoch 41: loss=0.9601436853408813\n",
      "epoch 42: loss=0.9557927250862122\n",
      "epoch 43: loss=0.9522432684898376\n",
      "epoch 44: loss=0.9490628242492676\n",
      "epoch 45: loss=0.945188045501709\n",
      "epoch 46: loss=0.94450843334198\n",
      "epoch 47: loss=0.9428472518920898\n",
      "epoch 48: loss=0.9420617818832397\n",
      "epoch 49: loss=0.9416607022285461\n",
      "epoch 0: loss=1.7706576585769653\n",
      "epoch 1: loss=1.6772953271865845\n",
      "epoch 2: loss=1.6232340335845947\n",
      "epoch 3: loss=1.5614254474639893\n",
      "epoch 4: loss=1.5203059911727905\n",
      "epoch 5: loss=1.477824330329895\n",
      "epoch 6: loss=1.4406204223632812\n",
      "epoch 7: loss=1.3907309770584106\n",
      "epoch 8: loss=1.3443797826766968\n",
      "epoch 9: loss=1.297407865524292\n",
      "epoch 10: loss=1.2523958683013916\n",
      "epoch 11: loss=1.2023237943649292\n",
      "epoch 12: loss=1.1720930337905884\n",
      "epoch 13: loss=1.1534885168075562\n",
      "epoch 14: loss=1.1494197845458984\n",
      "epoch 15: loss=1.1482685804367065\n",
      "epoch 16: loss=1.138628602027893\n",
      "epoch 17: loss=1.1247512102127075\n",
      "epoch 18: loss=1.1021780967712402\n",
      "epoch 19: loss=1.082074761390686\n",
      "epoch 20: loss=1.0630484819412231\n",
      "epoch 21: loss=1.0494474172592163\n",
      "epoch 22: loss=1.0362191200256348\n",
      "epoch 23: loss=1.0293813943862915\n",
      "epoch 24: loss=1.0252115726470947\n",
      "epoch 25: loss=1.0198270082473755\n",
      "epoch 26: loss=1.0113569498062134\n",
      "epoch 27: loss=1.0007963180541992\n",
      "epoch 28: loss=0.9912202954292297\n",
      "epoch 29: loss=0.9807689785957336\n",
      "epoch 30: loss=0.9719057083129883\n",
      "epoch 31: loss=0.9654914140701294\n",
      "epoch 32: loss=0.9619557857513428\n",
      "epoch 33: loss=0.9586421251296997\n",
      "epoch 34: loss=0.9528495073318481\n",
      "epoch 35: loss=0.9512733221054077\n",
      "epoch 36: loss=0.9473656415939331\n",
      "epoch 37: loss=0.9434205889701843\n",
      "epoch 38: loss=0.9388472437858582\n",
      "epoch 39: loss=0.936368465423584\n",
      "epoch 40: loss=0.9352003335952759\n",
      "epoch 41: loss=0.9351092576980591\n",
      "epoch 42: loss=0.9355126619338989\n",
      "epoch 43: loss=0.9347080588340759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44: loss=0.9352750778198242\n",
      "epoch 45: loss=0.9355084300041199\n",
      "epoch 46: loss=0.933463990688324\n",
      "epoch 47: loss=0.9334523677825928\n",
      "epoch 48: loss=0.9326539635658264\n",
      "epoch 49: loss=0.9337729811668396\n",
      "epoch 0: loss=1.7389259338378906\n",
      "epoch 1: loss=1.6948637962341309\n",
      "epoch 2: loss=1.6167941093444824\n",
      "epoch 3: loss=1.573102593421936\n",
      "epoch 4: loss=1.5186173915863037\n",
      "epoch 5: loss=1.4710509777069092\n",
      "epoch 6: loss=1.4128165245056152\n",
      "epoch 7: loss=1.35611093044281\n",
      "epoch 8: loss=1.2990716695785522\n",
      "epoch 9: loss=1.2542462348937988\n",
      "epoch 10: loss=1.2073475122451782\n",
      "epoch 11: loss=1.1784305572509766\n",
      "epoch 12: loss=1.1610456705093384\n",
      "epoch 13: loss=1.1539485454559326\n",
      "epoch 14: loss=1.1405702829360962\n",
      "epoch 15: loss=1.1293916702270508\n",
      "epoch 16: loss=1.1115447282791138\n",
      "epoch 17: loss=1.0906119346618652\n",
      "epoch 18: loss=1.0712653398513794\n",
      "epoch 19: loss=1.0535056591033936\n",
      "epoch 20: loss=1.0385147333145142\n",
      "epoch 21: loss=1.0278640985488892\n",
      "epoch 22: loss=1.0184701681137085\n",
      "epoch 23: loss=1.0133486986160278\n",
      "epoch 24: loss=1.0062652826309204\n",
      "epoch 25: loss=0.9987384080886841\n",
      "epoch 26: loss=0.9903119802474976\n",
      "epoch 27: loss=0.9833949208259583\n",
      "epoch 28: loss=0.9737803339958191\n",
      "epoch 29: loss=0.9652590155601501\n",
      "epoch 30: loss=0.9589258432388306\n",
      "epoch 31: loss=0.9547443389892578\n",
      "epoch 32: loss=0.95086669921875\n",
      "epoch 33: loss=0.9500888586044312\n",
      "epoch 34: loss=0.9457571506500244\n",
      "epoch 35: loss=0.9435473680496216\n",
      "epoch 36: loss=0.9403243064880371\n",
      "epoch 37: loss=0.9378587007522583\n",
      "epoch 38: loss=0.9358099699020386\n",
      "epoch 39: loss=0.9352445006370544\n",
      "epoch 40: loss=0.93706876039505\n",
      "epoch 41: loss=0.935177743434906\n",
      "epoch 42: loss=0.9343333840370178\n",
      "epoch 43: loss=0.9344276785850525\n",
      "epoch 44: loss=0.9324430823326111\n",
      "epoch 45: loss=0.9311268329620361\n",
      "epoch 46: loss=0.9315528869628906\n",
      "epoch 47: loss=0.9307696223258972\n",
      "epoch 48: loss=0.9295260310173035\n",
      "epoch 49: loss=0.927985429763794\n",
      "epoch 0: loss=1.7227152585983276\n",
      "epoch 1: loss=1.6298514604568481\n",
      "epoch 2: loss=1.571543574333191\n",
      "epoch 3: loss=1.5234992504119873\n",
      "epoch 4: loss=1.4828747510910034\n",
      "epoch 5: loss=1.450535535812378\n",
      "epoch 6: loss=1.4114022254943848\n",
      "epoch 7: loss=1.3619675636291504\n",
      "epoch 8: loss=1.3199807405471802\n",
      "epoch 9: loss=1.2622153759002686\n",
      "epoch 10: loss=1.208276629447937\n",
      "epoch 11: loss=1.1677416563034058\n",
      "epoch 12: loss=1.1479175090789795\n",
      "epoch 13: loss=1.144405722618103\n",
      "epoch 14: loss=1.1420387029647827\n",
      "epoch 15: loss=1.1388746500015259\n",
      "epoch 16: loss=1.1227089166641235\n",
      "epoch 17: loss=1.1054104566574097\n",
      "epoch 18: loss=1.0868271589279175\n",
      "epoch 19: loss=1.0628350973129272\n",
      "epoch 20: loss=1.0440853834152222\n",
      "epoch 21: loss=1.0309828519821167\n",
      "epoch 22: loss=1.0250929594039917\n",
      "epoch 23: loss=1.020498275756836\n",
      "epoch 24: loss=1.0173583030700684\n",
      "epoch 25: loss=1.011928915977478\n",
      "epoch 26: loss=1.005478024482727\n",
      "epoch 27: loss=0.9964861273765564\n",
      "epoch 28: loss=0.9864438772201538\n",
      "epoch 29: loss=0.9772242307662964\n",
      "epoch 30: loss=0.9686434864997864\n",
      "epoch 31: loss=0.961453378200531\n",
      "epoch 32: loss=0.9572022557258606\n",
      "epoch 33: loss=0.9529853463172913\n",
      "epoch 34: loss=0.9510378241539001\n",
      "epoch 35: loss=0.9484548568725586\n",
      "epoch 36: loss=0.9453810453414917\n",
      "epoch 37: loss=0.9413871765136719\n",
      "epoch 38: loss=0.9386264681816101\n",
      "epoch 39: loss=0.9353348612785339\n",
      "epoch 40: loss=0.9332505464553833\n",
      "epoch 41: loss=0.9319376349449158\n",
      "epoch 42: loss=0.9322295188903809\n",
      "epoch 43: loss=0.9326270818710327\n",
      "epoch 44: loss=0.9336021542549133\n",
      "epoch 45: loss=0.9334004521369934\n",
      "epoch 46: loss=0.9316930174827576\n",
      "epoch 47: loss=0.9308578372001648\n",
      "epoch 48: loss=0.9293972253799438\n",
      "epoch 49: loss=0.9292927384376526\n",
      "epoch 0: loss=1.7232000827789307\n",
      "epoch 1: loss=1.667994499206543\n",
      "epoch 2: loss=1.6005640029907227\n",
      "epoch 3: loss=1.5355169773101807\n",
      "epoch 4: loss=1.4875226020812988\n",
      "epoch 5: loss=1.422910451889038\n",
      "epoch 6: loss=1.3636610507965088\n",
      "epoch 7: loss=1.2890821695327759\n",
      "epoch 8: loss=1.2351685762405396\n",
      "epoch 9: loss=1.1869927644729614\n",
      "epoch 10: loss=1.1631107330322266\n",
      "epoch 11: loss=1.1577078104019165\n",
      "epoch 12: loss=1.1509954929351807\n",
      "epoch 13: loss=1.1334874629974365\n",
      "epoch 14: loss=1.110980749130249\n",
      "epoch 15: loss=1.0899630784988403\n",
      "epoch 16: loss=1.0664833784103394\n",
      "epoch 17: loss=1.0454049110412598\n",
      "epoch 18: loss=1.028316617012024\n",
      "epoch 19: loss=1.0165973901748657\n",
      "epoch 20: loss=1.0090157985687256\n",
      "epoch 21: loss=1.0050989389419556\n",
      "epoch 22: loss=1.0008403062820435\n",
      "epoch 23: loss=0.9952810406684875\n",
      "epoch 24: loss=0.9873530864715576\n",
      "epoch 25: loss=0.9783857464790344\n",
      "epoch 26: loss=0.97035151720047\n",
      "epoch 27: loss=0.9616717100143433\n",
      "epoch 28: loss=0.9564011693000793\n",
      "epoch 29: loss=0.9516717195510864\n",
      "epoch 30: loss=0.9462074041366577\n",
      "epoch 31: loss=0.9447813034057617\n",
      "epoch 32: loss=0.9427205324172974\n",
      "epoch 33: loss=0.9409279823303223\n",
      "epoch 34: loss=0.9379605650901794\n",
      "epoch 35: loss=0.9361074566841125\n",
      "epoch 36: loss=0.9330345392227173\n",
      "epoch 37: loss=0.9324288368225098\n",
      "epoch 38: loss=0.9323081374168396\n",
      "epoch 39: loss=0.9309569001197815\n",
      "epoch 40: loss=0.9310969114303589\n",
      "epoch 41: loss=0.931329607963562\n",
      "epoch 42: loss=0.93116295337677\n",
      "epoch 43: loss=0.930137574672699\n",
      "epoch 44: loss=0.9287863969802856\n",
      "epoch 45: loss=0.9276983737945557\n",
      "epoch 46: loss=0.9286269545555115\n",
      "epoch 47: loss=0.9276900291442871\n",
      "epoch 48: loss=0.9261215329170227\n",
      "epoch 49: loss=0.927238404750824\n",
      "epoch 0: loss=1.717278003692627\n",
      "epoch 1: loss=1.6558536291122437\n",
      "epoch 2: loss=1.5833979845046997\n",
      "epoch 3: loss=1.5293974876403809\n",
      "epoch 4: loss=1.4912614822387695\n",
      "epoch 5: loss=1.4370081424713135\n",
      "epoch 6: loss=1.3930939435958862\n",
      "epoch 7: loss=1.3360111713409424\n",
      "epoch 8: loss=1.2771774530410767\n",
      "epoch 9: loss=1.21718168258667\n",
      "epoch 10: loss=1.1707836389541626\n",
      "epoch 11: loss=1.1414324045181274\n",
      "epoch 12: loss=1.125510573387146\n",
      "epoch 13: loss=1.1242300271987915\n",
      "epoch 14: loss=1.1139461994171143\n",
      "epoch 15: loss=1.1043751239776611\n",
      "epoch 16: loss=1.085540771484375\n",
      "epoch 17: loss=1.0633015632629395\n",
      "epoch 18: loss=1.0415682792663574\n",
      "epoch 19: loss=1.023511290550232\n",
      "epoch 20: loss=1.0090004205703735\n",
      "epoch 21: loss=1.0006834268569946\n",
      "epoch 22: loss=0.9948763251304626\n",
      "epoch 23: loss=0.9910589456558228\n",
      "epoch 24: loss=0.9877296090126038\n",
      "epoch 25: loss=0.9821834564208984\n",
      "epoch 26: loss=0.9758935570716858\n",
      "epoch 27: loss=0.9679135680198669\n",
      "epoch 28: loss=0.9581352472305298\n",
      "epoch 29: loss=0.9523082971572876\n",
      "epoch 30: loss=0.9470706582069397\n",
      "epoch 31: loss=0.9421005845069885\n",
      "epoch 32: loss=0.9395657777786255\n",
      "epoch 33: loss=0.9394969940185547\n",
      "epoch 34: loss=0.9391643404960632\n",
      "epoch 35: loss=0.937612771987915\n",
      "epoch 36: loss=0.9377451539039612\n",
      "epoch 37: loss=0.9362339973449707\n",
      "epoch 38: loss=0.9354336261749268\n",
      "epoch 39: loss=0.9341900944709778\n",
      "epoch 40: loss=0.9339728951454163\n",
      "epoch 41: loss=0.9328222870826721\n",
      "epoch 42: loss=0.933972179889679\n",
      "epoch 43: loss=0.9332236647605896\n",
      "epoch 44: loss=0.9328518509864807\n",
      "epoch 45: loss=0.9311546683311462\n",
      "epoch 46: loss=0.9290493726730347\n",
      "epoch 47: loss=0.9284370541572571\n",
      "epoch 48: loss=0.927361011505127\n",
      "epoch 49: loss=0.9263158440589905\n",
      "epoch 0: loss=1.7104401588439941\n",
      "epoch 1: loss=1.6307772397994995\n",
      "epoch 2: loss=1.5882915258407593\n",
      "epoch 3: loss=1.5269936323165894\n",
      "epoch 4: loss=1.4731310606002808\n",
      "epoch 5: loss=1.4229402542114258\n",
      "epoch 6: loss=1.3633404970169067\n",
      "epoch 7: loss=1.2939156293869019\n",
      "epoch 8: loss=1.2199362516403198\n",
      "epoch 9: loss=1.1705009937286377\n",
      "epoch 10: loss=1.146314024925232\n",
      "epoch 11: loss=1.1388977766036987\n",
      "epoch 12: loss=1.1355518102645874\n",
      "epoch 13: loss=1.122870683670044\n",
      "epoch 14: loss=1.1003305912017822\n",
      "epoch 15: loss=1.070049524307251\n",
      "epoch 16: loss=1.047221064567566\n",
      "epoch 17: loss=1.0246293544769287\n",
      "epoch 18: loss=1.012070894241333\n",
      "epoch 19: loss=1.002624273300171\n",
      "epoch 20: loss=1.0022608041763306\n",
      "epoch 21: loss=0.9965840578079224\n",
      "epoch 22: loss=0.9901256561279297\n",
      "epoch 23: loss=0.9796106815338135\n",
      "epoch 24: loss=0.9705883264541626\n",
      "epoch 25: loss=0.959132194519043\n",
      "epoch 26: loss=0.952640175819397\n",
      "epoch 27: loss=0.9474422931671143\n",
      "epoch 28: loss=0.9450364112854004\n",
      "epoch 29: loss=0.9451557397842407\n",
      "epoch 30: loss=0.9456917643547058\n",
      "epoch 31: loss=0.9429433345794678\n",
      "epoch 32: loss=0.9399946331977844\n",
      "epoch 33: loss=0.9383699297904968\n",
      "epoch 34: loss=0.9373303055763245\n",
      "epoch 35: loss=0.9373076558113098\n",
      "epoch 36: loss=0.9375364184379578\n",
      "epoch 37: loss=0.9379848837852478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: loss=0.9365001320838928\n",
      "epoch 39: loss=0.9370334148406982\n",
      "epoch 40: loss=0.9359393119812012\n",
      "epoch 41: loss=0.9343234300613403\n",
      "epoch 42: loss=0.9329422116279602\n",
      "epoch 43: loss=0.9316033124923706\n",
      "epoch 44: loss=0.9313104152679443\n",
      "epoch 45: loss=0.9305852055549622\n",
      "epoch 46: loss=0.9306595325469971\n",
      "epoch 47: loss=0.9288727045059204\n",
      "epoch 48: loss=0.9277167916297913\n",
      "epoch 49: loss=0.9272453188896179\n",
      "epoch 0: loss=1.7050737142562866\n",
      "epoch 1: loss=1.6376488208770752\n",
      "epoch 2: loss=1.5744062662124634\n",
      "epoch 3: loss=1.5261212587356567\n",
      "epoch 4: loss=1.4768846035003662\n",
      "epoch 5: loss=1.4269887208938599\n",
      "epoch 6: loss=1.3687924146652222\n",
      "epoch 7: loss=1.3054782152175903\n",
      "epoch 8: loss=1.2424440383911133\n",
      "epoch 9: loss=1.1964812278747559\n",
      "epoch 10: loss=1.1616384983062744\n",
      "epoch 11: loss=1.1421606540679932\n",
      "epoch 12: loss=1.1386628150939941\n",
      "epoch 13: loss=1.1268794536590576\n",
      "epoch 14: loss=1.117405652999878\n",
      "epoch 15: loss=1.0948486328125\n",
      "epoch 16: loss=1.0710431337356567\n",
      "epoch 17: loss=1.0473883152008057\n",
      "epoch 18: loss=1.0284343957901\n",
      "epoch 19: loss=1.0119781494140625\n",
      "epoch 20: loss=1.0004463195800781\n",
      "epoch 21: loss=0.9956490993499756\n",
      "epoch 22: loss=0.9915499091148376\n",
      "epoch 23: loss=0.986836314201355\n",
      "epoch 24: loss=0.9832322001457214\n",
      "epoch 25: loss=0.9741748571395874\n",
      "epoch 26: loss=0.9672244787216187\n",
      "epoch 27: loss=0.9587283134460449\n",
      "epoch 28: loss=0.9519281983375549\n",
      "epoch 29: loss=0.9472989439964294\n",
      "epoch 30: loss=0.9419140815734863\n",
      "epoch 31: loss=0.93962562084198\n",
      "epoch 32: loss=0.9377375841140747\n",
      "epoch 33: loss=0.9367519021034241\n",
      "epoch 34: loss=0.9354891180992126\n",
      "epoch 35: loss=0.9355797171592712\n",
      "epoch 36: loss=0.9336397051811218\n",
      "epoch 37: loss=0.9329517483711243\n",
      "epoch 38: loss=0.9321809411048889\n",
      "epoch 39: loss=0.931730329990387\n",
      "epoch 40: loss=0.9319351315498352\n",
      "epoch 41: loss=0.9316057562828064\n",
      "epoch 42: loss=0.9326006174087524\n",
      "epoch 43: loss=0.9317374229431152\n",
      "epoch 44: loss=0.9298281669616699\n",
      "epoch 45: loss=0.9279659390449524\n",
      "epoch 46: loss=0.9280505180358887\n",
      "epoch 47: loss=0.9258759021759033\n",
      "epoch 48: loss=0.9254517555236816\n",
      "epoch 49: loss=0.9251490235328674\n",
      "epoch 0: loss=1.700144648551941\n",
      "epoch 1: loss=1.6324974298477173\n",
      "epoch 2: loss=1.5546166896820068\n",
      "epoch 3: loss=1.5129963159561157\n",
      "epoch 4: loss=1.4484366178512573\n",
      "epoch 5: loss=1.3750022649765015\n",
      "epoch 6: loss=1.2996749877929688\n",
      "epoch 7: loss=1.2372013330459595\n",
      "epoch 8: loss=1.1805572509765625\n",
      "epoch 9: loss=1.146293044090271\n",
      "epoch 10: loss=1.1265243291854858\n",
      "epoch 11: loss=1.113480806350708\n",
      "epoch 12: loss=1.1007941961288452\n",
      "epoch 13: loss=1.082564353942871\n",
      "epoch 14: loss=1.0598115921020508\n",
      "epoch 15: loss=1.0401700735092163\n",
      "epoch 16: loss=1.0160160064697266\n",
      "epoch 17: loss=0.9987773299217224\n",
      "epoch 18: loss=0.9848369359970093\n",
      "epoch 19: loss=0.9770268797874451\n",
      "epoch 20: loss=0.9734176993370056\n",
      "epoch 21: loss=0.9708107113838196\n",
      "epoch 22: loss=0.9670227766036987\n",
      "epoch 23: loss=0.9633790850639343\n",
      "epoch 24: loss=0.9548184275627136\n",
      "epoch 25: loss=0.9472270011901855\n",
      "epoch 26: loss=0.9420589804649353\n",
      "epoch 27: loss=0.9373703598976135\n",
      "epoch 28: loss=0.9378782510757446\n",
      "epoch 29: loss=0.9364230036735535\n",
      "epoch 30: loss=0.9369544982910156\n",
      "epoch 31: loss=0.9343538284301758\n",
      "epoch 32: loss=0.9355989694595337\n",
      "epoch 33: loss=0.934698224067688\n",
      "epoch 34: loss=0.9319814443588257\n",
      "epoch 35: loss=0.9304525852203369\n",
      "epoch 36: loss=0.9291043281555176\n",
      "epoch 37: loss=0.9278905391693115\n",
      "epoch 38: loss=0.9280751347541809\n",
      "epoch 39: loss=0.9273115396499634\n",
      "epoch 40: loss=0.9260966181755066\n",
      "epoch 41: loss=0.9253860116004944\n",
      "epoch 42: loss=0.9238519668579102\n",
      "epoch 43: loss=0.9223897457122803\n",
      "epoch 44: loss=0.9217454791069031\n",
      "epoch 45: loss=0.9206681847572327\n",
      "epoch 46: loss=0.9211342930793762\n",
      "epoch 47: loss=0.9205003976821899\n",
      "epoch 48: loss=0.9194298982620239\n",
      "epoch 49: loss=0.9188878536224365\n",
      "epoch 0: loss=1.809547781944275\n",
      "epoch 1: loss=1.722845196723938\n",
      "epoch 2: loss=1.6718941926956177\n",
      "epoch 3: loss=1.5766054391860962\n",
      "epoch 4: loss=1.5403380393981934\n",
      "epoch 5: loss=1.4829519987106323\n",
      "epoch 6: loss=1.4251288175582886\n",
      "epoch 7: loss=1.3819632530212402\n",
      "epoch 8: loss=1.3246852159500122\n",
      "epoch 9: loss=1.2695475816726685\n",
      "epoch 10: loss=1.2178633213043213\n",
      "epoch 11: loss=1.177564024925232\n",
      "epoch 12: loss=1.1505279541015625\n",
      "epoch 13: loss=1.1355706453323364\n",
      "epoch 14: loss=1.1218630075454712\n",
      "epoch 15: loss=1.1118111610412598\n",
      "epoch 16: loss=1.0925489664077759\n",
      "epoch 17: loss=1.0735862255096436\n",
      "epoch 18: loss=1.0512055158615112\n",
      "epoch 19: loss=1.0292271375656128\n",
      "epoch 20: loss=1.0152959823608398\n",
      "epoch 21: loss=1.0052472352981567\n",
      "epoch 22: loss=0.9986417293548584\n",
      "epoch 23: loss=0.9920943975448608\n",
      "epoch 24: loss=0.9863340258598328\n",
      "epoch 25: loss=0.9783678650856018\n",
      "epoch 26: loss=0.9678958654403687\n",
      "epoch 27: loss=0.9595932364463806\n",
      "epoch 28: loss=0.9516328573226929\n",
      "epoch 29: loss=0.9472978711128235\n",
      "epoch 30: loss=0.9441769123077393\n",
      "epoch 31: loss=0.943313717842102\n",
      "epoch 32: loss=0.9411691427230835\n",
      "epoch 33: loss=0.9405131340026855\n",
      "epoch 34: loss=0.938784658908844\n",
      "epoch 35: loss=0.9360517859458923\n",
      "epoch 36: loss=0.9348650574684143\n",
      "epoch 37: loss=0.934887707233429\n",
      "epoch 38: loss=0.9357253909111023\n",
      "epoch 39: loss=0.9363821148872375\n",
      "epoch 40: loss=0.9344281554222107\n",
      "epoch 41: loss=0.93418288230896\n",
      "epoch 42: loss=0.9339655637741089\n",
      "epoch 43: loss=0.9327828288078308\n",
      "epoch 44: loss=0.9308515191078186\n",
      "epoch 45: loss=0.9298487305641174\n",
      "epoch 46: loss=0.9292482733726501\n",
      "epoch 47: loss=0.9277639389038086\n",
      "epoch 48: loss=0.9272328019142151\n",
      "epoch 49: loss=0.9266626834869385\n",
      "epoch 0: loss=1.704817533493042\n",
      "epoch 1: loss=1.6113207340240479\n",
      "epoch 2: loss=1.5493108034133911\n",
      "epoch 3: loss=1.5027186870574951\n",
      "epoch 4: loss=1.4506683349609375\n",
      "epoch 5: loss=1.403773307800293\n",
      "epoch 6: loss=1.3504279851913452\n",
      "epoch 7: loss=1.2978301048278809\n",
      "epoch 8: loss=1.2448214292526245\n",
      "epoch 9: loss=1.1986091136932373\n",
      "epoch 10: loss=1.1630440950393677\n",
      "epoch 11: loss=1.1510590314865112\n",
      "epoch 12: loss=1.1414936780929565\n",
      "epoch 13: loss=1.1319526433944702\n",
      "epoch 14: loss=1.122665524482727\n",
      "epoch 15: loss=1.1116142272949219\n",
      "epoch 16: loss=1.0923796892166138\n",
      "epoch 17: loss=1.073461651802063\n",
      "epoch 18: loss=1.0523555278778076\n",
      "epoch 19: loss=1.03627347946167\n",
      "epoch 20: loss=1.0222877264022827\n",
      "epoch 21: loss=1.013472318649292\n",
      "epoch 22: loss=1.0077009201049805\n",
      "epoch 23: loss=1.0021870136260986\n",
      "epoch 24: loss=0.9961678385734558\n",
      "epoch 25: loss=0.9900453090667725\n",
      "epoch 26: loss=0.9847400188446045\n",
      "epoch 27: loss=0.9754845499992371\n",
      "epoch 28: loss=0.9683526754379272\n",
      "epoch 29: loss=0.9606654047966003\n",
      "epoch 30: loss=0.9555025100708008\n",
      "epoch 31: loss=0.950513482093811\n",
      "epoch 32: loss=0.9472128748893738\n",
      "epoch 33: loss=0.9457043409347534\n",
      "epoch 34: loss=0.943253755569458\n",
      "epoch 35: loss=0.9414352178573608\n",
      "epoch 36: loss=0.9391705393791199\n",
      "epoch 37: loss=0.9387223124504089\n",
      "epoch 38: loss=0.9370149970054626\n",
      "epoch 39: loss=0.9369701147079468\n",
      "epoch 40: loss=0.9358094930648804\n",
      "epoch 41: loss=0.9355613589286804\n",
      "epoch 42: loss=0.9347513914108276\n",
      "epoch 43: loss=0.9355696439743042\n",
      "epoch 44: loss=0.9338054060935974\n",
      "epoch 45: loss=0.9321655631065369\n",
      "epoch 46: loss=0.9314568042755127\n",
      "epoch 47: loss=0.9295991659164429\n",
      "epoch 48: loss=0.9292343258857727\n",
      "epoch 49: loss=0.928642988204956\n",
      "epoch 0: loss=1.860788345336914\n",
      "epoch 1: loss=1.725531816482544\n",
      "epoch 2: loss=1.645790934562683\n",
      "epoch 3: loss=1.5832964181900024\n",
      "epoch 4: loss=1.535215973854065\n",
      "epoch 5: loss=1.4929943084716797\n",
      "epoch 6: loss=1.4547812938690186\n",
      "epoch 7: loss=1.4198192358016968\n",
      "epoch 8: loss=1.3699661493301392\n",
      "epoch 9: loss=1.3219184875488281\n",
      "epoch 10: loss=1.2751662731170654\n",
      "epoch 11: loss=1.2316557168960571\n",
      "epoch 12: loss=1.1907612085342407\n",
      "epoch 13: loss=1.1634825468063354\n",
      "epoch 14: loss=1.158358097076416\n",
      "epoch 15: loss=1.148330569267273\n",
      "epoch 16: loss=1.1460179090499878\n",
      "epoch 17: loss=1.1390119791030884\n",
      "epoch 18: loss=1.1277083158493042\n",
      "epoch 19: loss=1.1077017784118652\n",
      "epoch 20: loss=1.0902347564697266\n",
      "epoch 21: loss=1.072471022605896\n",
      "epoch 22: loss=1.0560753345489502\n",
      "epoch 23: loss=1.0422155857086182\n",
      "epoch 24: loss=1.0332163572311401\n",
      "epoch 25: loss=1.0245980024337769\n",
      "epoch 26: loss=1.0207953453063965\n",
      "epoch 27: loss=1.0171982049942017\n",
      "epoch 28: loss=1.0128580331802368\n",
      "epoch 29: loss=1.007235050201416\n",
      "epoch 30: loss=0.9988899230957031\n",
      "epoch 31: loss=0.990551233291626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32: loss=0.9831262826919556\n",
      "epoch 33: loss=0.9748022556304932\n",
      "epoch 34: loss=0.9693297743797302\n",
      "epoch 35: loss=0.9625515937805176\n",
      "epoch 36: loss=0.9589661955833435\n",
      "epoch 37: loss=0.9549975395202637\n",
      "epoch 38: loss=0.9526513814926147\n",
      "epoch 39: loss=0.94874107837677\n",
      "epoch 40: loss=0.9467877149581909\n",
      "epoch 41: loss=0.941573441028595\n",
      "epoch 42: loss=0.9408177137374878\n",
      "epoch 43: loss=0.9377127885818481\n",
      "epoch 44: loss=0.9357125163078308\n",
      "epoch 45: loss=0.9341672658920288\n",
      "epoch 46: loss=0.9337446689605713\n",
      "epoch 47: loss=0.9329743385314941\n",
      "epoch 48: loss=0.9326521754264832\n",
      "epoch 49: loss=0.9314610362052917\n",
      "epoch 0: loss=1.6553219556808472\n",
      "epoch 1: loss=1.588205337524414\n",
      "epoch 2: loss=1.536555528640747\n",
      "epoch 3: loss=1.489577293395996\n",
      "epoch 4: loss=1.4244359731674194\n",
      "epoch 5: loss=1.3610661029815674\n",
      "epoch 6: loss=1.2859240770339966\n",
      "epoch 7: loss=1.218628168106079\n",
      "epoch 8: loss=1.1706304550170898\n",
      "epoch 9: loss=1.1474460363388062\n",
      "epoch 10: loss=1.1398428678512573\n",
      "epoch 11: loss=1.137768268585205\n",
      "epoch 12: loss=1.1294962167739868\n",
      "epoch 13: loss=1.109703779220581\n",
      "epoch 14: loss=1.0873019695281982\n",
      "epoch 15: loss=1.0642836093902588\n",
      "epoch 16: loss=1.0414714813232422\n",
      "epoch 17: loss=1.0230337381362915\n",
      "epoch 18: loss=1.0120947360992432\n",
      "epoch 19: loss=1.0069111585617065\n",
      "epoch 20: loss=1.0034183263778687\n",
      "epoch 21: loss=0.9994445443153381\n",
      "epoch 22: loss=0.99459308385849\n",
      "epoch 23: loss=0.985429048538208\n",
      "epoch 24: loss=0.9761612415313721\n",
      "epoch 25: loss=0.9661122560501099\n",
      "epoch 26: loss=0.9591432213783264\n",
      "epoch 27: loss=0.9534136652946472\n",
      "epoch 28: loss=0.9495052099227905\n",
      "epoch 29: loss=0.9464269280433655\n",
      "epoch 30: loss=0.9452121257781982\n",
      "epoch 31: loss=0.9441462755203247\n",
      "epoch 32: loss=0.9423936009407043\n",
      "epoch 33: loss=0.9373825192451477\n",
      "epoch 34: loss=0.9362228512763977\n",
      "epoch 35: loss=0.9329622387886047\n",
      "epoch 36: loss=0.931441605091095\n",
      "epoch 37: loss=0.9317626357078552\n",
      "epoch 38: loss=0.9332645535469055\n",
      "epoch 39: loss=0.9325825572013855\n",
      "epoch 40: loss=0.9323536157608032\n",
      "epoch 41: loss=0.9313598871231079\n",
      "epoch 42: loss=0.9292051792144775\n",
      "epoch 43: loss=0.9299031496047974\n",
      "epoch 44: loss=0.9277241230010986\n",
      "epoch 45: loss=0.9282916188240051\n",
      "epoch 46: loss=0.9281771779060364\n",
      "epoch 47: loss=0.927159309387207\n",
      "epoch 48: loss=0.9261389970779419\n",
      "epoch 49: loss=0.924649178981781\n",
      "epoch 0: loss=1.731632947921753\n",
      "epoch 1: loss=1.6271326541900635\n",
      "epoch 2: loss=1.5448994636535645\n",
      "epoch 3: loss=1.475203037261963\n",
      "epoch 4: loss=1.4100472927093506\n",
      "epoch 5: loss=1.316038727760315\n",
      "epoch 6: loss=1.2510936260223389\n",
      "epoch 7: loss=1.2023773193359375\n",
      "epoch 8: loss=1.169853925704956\n",
      "epoch 9: loss=1.1530617475509644\n",
      "epoch 10: loss=1.1417702436447144\n",
      "epoch 11: loss=1.1197609901428223\n",
      "epoch 12: loss=1.09610116481781\n",
      "epoch 13: loss=1.0686136484146118\n",
      "epoch 14: loss=1.045645833015442\n",
      "epoch 15: loss=1.0248963832855225\n",
      "epoch 16: loss=1.012113094329834\n",
      "epoch 17: loss=1.008183240890503\n",
      "epoch 18: loss=1.0015050172805786\n",
      "epoch 19: loss=0.9959763884544373\n",
      "epoch 20: loss=0.986781656742096\n",
      "epoch 21: loss=0.9782238602638245\n",
      "epoch 22: loss=0.9681888818740845\n",
      "epoch 23: loss=0.9615756273269653\n",
      "epoch 24: loss=0.9559418559074402\n",
      "epoch 25: loss=0.9547131061553955\n",
      "epoch 26: loss=0.954219400882721\n",
      "epoch 27: loss=0.9534279108047485\n",
      "epoch 28: loss=0.9521456956863403\n",
      "epoch 29: loss=0.950779378414154\n",
      "epoch 30: loss=0.9488675594329834\n",
      "epoch 31: loss=0.9463096857070923\n",
      "epoch 32: loss=0.9455695152282715\n",
      "epoch 33: loss=0.9438714385032654\n",
      "epoch 34: loss=0.942990243434906\n",
      "epoch 35: loss=0.9432103633880615\n",
      "epoch 36: loss=0.9415538311004639\n",
      "epoch 37: loss=0.9408401250839233\n",
      "epoch 38: loss=0.9396719336509705\n",
      "epoch 39: loss=0.937217652797699\n",
      "epoch 40: loss=0.9358896613121033\n",
      "epoch 41: loss=0.9339727759361267\n",
      "epoch 42: loss=0.9341636896133423\n",
      "epoch 43: loss=0.9332762360572815\n",
      "epoch 44: loss=0.9329394698143005\n",
      "epoch 45: loss=0.9318877458572388\n",
      "epoch 46: loss=0.9296491146087646\n",
      "epoch 47: loss=0.929609477519989\n",
      "epoch 48: loss=0.9288243651390076\n",
      "epoch 49: loss=0.9287036061286926\n",
      "epoch 0: loss=1.7096498012542725\n",
      "epoch 1: loss=1.627943992614746\n",
      "epoch 2: loss=1.5605179071426392\n",
      "epoch 3: loss=1.5052610635757446\n",
      "epoch 4: loss=1.4474408626556396\n",
      "epoch 5: loss=1.3858709335327148\n",
      "epoch 6: loss=1.325072169303894\n",
      "epoch 7: loss=1.2643921375274658\n",
      "epoch 8: loss=1.2036314010620117\n",
      "epoch 9: loss=1.1695255041122437\n",
      "epoch 10: loss=1.1406141519546509\n",
      "epoch 11: loss=1.1337352991104126\n",
      "epoch 12: loss=1.122096061706543\n",
      "epoch 13: loss=1.108947992324829\n",
      "epoch 14: loss=1.0936747789382935\n",
      "epoch 15: loss=1.0706846714019775\n",
      "epoch 16: loss=1.0496801137924194\n",
      "epoch 17: loss=1.0303311347961426\n",
      "epoch 18: loss=1.0164968967437744\n",
      "epoch 19: loss=1.009357213973999\n",
      "epoch 20: loss=1.002427577972412\n",
      "epoch 21: loss=0.9990395307540894\n",
      "epoch 22: loss=0.9963907599449158\n",
      "epoch 23: loss=0.9872952103614807\n",
      "epoch 24: loss=0.9792138934135437\n",
      "epoch 25: loss=0.9697770476341248\n",
      "epoch 26: loss=0.9605712294578552\n",
      "epoch 27: loss=0.9563806056976318\n",
      "epoch 28: loss=0.9524989724159241\n",
      "epoch 29: loss=0.9498714208602905\n",
      "epoch 30: loss=0.9493328332901001\n",
      "epoch 31: loss=0.9496704936027527\n",
      "epoch 32: loss=0.9477906227111816\n",
      "epoch 33: loss=0.9462488889694214\n",
      "epoch 34: loss=0.944007933139801\n",
      "epoch 35: loss=0.9414256811141968\n",
      "epoch 36: loss=0.9416100382804871\n",
      "epoch 37: loss=0.9418516755104065\n",
      "epoch 38: loss=0.9412983655929565\n",
      "epoch 39: loss=0.9411798119544983\n",
      "epoch 40: loss=0.942918598651886\n",
      "epoch 41: loss=0.9407948851585388\n",
      "epoch 42: loss=0.9383141398429871\n",
      "epoch 43: loss=0.937693178653717\n",
      "epoch 44: loss=0.9370638132095337\n",
      "epoch 45: loss=0.9361069798469543\n",
      "epoch 46: loss=0.9370331168174744\n",
      "epoch 47: loss=0.9344497323036194\n",
      "epoch 48: loss=0.9327690005302429\n",
      "epoch 49: loss=0.932904064655304\n",
      "epoch 0: loss=1.700291395187378\n",
      "epoch 1: loss=1.6342953443527222\n",
      "epoch 2: loss=1.5743695497512817\n",
      "epoch 3: loss=1.5169401168823242\n",
      "epoch 4: loss=1.4749518632888794\n",
      "epoch 5: loss=1.42063307762146\n",
      "epoch 6: loss=1.3650134801864624\n",
      "epoch 7: loss=1.30029296875\n",
      "epoch 8: loss=1.2399017810821533\n",
      "epoch 9: loss=1.189526915550232\n",
      "epoch 10: loss=1.158107042312622\n",
      "epoch 11: loss=1.1369975805282593\n",
      "epoch 12: loss=1.1360822916030884\n",
      "epoch 13: loss=1.1261332035064697\n",
      "epoch 14: loss=1.1111935377120972\n",
      "epoch 15: loss=1.0884543657302856\n",
      "epoch 16: loss=1.065619945526123\n",
      "epoch 17: loss=1.0410128831863403\n",
      "epoch 18: loss=1.0203191041946411\n",
      "epoch 19: loss=1.0043741464614868\n",
      "epoch 20: loss=0.9944119453430176\n",
      "epoch 21: loss=0.9897691011428833\n",
      "epoch 22: loss=0.9858709573745728\n",
      "epoch 23: loss=0.982074499130249\n",
      "epoch 24: loss=0.9750921130180359\n",
      "epoch 25: loss=0.9671435952186584\n",
      "epoch 26: loss=0.9599519968032837\n",
      "epoch 27: loss=0.9516918659210205\n",
      "epoch 28: loss=0.9445809721946716\n",
      "epoch 29: loss=0.9399081468582153\n",
      "epoch 30: loss=0.937252938747406\n",
      "epoch 31: loss=0.9374376535415649\n",
      "epoch 32: loss=0.9374133944511414\n",
      "epoch 33: loss=0.9366936087608337\n",
      "epoch 34: loss=0.935663104057312\n",
      "epoch 35: loss=0.9351430535316467\n",
      "epoch 36: loss=0.9331572651863098\n",
      "epoch 37: loss=0.9320517182350159\n",
      "epoch 38: loss=0.9323941469192505\n",
      "epoch 39: loss=0.9320233464241028\n",
      "epoch 40: loss=0.931486964225769\n",
      "epoch 41: loss=0.9308948516845703\n",
      "epoch 42: loss=0.9300693869590759\n",
      "epoch 43: loss=0.9293466806411743\n",
      "epoch 44: loss=0.9278849363327026\n",
      "epoch 45: loss=0.9265830516815186\n",
      "epoch 46: loss=0.9257563948631287\n",
      "epoch 47: loss=0.9234542846679688\n",
      "epoch 48: loss=0.9233250617980957\n",
      "epoch 49: loss=0.9229824542999268\n",
      "epoch 0: loss=1.7117719650268555\n",
      "epoch 1: loss=1.6482561826705933\n",
      "epoch 2: loss=1.5746146440505981\n",
      "epoch 3: loss=1.520726203918457\n",
      "epoch 4: loss=1.4616714715957642\n",
      "epoch 5: loss=1.4015886783599854\n",
      "epoch 6: loss=1.3374985456466675\n",
      "epoch 7: loss=1.2721139192581177\n",
      "epoch 8: loss=1.2141222953796387\n",
      "epoch 9: loss=1.1734552383422852\n",
      "epoch 10: loss=1.1511600017547607\n",
      "epoch 11: loss=1.1452996730804443\n",
      "epoch 12: loss=1.1417545080184937\n",
      "epoch 13: loss=1.1314842700958252\n",
      "epoch 14: loss=1.1146447658538818\n",
      "epoch 15: loss=1.0926623344421387\n",
      "epoch 16: loss=1.0691596269607544\n",
      "epoch 17: loss=1.0488288402557373\n",
      "epoch 18: loss=1.0321165323257446\n",
      "epoch 19: loss=1.0208593606948853\n",
      "epoch 20: loss=1.0157405138015747\n",
      "epoch 21: loss=1.0132697820663452\n",
      "epoch 22: loss=1.0108137130737305\n",
      "epoch 23: loss=1.0039997100830078\n",
      "epoch 24: loss=0.9941864013671875\n",
      "epoch 25: loss=0.9840599298477173\n",
      "epoch 26: loss=0.9745558500289917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27: loss=0.9672999382019043\n",
      "epoch 28: loss=0.9616717100143433\n",
      "epoch 29: loss=0.9569382667541504\n",
      "epoch 30: loss=0.9553229808807373\n",
      "epoch 31: loss=0.9510189294815063\n",
      "epoch 32: loss=0.9486675262451172\n",
      "epoch 33: loss=0.945694088935852\n",
      "epoch 34: loss=0.9445459842681885\n",
      "epoch 35: loss=0.9397624731063843\n",
      "epoch 36: loss=0.9373486042022705\n",
      "epoch 37: loss=0.9353430867195129\n",
      "epoch 38: loss=0.9363089799880981\n",
      "epoch 39: loss=0.9354344606399536\n",
      "epoch 40: loss=0.9357889294624329\n",
      "epoch 41: loss=0.9350521564483643\n",
      "epoch 42: loss=0.9347767233848572\n",
      "epoch 43: loss=0.9345320463180542\n",
      "epoch 44: loss=0.9323269724845886\n",
      "epoch 45: loss=0.9323750734329224\n",
      "epoch 46: loss=0.9328485131263733\n",
      "epoch 47: loss=0.9303383231163025\n",
      "epoch 48: loss=0.9307072758674622\n",
      "epoch 49: loss=0.9297037124633789\n",
      "epoch 0: loss=1.8155747652053833\n",
      "epoch 1: loss=1.7099344730377197\n",
      "epoch 2: loss=1.6506571769714355\n",
      "epoch 3: loss=1.5998460054397583\n",
      "epoch 4: loss=1.5511198043823242\n",
      "epoch 5: loss=1.5012555122375488\n",
      "epoch 6: loss=1.4467693567276\n",
      "epoch 7: loss=1.4038360118865967\n",
      "epoch 8: loss=1.3549927473068237\n",
      "epoch 9: loss=1.290920615196228\n",
      "epoch 10: loss=1.2450854778289795\n",
      "epoch 11: loss=1.2065092325210571\n",
      "epoch 12: loss=1.1852705478668213\n",
      "epoch 13: loss=1.1685665845870972\n",
      "epoch 14: loss=1.1629337072372437\n",
      "epoch 15: loss=1.151672124862671\n",
      "epoch 16: loss=1.136995792388916\n",
      "epoch 17: loss=1.1110154390335083\n",
      "epoch 18: loss=1.0890837907791138\n",
      "epoch 19: loss=1.0689315795898438\n",
      "epoch 20: loss=1.0527712106704712\n",
      "epoch 21: loss=1.0391353368759155\n",
      "epoch 22: loss=1.030403733253479\n",
      "epoch 23: loss=1.0253045558929443\n",
      "epoch 24: loss=1.019540548324585\n",
      "epoch 25: loss=1.0122274160385132\n",
      "epoch 26: loss=1.0044522285461426\n",
      "epoch 27: loss=0.9939026832580566\n",
      "epoch 28: loss=0.9861658215522766\n",
      "epoch 29: loss=0.9748188853263855\n",
      "epoch 30: loss=0.9695786833763123\n",
      "epoch 31: loss=0.9605741500854492\n",
      "epoch 32: loss=0.9590368270874023\n",
      "epoch 33: loss=0.9557931423187256\n",
      "epoch 34: loss=0.9527466297149658\n",
      "epoch 35: loss=0.9487982392311096\n",
      "epoch 36: loss=0.9456157684326172\n",
      "epoch 37: loss=0.9425531625747681\n",
      "epoch 38: loss=0.9408859610557556\n",
      "epoch 39: loss=0.9382991790771484\n",
      "epoch 40: loss=0.9366312026977539\n",
      "epoch 41: loss=0.9379563927650452\n",
      "epoch 42: loss=0.9380428791046143\n",
      "epoch 43: loss=0.936504065990448\n",
      "epoch 44: loss=0.9355148673057556\n",
      "epoch 45: loss=0.935332715511322\n",
      "epoch 46: loss=0.9349775314331055\n",
      "epoch 47: loss=0.9328996539115906\n",
      "epoch 48: loss=0.9333171844482422\n",
      "epoch 49: loss=0.9341648817062378\n",
      "epoch 0: loss=1.6548644304275513\n",
      "epoch 1: loss=1.5968431234359741\n",
      "epoch 2: loss=1.524625301361084\n",
      "epoch 3: loss=1.4610333442687988\n",
      "epoch 4: loss=1.393269419670105\n",
      "epoch 5: loss=1.3139760494232178\n",
      "epoch 6: loss=1.2381186485290527\n",
      "epoch 7: loss=1.1768327951431274\n",
      "epoch 8: loss=1.1485122442245483\n",
      "epoch 9: loss=1.1398309469223022\n",
      "epoch 10: loss=1.1389187574386597\n",
      "epoch 11: loss=1.1202324628829956\n",
      "epoch 12: loss=1.0936321020126343\n",
      "epoch 13: loss=1.0689468383789062\n",
      "epoch 14: loss=1.0401839017868042\n",
      "epoch 15: loss=1.0182336568832397\n",
      "epoch 16: loss=1.0044718980789185\n",
      "epoch 17: loss=0.9979313015937805\n",
      "epoch 18: loss=0.9936172962188721\n",
      "epoch 19: loss=0.9900131821632385\n",
      "epoch 20: loss=0.9843422174453735\n",
      "epoch 21: loss=0.9745508432388306\n",
      "epoch 22: loss=0.9650349617004395\n",
      "epoch 23: loss=0.9551615118980408\n",
      "epoch 24: loss=0.9471244215965271\n",
      "epoch 25: loss=0.9422504901885986\n",
      "epoch 26: loss=0.9387437105178833\n",
      "epoch 27: loss=0.937147319316864\n",
      "epoch 28: loss=0.9360238909721375\n",
      "epoch 29: loss=0.9359721541404724\n",
      "epoch 30: loss=0.9344549179077148\n",
      "epoch 31: loss=0.9327030181884766\n",
      "epoch 32: loss=0.9316463470458984\n",
      "epoch 33: loss=0.9312708377838135\n",
      "epoch 34: loss=0.9314543604850769\n",
      "epoch 35: loss=0.9320160150527954\n",
      "epoch 36: loss=0.9321709275245667\n",
      "epoch 37: loss=0.9323688745498657\n",
      "epoch 38: loss=0.9305539727210999\n",
      "epoch 39: loss=0.9291395545005798\n",
      "epoch 40: loss=0.9284632205963135\n",
      "epoch 41: loss=0.9276443123817444\n",
      "epoch 42: loss=0.9263041019439697\n",
      "epoch 43: loss=0.9268197417259216\n",
      "epoch 44: loss=0.9247745871543884\n",
      "epoch 45: loss=0.924447774887085\n",
      "epoch 46: loss=0.9231933951377869\n",
      "epoch 47: loss=0.922079861164093\n",
      "epoch 48: loss=0.9217156171798706\n",
      "epoch 49: loss=0.9208462834358215\n",
      "epoch 0: loss=1.7916216850280762\n",
      "epoch 1: loss=1.6877983808517456\n",
      "epoch 2: loss=1.6168568134307861\n",
      "epoch 3: loss=1.5518813133239746\n",
      "epoch 4: loss=1.492533802986145\n",
      "epoch 5: loss=1.4403046369552612\n",
      "epoch 6: loss=1.3715964555740356\n",
      "epoch 7: loss=1.3078070878982544\n",
      "epoch 8: loss=1.252654790878296\n",
      "epoch 9: loss=1.2029999494552612\n",
      "epoch 10: loss=1.1780834197998047\n",
      "epoch 11: loss=1.1645612716674805\n",
      "epoch 12: loss=1.149635672569275\n",
      "epoch 13: loss=1.136440396308899\n",
      "epoch 14: loss=1.1168934106826782\n",
      "epoch 15: loss=1.0964946746826172\n",
      "epoch 16: loss=1.0739299058914185\n",
      "epoch 17: loss=1.0518711805343628\n",
      "epoch 18: loss=1.0321996212005615\n",
      "epoch 19: loss=1.020370602607727\n",
      "epoch 20: loss=1.00893235206604\n",
      "epoch 21: loss=1.0046916007995605\n",
      "epoch 22: loss=0.9980139136314392\n",
      "epoch 23: loss=0.9918326735496521\n",
      "epoch 24: loss=0.986723780632019\n",
      "epoch 25: loss=0.9778749942779541\n",
      "epoch 26: loss=0.9722785353660583\n",
      "epoch 27: loss=0.9645333886146545\n",
      "epoch 28: loss=0.957383394241333\n",
      "epoch 29: loss=0.9538654088973999\n",
      "epoch 30: loss=0.9504381418228149\n",
      "epoch 31: loss=0.9481624960899353\n",
      "epoch 32: loss=0.9472266435623169\n",
      "epoch 33: loss=0.9456483721733093\n",
      "epoch 34: loss=0.9459142684936523\n",
      "epoch 35: loss=0.9435750246047974\n",
      "epoch 36: loss=0.9439347982406616\n",
      "epoch 37: loss=0.9423786997795105\n",
      "epoch 38: loss=0.9412476420402527\n",
      "epoch 39: loss=0.9410674571990967\n",
      "epoch 40: loss=0.9398637413978577\n",
      "epoch 41: loss=0.9379408359527588\n",
      "epoch 42: loss=0.9375795722007751\n",
      "epoch 43: loss=0.9349879622459412\n",
      "epoch 44: loss=0.9353442192077637\n",
      "epoch 45: loss=0.9342804551124573\n",
      "epoch 46: loss=0.9333391189575195\n",
      "epoch 47: loss=0.9323062300682068\n",
      "epoch 48: loss=0.9301229119300842\n",
      "epoch 49: loss=0.9289199113845825\n",
      "epoch 0: loss=1.7299456596374512\n",
      "epoch 1: loss=1.668076515197754\n",
      "epoch 2: loss=1.6004432439804077\n",
      "epoch 3: loss=1.5482367277145386\n",
      "epoch 4: loss=1.485836386680603\n",
      "epoch 5: loss=1.4209263324737549\n",
      "epoch 6: loss=1.353203296661377\n",
      "epoch 7: loss=1.2910648584365845\n",
      "epoch 8: loss=1.2314324378967285\n",
      "epoch 9: loss=1.1848251819610596\n",
      "epoch 10: loss=1.1663978099822998\n",
      "epoch 11: loss=1.1594593524932861\n",
      "epoch 12: loss=1.1476285457611084\n",
      "epoch 13: loss=1.130794644355774\n",
      "epoch 14: loss=1.112313985824585\n",
      "epoch 15: loss=1.091692566871643\n",
      "epoch 16: loss=1.0710725784301758\n",
      "epoch 17: loss=1.0473082065582275\n",
      "epoch 18: loss=1.0348172187805176\n",
      "epoch 19: loss=1.0232958793640137\n",
      "epoch 20: loss=1.0189849138259888\n",
      "epoch 21: loss=1.0169880390167236\n",
      "epoch 22: loss=1.0117335319519043\n",
      "epoch 23: loss=1.004338264465332\n",
      "epoch 24: loss=0.9963215589523315\n",
      "epoch 25: loss=0.984992265701294\n",
      "epoch 26: loss=0.9756176471710205\n",
      "epoch 27: loss=0.9717522859573364\n",
      "epoch 28: loss=0.9680371284484863\n",
      "epoch 29: loss=0.9662944078445435\n",
      "epoch 30: loss=0.9639245867729187\n",
      "epoch 31: loss=0.9624571800231934\n",
      "epoch 32: loss=0.9564340114593506\n",
      "epoch 33: loss=0.9525018930435181\n",
      "epoch 34: loss=0.9497120380401611\n",
      "epoch 35: loss=0.9447519779205322\n",
      "epoch 36: loss=0.9465498924255371\n",
      "epoch 37: loss=0.9435511827468872\n",
      "epoch 38: loss=0.943527340888977\n",
      "epoch 39: loss=0.9439762830734253\n",
      "epoch 40: loss=0.9421765804290771\n",
      "epoch 41: loss=0.9416946172714233\n",
      "epoch 42: loss=0.9388392567634583\n",
      "epoch 43: loss=0.9388511180877686\n",
      "epoch 44: loss=0.9382070899009705\n",
      "epoch 45: loss=0.9356977939605713\n",
      "epoch 46: loss=0.9374528527259827\n",
      "epoch 47: loss=0.9373183250427246\n",
      "epoch 48: loss=0.9348405003547668\n",
      "epoch 49: loss=0.9344436526298523\n",
      "epoch 0: loss=1.7576245069503784\n",
      "epoch 1: loss=1.7036110162734985\n",
      "epoch 2: loss=1.647181510925293\n",
      "epoch 3: loss=1.5959100723266602\n",
      "epoch 4: loss=1.555393099784851\n",
      "epoch 5: loss=1.5094143152236938\n",
      "epoch 6: loss=1.460423469543457\n",
      "epoch 7: loss=1.4005769491195679\n",
      "epoch 8: loss=1.3520221710205078\n",
      "epoch 9: loss=1.2882057428359985\n",
      "epoch 10: loss=1.2409363985061646\n",
      "epoch 11: loss=1.203814148902893\n",
      "epoch 12: loss=1.1797744035720825\n",
      "epoch 13: loss=1.1660476922988892\n",
      "epoch 14: loss=1.1614364385604858\n",
      "epoch 15: loss=1.1504194736480713\n",
      "epoch 16: loss=1.1417256593704224\n",
      "epoch 17: loss=1.124848484992981\n",
      "epoch 18: loss=1.1057438850402832\n",
      "epoch 19: loss=1.0825897455215454\n",
      "epoch 20: loss=1.0665152072906494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: loss=1.0544513463974\n",
      "epoch 22: loss=1.044603705406189\n",
      "epoch 23: loss=1.0399036407470703\n",
      "epoch 24: loss=1.0358997583389282\n",
      "epoch 25: loss=1.0316420793533325\n",
      "epoch 26: loss=1.0264285802841187\n",
      "epoch 27: loss=1.018023133277893\n",
      "epoch 28: loss=1.0108145475387573\n",
      "epoch 29: loss=1.0032525062561035\n",
      "epoch 30: loss=0.993445098400116\n",
      "epoch 31: loss=0.9878811836242676\n",
      "epoch 32: loss=0.982354998588562\n",
      "epoch 33: loss=0.9765857458114624\n",
      "epoch 34: loss=0.9728778004646301\n",
      "epoch 35: loss=0.9699106216430664\n",
      "epoch 36: loss=0.9671823382377625\n",
      "epoch 37: loss=0.9644292593002319\n",
      "epoch 38: loss=0.9588722586631775\n",
      "epoch 39: loss=0.9555097222328186\n",
      "epoch 40: loss=0.9529979228973389\n",
      "epoch 41: loss=0.9509998559951782\n",
      "epoch 42: loss=0.9504541754722595\n",
      "epoch 43: loss=0.9477017521858215\n",
      "epoch 44: loss=0.9480350017547607\n",
      "epoch 45: loss=0.9454212784767151\n",
      "epoch 46: loss=0.943938136100769\n",
      "epoch 47: loss=0.9437795877456665\n",
      "epoch 48: loss=0.9421513080596924\n",
      "epoch 49: loss=0.9422210454940796\n",
      "epoch 0: loss=1.7054152488708496\n",
      "epoch 1: loss=1.6228423118591309\n",
      "epoch 2: loss=1.5552427768707275\n",
      "epoch 3: loss=1.5016844272613525\n",
      "epoch 4: loss=1.4574803113937378\n",
      "epoch 5: loss=1.4105610847473145\n",
      "epoch 6: loss=1.3565573692321777\n",
      "epoch 7: loss=1.2994760274887085\n",
      "epoch 8: loss=1.2411268949508667\n",
      "epoch 9: loss=1.1912932395935059\n",
      "epoch 10: loss=1.157328486442566\n",
      "epoch 11: loss=1.1465882062911987\n",
      "epoch 12: loss=1.142325758934021\n",
      "epoch 13: loss=1.1368317604064941\n",
      "epoch 14: loss=1.1237305402755737\n",
      "epoch 15: loss=1.1051727533340454\n",
      "epoch 16: loss=1.080749273300171\n",
      "epoch 17: loss=1.0568076372146606\n",
      "epoch 18: loss=1.0364856719970703\n",
      "epoch 19: loss=1.024997591972351\n",
      "epoch 20: loss=1.016682505607605\n",
      "epoch 21: loss=1.0112814903259277\n",
      "epoch 22: loss=1.0082532167434692\n",
      "epoch 23: loss=1.0013176202774048\n",
      "epoch 24: loss=0.9938900470733643\n",
      "epoch 25: loss=0.9820542931556702\n",
      "epoch 26: loss=0.9716199636459351\n",
      "epoch 27: loss=0.9622523188591003\n",
      "epoch 28: loss=0.9554614424705505\n",
      "epoch 29: loss=0.9505233764648438\n",
      "epoch 30: loss=0.9463358521461487\n",
      "epoch 31: loss=0.9444513916969299\n",
      "epoch 32: loss=0.9418337345123291\n",
      "epoch 33: loss=0.9388073682785034\n",
      "epoch 34: loss=0.9372708201408386\n",
      "epoch 35: loss=0.9341949224472046\n",
      "epoch 36: loss=0.9316245317459106\n",
      "epoch 37: loss=0.9312448501586914\n",
      "epoch 38: loss=0.9314667582511902\n",
      "epoch 39: loss=0.9318302869796753\n",
      "epoch 40: loss=0.9324719905853271\n",
      "epoch 41: loss=0.9321181178092957\n",
      "epoch 42: loss=0.9315453171730042\n",
      "epoch 43: loss=0.9284612536430359\n",
      "epoch 44: loss=0.9272999167442322\n",
      "epoch 45: loss=0.9279664754867554\n",
      "epoch 46: loss=0.9282221794128418\n",
      "epoch 47: loss=0.9274405241012573\n",
      "epoch 48: loss=0.926201343536377\n",
      "epoch 49: loss=0.9254970550537109\n",
      "epoch 0: loss=1.7804512977600098\n",
      "epoch 1: loss=1.7072129249572754\n",
      "epoch 2: loss=1.6432946920394897\n",
      "epoch 3: loss=1.5818456411361694\n",
      "epoch 4: loss=1.5292863845825195\n",
      "epoch 5: loss=1.476623773574829\n",
      "epoch 6: loss=1.4191169738769531\n",
      "epoch 7: loss=1.3599720001220703\n",
      "epoch 8: loss=1.3033404350280762\n",
      "epoch 9: loss=1.2482678890228271\n",
      "epoch 10: loss=1.2099519968032837\n",
      "epoch 11: loss=1.176928162574768\n",
      "epoch 12: loss=1.1583784818649292\n",
      "epoch 13: loss=1.151550531387329\n",
      "epoch 14: loss=1.138039469718933\n",
      "epoch 15: loss=1.1297130584716797\n",
      "epoch 16: loss=1.1126322746276855\n",
      "epoch 17: loss=1.0934637784957886\n",
      "epoch 18: loss=1.0720431804656982\n",
      "epoch 19: loss=1.0531542301177979\n",
      "epoch 20: loss=1.0370839834213257\n",
      "epoch 21: loss=1.0232888460159302\n",
      "epoch 22: loss=1.0161871910095215\n",
      "epoch 23: loss=1.008110523223877\n",
      "epoch 24: loss=1.001847267150879\n",
      "epoch 25: loss=0.9966176748275757\n",
      "epoch 26: loss=0.9898403882980347\n",
      "epoch 27: loss=0.9808433651924133\n",
      "epoch 28: loss=0.9718617796897888\n",
      "epoch 29: loss=0.9627883434295654\n",
      "epoch 30: loss=0.9565951824188232\n",
      "epoch 31: loss=0.9513236284255981\n",
      "epoch 32: loss=0.9477167725563049\n",
      "epoch 33: loss=0.9465306997299194\n",
      "epoch 34: loss=0.9428068399429321\n",
      "epoch 35: loss=0.9401764869689941\n",
      "epoch 36: loss=0.9379542469978333\n",
      "epoch 37: loss=0.9353189468383789\n",
      "epoch 38: loss=0.9339231252670288\n",
      "epoch 39: loss=0.9338280558586121\n",
      "epoch 40: loss=0.9344820976257324\n",
      "epoch 41: loss=0.9327903985977173\n",
      "epoch 42: loss=0.9337834715843201\n",
      "epoch 43: loss=0.9320606589317322\n",
      "epoch 44: loss=0.9315841197967529\n",
      "epoch 45: loss=0.9283316135406494\n",
      "epoch 46: loss=0.9266194105148315\n",
      "epoch 47: loss=0.9255259037017822\n",
      "epoch 48: loss=0.9261946082115173\n",
      "epoch 49: loss=0.9241661429405212\n",
      "epoch 0: loss=1.757849931716919\n",
      "epoch 1: loss=1.6711177825927734\n",
      "epoch 2: loss=1.6101765632629395\n",
      "epoch 3: loss=1.5627280473709106\n",
      "epoch 4: loss=1.4985158443450928\n",
      "epoch 5: loss=1.4479695558547974\n",
      "epoch 6: loss=1.3831233978271484\n",
      "epoch 7: loss=1.3130419254302979\n",
      "epoch 8: loss=1.2553257942199707\n",
      "epoch 9: loss=1.2086580991744995\n",
      "epoch 10: loss=1.1868032217025757\n",
      "epoch 11: loss=1.1625477075576782\n",
      "epoch 12: loss=1.1577504873275757\n",
      "epoch 13: loss=1.1478447914123535\n",
      "epoch 14: loss=1.1387488842010498\n",
      "epoch 15: loss=1.1178576946258545\n",
      "epoch 16: loss=1.0988637208938599\n",
      "epoch 17: loss=1.0765472650527954\n",
      "epoch 18: loss=1.0589343309402466\n",
      "epoch 19: loss=1.0438001155853271\n",
      "epoch 20: loss=1.0338889360427856\n",
      "epoch 21: loss=1.0293617248535156\n",
      "epoch 22: loss=1.0249041318893433\n",
      "epoch 23: loss=1.0219285488128662\n",
      "epoch 24: loss=1.014918327331543\n",
      "epoch 25: loss=1.0083099603652954\n",
      "epoch 26: loss=0.9994893074035645\n",
      "epoch 27: loss=0.990120530128479\n",
      "epoch 28: loss=0.9806528687477112\n",
      "epoch 29: loss=0.9743574261665344\n",
      "epoch 30: loss=0.9682660102844238\n",
      "epoch 31: loss=0.9652867317199707\n",
      "epoch 32: loss=0.9606283903121948\n",
      "epoch 33: loss=0.9578607082366943\n",
      "epoch 34: loss=0.9549318552017212\n",
      "epoch 35: loss=0.9498418569564819\n",
      "epoch 36: loss=0.9467790126800537\n",
      "epoch 37: loss=0.9434606432914734\n",
      "epoch 38: loss=0.9415627717971802\n",
      "epoch 39: loss=0.940422534942627\n",
      "epoch 40: loss=0.9404792189598083\n",
      "epoch 41: loss=0.9406411051750183\n",
      "epoch 42: loss=0.9398781657218933\n",
      "epoch 43: loss=0.9382973313331604\n",
      "epoch 44: loss=0.9374861717224121\n",
      "epoch 45: loss=0.9378085136413574\n",
      "epoch 46: loss=0.9374322891235352\n",
      "epoch 47: loss=0.9370179176330566\n",
      "epoch 48: loss=0.9370982646942139\n",
      "epoch 49: loss=0.9365387558937073\n",
      "epoch 0: loss=1.776713490486145\n",
      "epoch 1: loss=1.6977379322052002\n",
      "epoch 2: loss=1.621383547782898\n",
      "epoch 3: loss=1.566169023513794\n",
      "epoch 4: loss=1.499555230140686\n",
      "epoch 5: loss=1.4463486671447754\n",
      "epoch 6: loss=1.3827927112579346\n",
      "epoch 7: loss=1.3208664655685425\n",
      "epoch 8: loss=1.2543538808822632\n",
      "epoch 9: loss=1.2144923210144043\n",
      "epoch 10: loss=1.189243197441101\n",
      "epoch 11: loss=1.1785110235214233\n",
      "epoch 12: loss=1.1655255556106567\n",
      "epoch 13: loss=1.1506142616271973\n",
      "epoch 14: loss=1.1263092756271362\n",
      "epoch 15: loss=1.0985543727874756\n",
      "epoch 16: loss=1.0791351795196533\n",
      "epoch 17: loss=1.0545650720596313\n",
      "epoch 18: loss=1.0392687320709229\n",
      "epoch 19: loss=1.0267287492752075\n",
      "epoch 20: loss=1.0220067501068115\n",
      "epoch 21: loss=1.0136743783950806\n",
      "epoch 22: loss=1.0090545415878296\n",
      "epoch 23: loss=1.0011330842971802\n",
      "epoch 24: loss=0.9892820715904236\n",
      "epoch 25: loss=0.9782293438911438\n",
      "epoch 26: loss=0.9694294333457947\n",
      "epoch 27: loss=0.9635552763938904\n",
      "epoch 28: loss=0.9576032757759094\n",
      "epoch 29: loss=0.9532011151313782\n",
      "epoch 30: loss=0.9489424228668213\n",
      "epoch 31: loss=0.9497913718223572\n",
      "epoch 32: loss=0.9474871754646301\n",
      "epoch 33: loss=0.9438567757606506\n",
      "epoch 34: loss=0.9432262182235718\n",
      "epoch 35: loss=0.9419606924057007\n",
      "epoch 36: loss=0.9425252676010132\n",
      "epoch 37: loss=0.9429445266723633\n",
      "epoch 38: loss=0.9425439238548279\n",
      "epoch 39: loss=0.9405368566513062\n",
      "epoch 40: loss=0.9404412508010864\n",
      "epoch 41: loss=0.9380477666854858\n",
      "epoch 42: loss=0.9368128180503845\n",
      "epoch 43: loss=0.9360635280609131\n",
      "epoch 44: loss=0.9348399639129639\n",
      "epoch 45: loss=0.9337681531906128\n",
      "epoch 46: loss=0.9319368004798889\n",
      "epoch 47: loss=0.9308567643165588\n",
      "epoch 48: loss=0.9297378063201904\n",
      "epoch 49: loss=0.9292567372322083\n",
      "epoch 0: loss=1.7457454204559326\n",
      "epoch 1: loss=1.6683989763259888\n",
      "epoch 2: loss=1.6172754764556885\n",
      "epoch 3: loss=1.5710499286651611\n",
      "epoch 4: loss=1.516562581062317\n",
      "epoch 5: loss=1.4682731628417969\n",
      "epoch 6: loss=1.4203472137451172\n",
      "epoch 7: loss=1.3623398542404175\n",
      "epoch 8: loss=1.3207701444625854\n",
      "epoch 9: loss=1.2630146741867065\n",
      "epoch 10: loss=1.2151384353637695\n",
      "epoch 11: loss=1.18326997756958\n",
      "epoch 12: loss=1.1601077318191528\n",
      "epoch 13: loss=1.1447701454162598\n",
      "epoch 14: loss=1.1285749673843384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: loss=1.115351915359497\n",
      "epoch 16: loss=1.090338945388794\n",
      "epoch 17: loss=1.0656782388687134\n",
      "epoch 18: loss=1.0440372228622437\n",
      "epoch 19: loss=1.02473783493042\n",
      "epoch 20: loss=1.0109702348709106\n",
      "epoch 21: loss=1.0026800632476807\n",
      "epoch 22: loss=0.9942557215690613\n",
      "epoch 23: loss=0.9874855279922485\n",
      "epoch 24: loss=0.978994607925415\n",
      "epoch 25: loss=0.9670261144638062\n",
      "epoch 26: loss=0.9563136100769043\n",
      "epoch 27: loss=0.9516518712043762\n",
      "epoch 28: loss=0.9465218186378479\n",
      "epoch 29: loss=0.9449506402015686\n",
      "epoch 30: loss=0.9432404041290283\n",
      "epoch 31: loss=0.9444301128387451\n",
      "epoch 32: loss=0.9420732855796814\n",
      "epoch 33: loss=0.9422962069511414\n",
      "epoch 34: loss=0.9402437210083008\n",
      "epoch 35: loss=0.9382897615432739\n",
      "epoch 36: loss=0.9386165738105774\n",
      "epoch 37: loss=0.9389564990997314\n",
      "epoch 38: loss=0.937715470790863\n",
      "epoch 39: loss=0.9367771148681641\n",
      "epoch 40: loss=0.9353109002113342\n",
      "epoch 41: loss=0.9336538314819336\n",
      "epoch 42: loss=0.9314667582511902\n",
      "epoch 43: loss=0.9294714331626892\n",
      "epoch 44: loss=0.9295159578323364\n",
      "epoch 45: loss=0.9282522797584534\n",
      "epoch 46: loss=0.926597535610199\n",
      "epoch 47: loss=0.9262187480926514\n",
      "epoch 48: loss=0.9254357218742371\n",
      "epoch 49: loss=0.9229568243026733\n"
     ]
    }
   ],
   "source": [
    "patches_embedding, VGAE_model=VGAE_patch_embeddings(patches, dim=dim, hidden_dim=32, num_epochs=50, decoder=None, device=device, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7ef4871-a108-4071-8411-0ccbcfac5731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f491608eee44d7949a304249921062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m patch \u001b[38;5;241m=\u001b[39m tg_graphs[d]\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;241m=\u001b[39m VGAE(dim\u001b[38;5;241m=\u001b[39mdim, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_features\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mnum_node_features)\n\u001b[0;32m----> 8\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVGAE_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m patch_emb\u001b[38;5;241m.\u001b[39mappend(Patch(patch\u001b[38;5;241m.\u001b[39mnodes, model\u001b[38;5;241m.\u001b[39mencode(patch)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())) \u001b[38;5;66;03m#ADDED\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#coordinates = model.encode(patch).detach().numpy()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [25], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, model, loss_fun, num_epochs, verbose, lr, logger)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/pytorch_env/Local2Global_private-master/doc/git/l2glite/l2gl/embedding/gae/utils/loss.py:36\u001b[0m, in \u001b[0;36mVGAE_loss\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mVGAE_loss\u001b[39m(model, data):\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    loss function for use with :func:`VGAE_model`\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m        loss value\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 36\u001b[0m         model\u001b[38;5;241m.\u001b[39mrecon_loss(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mkl_loss() \u001b[38;5;241m/\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/models/autoencoder.py:169\u001b[0m, in \u001b[0;36mVGAE.encode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mMAX_LOGSTD)\n\u001b[1;32m    171\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/pytorch_env/Local2Global_private-master/doc/git/l2glite/l2gl/embedding/gae/layers/VGAEconv.py:87\u001b[0m, in \u001b[0;36mVGAEconv.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     85\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     90\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_conv2(x, edge_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "#patch_list = []\n",
    "models = []\n",
    "embeddings = []\n",
    "patch_emb=[] #ADDED\n",
    "for d in dates[:n_patches]:\n",
    "    patch = tg_graphs[d]\n",
    "    model = model = VGAE(dim=dim, hidden_dim=128, num_features=data.num_node_features)\n",
    "    model, _ = train(patch, model, VGAE_loss, num_epochs=5, lr=0.01)\n",
    "    patch_emb.append(Patch(patch.nodes, model.encode(patch).detach().numpy())) #ADDED\n",
    "    #coordinates = model.encode(patch).detach().numpy()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde5e3-f417-49d8-b884-62d2a8d6f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization on manifolds approach\n",
    "intersection_nodes=mopt.double_intersections_nodes(patch_emb)\n",
    "dim=64\n",
    "res, emb = mopt.optimization(patch_emb, intersection_nodes, dim) #res contain the result of the optimization, i.e., scales, rotations and traslations,\n",
    "                                                                # emb is the embedding of every nodes using the scales, rotations and translations \n",
    "                                                                #found with  the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4bde3-651c-4dd5-9a8f-1dcc92e20ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard l2g approach\n",
    "pr = AlignmentProblem(patch_emb)\n",
    "old_emb = pr.get_aligned_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d670d22-c6cb-42b0-9f02-b188f29e5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(data, date, most_common):\n",
    "    countries = dl.get_nodes(ts=date)['country'].to_list()\n",
    "    indices = [i for i in range(len(countries)) if countries[i] in most_common]\n",
    "    points = data[indices, :]\n",
    "    labels = [most_common.index(countries[i]) for i in indices]\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee98c96-b390-43d3-88ae-1a1b5f0bc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(umap_embedding, labels, p):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "    ax.scatter(\n",
    "        umap_embedding[p][:, 0],\n",
    "        umap_embedding[p][:, 1],\n",
    "        c=[sns.color_palette()[x] for x in labels[p]],\n",
    "        lw=1\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97461f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_graphs[dates[1]].nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all embeddings (using GAE or VGAE)\n",
    "dim = 64\n",
    "#patch_list = []\n",
    "models = []\n",
    "embeddings = []\n",
    "patch_emb = [] #ADDED\n",
    "for d in dates[:5]:\n",
    "    patch = tg_graphs[d]\n",
    "    model = GAE(dim=64, hidden_dim=128, num_features=data.num_node_features)\n",
    "    model, _ = train(patch, model, GAE_loss, num_epochs=40, lr=0.01)\n",
    "    patch_emb.append(Patch(patch.nodes, model.encode(patch).detach().numpy())) #ADDED\n",
    "    #coordinates = model.encode(patch).detach().numpy()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c235a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=10, min_dist=0.01, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8046a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, p in enumerate(patch_emb):\n",
    "    p_countries = dl.get_nodes(ts=dates[j])['country'].to_list()\n",
    "    p_indices = [i for i in range(len(p_countries)) if p_countries[i] in most_common]\n",
    "    p_labels = [most_common.index(p_countries[i]) for i in p_indices]\n",
    "    p_points = p.coordinates[p_indices, :]\n",
    "    umap_embedding = reducer.fit_transform(p_points)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "    ax.scatter(\n",
    "        umap_embedding[:, 0],\n",
    "        umap_embedding[:, 1],\n",
    "        c=[sns.color_palette()[x] for x in p_labels],\n",
    "        lw=1\n",
    "    )\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('UMAP Embedding', fontsize=12)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad870c54",
   "metadata": {},
   "source": [
    "## NEW VERSION OF ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3fb79cc-708a-4766-abe4-c85ce5a33103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_intersections_nodes(patches):\n",
    "    double_intersections = dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list = []\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list = list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "\n",
    "def get_embedding(patches, trained_model):\n",
    "    scales = [ s.detach().numpy().item() for s in trained_model.s] \n",
    "    rots = [r.detach().numpy() for r in trained_model.R]\n",
    "    shifts = [ s.detach().numpy() for s in trained_model.t] \n",
    "\n",
    "    emb_problem = AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rots[i] + shifts[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import geotorch\n",
    "\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches, num_epochs=100, learning_rate=0.05):\n",
    "    #device = get_device()\n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    \n",
    "    model = Model(dim, n_patches, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model, loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dd539eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<local2global.patch.Patch at 0x7f3d9bac8df0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c67280>,\n",
       " <local2global.patch.Patch at 0x7f3d54c67970>,\n",
       " <local2global.patch.Patch at 0x7f3d550d77f0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c653c0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c66fb0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c677c0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c668c0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c645e0>,\n",
       " <local2global.patch.Patch at 0x7f3d5514b7f0>,\n",
       " <local2global.patch.Patch at 0x7f3cb4275f30>,\n",
       " <local2global.patch.Patch at 0x7f3d54c658a0>,\n",
       " <local2global.patch.Patch at 0x7f3d5514a7a0>,\n",
       " <local2global.patch.Patch at 0x7f3cb4274070>,\n",
       " <local2global.patch.Patch at 0x7f3d54c66350>,\n",
       " <local2global.patch.Patch at 0x7f3d55149750>,\n",
       " <local2global.patch.Patch at 0x7f3cb42764d0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c64f70>,\n",
       " <local2global.patch.Patch at 0x7f3d5514a410>,\n",
       " <local2global.patch.Patch at 0x7f3cb42744f0>,\n",
       " <local2global.patch.Patch at 0x7f3d54c66c80>,\n",
       " <local2global.patch.Patch at 0x7f3d5514ada0>,\n",
       " <local2global.patch.Patch at 0x7f3cb4277250>,\n",
       " <local2global.patch.Patch at 0x7f3d54c64160>,\n",
       " <local2global.patch.Patch at 0x7f3d5514afe0>,\n",
       " <local2global.patch.Patch at 0x7f3cb4275960>,\n",
       " <local2global.patch.Patch at 0x7f3d54c666b0>,\n",
       " <local2global.patch.Patch at 0x7f3c84bff910>,\n",
       " <local2global.patch.Patch at 0x7f3c84bfc4c0>,\n",
       " <local2global.patch.Patch at 0x7f3c84bfd660>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_emb=patches_embedding\n",
    "patch_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "775738d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#patch_emb\n",
    "\n",
    "n_patches = len(patch_emb)\n",
    "nodes = double_intersections_nodes(patch_emb)\n",
    "emb_patches = preprocess_graphs(patch_emb, nodes)\n",
    "\n",
    "#best_model, best_params, results = grid_search(emb_patches, dim, n_patches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9b4ddbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79dc0af8d7b4938a5c7ce467cc5e7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3922214.5\n",
      "Epoch 10, Loss: 784976.25\n",
      "Epoch 20, Loss: 421837.9375\n",
      "Epoch 30, Loss: 297222.09375\n",
      "Epoch 40, Loss: 236388.921875\n",
      "Epoch 50, Loss: 220602.484375\n",
      "Epoch 60, Loss: 214030.0\n",
      "Epoch 70, Loss: 211398.796875\n",
      "Epoch 80, Loss: 210384.828125\n",
      "Epoch 90, Loss: 209840.921875\n",
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "res, loss_hist= train_model(emb_patches, dim, n_patches , num_epochs=100, learning_rate=0.5)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22065042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3afe8c3850>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d93ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    #prob=l2g.AlignmentProblem(patches)\n",
    "    #old_embedding=prob.get_aligned_embedding()\n",
    "    #embedding=embedding[nodes]\n",
    "    #old_embedding=old_embedding[nodes]\n",
    "    #error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78eba6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=get_embedding(patch_emb, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd48ced9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5bc8db7b0c46b8a0eb84537a9f6fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute relative transformations:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17151a6f4d0843459fd4f7994a331a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute mean embedding:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "prob=l2g.AlignmentProblem(patches_embedding)\n",
    "old_emb=prob.get_aligned_embedding()\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07f733e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procrustes error: 0.3325606459897007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Procrustes error: {l2g.utils.procrustes_error(old_emb, emb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "971f4adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_model_ip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m full_model\u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mVGAE(encoder\u001b[38;5;241m=\u001b[39mVGAEconv(dim, dim))\n\u001b[0;32m----> 2\u001b[0m auc, ap \u001b[38;5;241m=\u001b[39m \u001b[43mfull_model_ip\u001b[49m\u001b[38;5;241m.\u001b[39mtest(torch\u001b[38;5;241m.\u001b[39mtensor(emb), test_data\u001b[38;5;241m.\u001b[39medge_index, neg_edges)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_model_ip' is not defined"
     ]
    }
   ],
   "source": [
    "full_model= tg.nn.VGAE(encoder=VGAEconv(dim, dim))\n",
    "auc, ap = full_model_ip.test(torch.tensor(emb), test_data.edge_index, neg_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eebca4",
   "metadata": {},
   "source": [
    "Best Params: {'learning_rate': 0.05, 'num_epochs': 500, 'weight_decay': 0, 'scaling_factor': 1} with Loss: 65.79495239257812\n",
    "no scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cc36d",
   "metadata": {},
   "source": [
    "Best Params: {'learning_rate': 0.05, 'num_epochs': 500, 'weight_decay': 0, 'scaling_factor': 1} with Loss: 60.9526252746582  no scales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d4a2a",
   "metadata": {},
   "source": [
    "Best Params: {'learning_rate': 0.05, 'num_epochs': 500, 'weight_decay': 0.0001, 'scaling_factor': 1} with Loss: 81.65168762207031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cdf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, patch_emb, dim, n_patches):\n",
    "        super(AffineTransform, self).__init__()\n",
    "        \n",
    "        self.R = nn.ParameterList([nn.Parameter(torch.randn(dim, dim)) for _ in range(n_patches)])\n",
    "        self.s=nn.ParameterList([nn.Parameter(torch.randn(1)) for _ in range(n_patches)]) #nn.ParameterList([1] + [nn.Parameter(torch.randn(1)) for _ in range(n_patches-1)])\n",
    "        self.t = nn.ParameterList([nn.Parameter(torch.randn(dim)) for _ in range(n_patches)])\n",
    "        \n",
    "        # Parameters for Y transformation: W2 (d x d), b2 (d)\n",
    "        #self.R2 = nn.Parameter(torch.randn(d, d))\n",
    "        #self.s2=nn.Parameter(torch.randn(1))\n",
    "        #self.t2 = nn.Parameter(torch.randn(d))\n",
    "\n",
    "    def forward(self, patch_emb, n_patches, scales=True):\n",
    "        \n",
    "        m=2*n_patches -2\n",
    "        R = [self.R[0]] + [item for i in range(1, n_patches-1) for item in (self.R[i], self.R[i])] +[self.R[-1]]\n",
    "        \n",
    "        t= [self.t[0]] + [item for i in range(1, n_patches-1) for item in (self.t[i], self.t[i])] + [self.t[-1]]\n",
    "\n",
    "        s= [self.s[0]] + [item for i in range(1, n_patches-1) for item in (self.s[i], self.s[i])] +[self.s[-1]]\n",
    "        \n",
    "        transformed_emb= [s[i]*patch_emb[i] @ R[i] + t[i] for i in range(m)]\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        return transformed_emb \n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss_function(transformed_emb, rots):\n",
    "    d = rots[0].shape[0]\n",
    "    I = torch.eye(d)\n",
    "    m=len(transformed_emb)\n",
    "    \n",
    "    diff=[transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    \n",
    "    l=sum([torch.norm(d)**2 for d in diff])\n",
    "\n",
    "    ort_r=sum([torch.linalg.matrix_norm(R@R.T - I) for R in rots])\n",
    "    loss=l+10000*ort_r\n",
    "   \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches , num_epochs=1000, learning_rate=0.01):\n",
    "    loss_hist=[]\n",
    "    \n",
    "    \n",
    "    model = AffineTransform(patch_emb, dim, n_patches)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "     \n",
    "        transformed_patch_emb = model(patch_emb, n_patches)\n",
    "        \n",
    "      \n",
    "        loss = loss_function(transformed_patch_emb, model.R)\n",
    "        \n",
    "     \n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "    return model, loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, loss_hist2=train_model(emb_patches, dim, n_patches , num_epochs=1000, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf47f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b96a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb22=get_embedding(patch_emb, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcac778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Procrustes error: {l2g.utils.procrustes_error(old_emb, emb22)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_intersections_nodes(patches):\n",
    "    double_intersections = dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list = []\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list = list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "\n",
    "nodes=double_intersections_nodes(patch_emb)\n",
    "emb_patch=preprocess_graphs(patch_emb, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import geotorch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, patch_emb, dim, n_patches):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "\n",
    "        # Works with tensors: Instantiate a CNN with kernels of rank 1\n",
    "        #self.cnn = nn.Conv2d(16, 32, 3)\n",
    "        #geotorch.low_rank(self.cnn, \"weight\", rank=1)\n",
    "\n",
    "        # Weights are initialized to a random value when you put the constraints, but\n",
    "        # you may re-initialize them to a different value by assigning to them\n",
    "        #self.linear.weight = torch.eye(128, 64)\n",
    "        # And that's all you need to do. The rest is regular PyTorch code\n",
    "\n",
    "    def forward(self, patch_emb):\n",
    "        # self.linear is orthogonal and every 3x3 kernel in self.cnn is of rank 1\n",
    "        \n",
    "        m=len(patch_emb)\n",
    "        transformations= [self.transformation[0]] + [item for i in range(1, n_patches-1) for item in (self.transformation[i], self.transformation[i])] +[self.transformation[-1]]\n",
    "        transformed_emb=[transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "\n",
    "def loss_function(transformed_emb):\n",
    "    #d = rots[0].shape[0]\n",
    "    #I = torch.eye(d)\n",
    "    m=len(transformed_emb)\n",
    "    \n",
    "    diff=[transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    \n",
    "    loss=sum([torch.norm(d)**2 for d in diff])\n",
    "\n",
    "    #ort_r=sum([torch.linalg.matrix_norm(R@R.T - I) for R in rots])\n",
    "    #loss=l+10*ort_r\n",
    "   \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches , num_epochs=1000, learning_rate=0.01):\n",
    "    loss_hist=[]\n",
    "    \n",
    "    \n",
    "    model =Model(patch_emb, dim, n_patches)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "     \n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        \n",
    "      \n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        \n",
    "     \n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "    return model, loss_hist\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.05, 0.5], \n",
    "    \"num_epochs\": [500, 1000, 5000], \n",
    "    \"weight_decay\": [0, 1e-4, 1e-3], # regularization for Adam\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def grid_search(patch_emb, dim, n_patches):\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for lr, num_epochs, weight_decay in itertools.product(\n",
    "        param_grid[\"learning_rate\"], \n",
    "        param_grid[\"num_epochs\"], \n",
    "        param_grid[\"weight_decay\"], \n",
    "        #param_grid[\"scaling_factor\"],\n",
    "        #param_grid['scales']\n",
    "    ):\n",
    "        print(f\"Training with lr={lr}, epochs={num_epochs}, wd={weight_decay}\")  #scale={scaling_factor}\n",
    "\n",
    "        \n",
    "        def loss_function(transformed_emb):\n",
    "    #d = rots[0].shape[0]\n",
    "    #I = torch.eye(d)\n",
    "            m=len(transformed_emb)\n",
    "    \n",
    "            diff=[transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    \n",
    "            loss=sum([torch.norm(d)**2 for d in diff])\n",
    "\n",
    "    #ort_r=sum([torch.linalg.matrix_norm(R@R.T - I) for R in rots])\n",
    "    #loss=l+10*ort_r\n",
    "   \n",
    "        \n",
    "            return loss\n",
    "        \n",
    "        # Train model\n",
    "        model = Model(patch_emb, dim, n_patches)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_hist = []\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            optimizer.zero_grad()\n",
    "            transformed_patch_emb = model(patch_emb)\n",
    "            loss = loss_function(transformed_patch_emb)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "        final_loss = loss_hist[-1]  # Use last recorded loss\n",
    "        results.append((final_loss, lr, num_epochs, weight_decay))\n",
    "\n",
    "        # Track best model\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_model = model\n",
    "            best_params = {\"learning_rate\": lr, \"num_epochs\": num_epochs, \n",
    "                           \"weight_decay\": weight_decay} #, \"scaling_factor\": scaling_factor}\n",
    "    \n",
    "    print(f\"\\nBest Params: {best_params} with Loss: {best_loss}\")\n",
    "    return best_model, best_params, results\n",
    "\n",
    "last_best_model, last_best_params, last_results = grid_search(emb_patch, dim, n_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55552137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    prob=l2g.AlignmentProblem(patches)\n",
    "    old_embedding=prob.get_aligned_embedding()\n",
    "    error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding, old_embedding, error, rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_best_emb, old_emb, error, rotations=get_error(patch_emb, last_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae806956",
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb21558",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=l2g.AlignmentProblem(patch_emb)\n",
    "old=prob.get_aligned_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aligned_patches=preprocess_graphs(prob.patches, nodes)\n",
    "m=len(aligned_patches)\n",
    "\n",
    "l=[np.linalg.norm(aligned_patches[i]- aligned_patches[i+1])**2 for i in range(0, m-1, 2)]\n",
    "l_not_aligned=[np.linalg.norm(emb_patch[i]- emb_patch[i+1])**2 for i in range(0, m-1, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l_not_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01acb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_problem = l2g.AlignmentProblem(patches)\n",
    "embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "    embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@RRR[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc415b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Procrustes error: {l2g.utils.procrustes_error(old_emb, embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa624fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(old_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30414921",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=prob.calc_synchronised_rotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2=[]\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        dist2.append(np.linalg.norm(R[i]-R[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f940a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_er=[np.linalg.norm(rots[i]- R[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eea142",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=patch_emb[0].coordinates\n",
    "P_rots=P@rots[0]\n",
    "P_R= P@R[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a883071",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_P_rots=reducer.fit_transform(P_rots)\n",
    "umap_P_R=reducer.fit_transform(P_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_P_rots[:,0], umap_P_rots[:,1], label='rots')\n",
    "plt.scatter(umap_P_R[:,0], umap_P_R[:,1], label= 'R')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, patch_emb, dim, n_patches, scales=True):\n",
    "        super(AffineTransform, self).__init__()\n",
    "        self.R = nn.ParameterList([nn.Parameter(torch.randn(dim, dim)) for _ in range(n_patches)])\n",
    "        \n",
    "        self.t = nn.ParameterList([nn.Parameter(torch.randn(dim)) for _ in range(n_patches)])\n",
    "\n",
    "        if scales:\n",
    "            self.s=nn.ParameterList([nn.Parameter(torch.randn(1)) for _ in range(n_patches)]) #nn.ParameterList([1] + [nn.Parameter(torch.randn(1)) for _ in range(n_patches-1)])\n",
    "    \n",
    "        # Parameters for Y transformation: W2 (d x d), b2 (d)\n",
    "        #self.R2 = nn.Parameter(torch.randn(d, d))\n",
    "        #self.s2=nn.Parameter(torch.randn(1))\n",
    "        #self.t2 = nn.Parameter(torch.randn(d))\n",
    "\n",
    "    def forward(self, patch_emb, n_patches, scales=True):\n",
    "        \n",
    "        m=2*n_patches -2\n",
    "        R = [self.R[0]] + [item for i in range(1, n_patches) for item in (self.R[i], self.R[i])]\n",
    "        \n",
    "        t= [self.t[0]] + [item for i in range(1, n_patches) for item in (self.t[i], self.t[i])]\n",
    "\n",
    "        if scales:\n",
    "            s= [self.s[0]] + [item for i in range(1, n_patches) for item in (self.s[i], self.s[i])]\n",
    "            \n",
    "            transformed_emb= [s[i]*patch_emb[i] @ R[i] + t[i] for i in range(m)]\n",
    "        else:\n",
    "             transformed_emb= [patch_emb[i] @ R[i] + t[i] for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss_function(transformed_emb, rots):\n",
    "    d = rots[0].shape[0]\n",
    "    I = torch.eye(d)\n",
    "    m=len(transformed_emb)\n",
    "    \n",
    "    diff=[transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    \n",
    "    l=sum([torch.norm(d)**2 for d in diff])\n",
    "\n",
    "    ort_r=sum([torch.linalg.matrix_norm(R@R.T - I) for R in rots])\n",
    "    loss=l+100*ort_r\n",
    "   \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches , num_epochs=1000, learning_rate=0.01):\n",
    "    loss_hist=[]\n",
    "    \n",
    "    \n",
    "    model = AffineTransform(patch_emb, dim, n_patches)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "     \n",
    "        transformed_patch_emb = model(patch_emb, n_patches)\n",
    "        \n",
    "      \n",
    "        loss = loss_function(transformed_patch_emb, model.R)\n",
    "        \n",
    "     \n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "\n",
    "    return model, loss_hist\n",
    "\n",
    "def get_embedding(patches, trained_model):\n",
    "    scales=[ s.detach().numpy().item() for s in trained_model.s]   #[1]+ [ s.detach().numpy().item() for s in trained_model.s[1:]] \n",
    "    rots=[r.detach().numpy() for r in trained_model.R]\n",
    "    shifts=[ s.detach().numpy() for s in trained_model.t] \n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([scales[i]*emb_problem.patches[p].get_coordinate(node)@rots[i] + shifts[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, loss_hist2=train_model(emb_patches, dim, n_patches , num_epochs=5000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a80581",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2=get_embedding(patch_emb, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Procrustes error: {l2g.utils.procrustes_error(old_emb, emb2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354beef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1defd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb5f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bf663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c313df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84864771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea544be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957b6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a9468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e36d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_at_node_patch_p(p,node, rots,shifts, scales, patches):\n",
    "    i=patches.index(p)\n",
    "    q=patches[i+1]    \n",
    "    #rots=[torch.tensor(r) for r in rots]\n",
    "    #shifts=[torch.tensor(s) for s in shifts]\n",
    "    #scales=[torch.tensor(s) for s in scales]\n",
    "    \n",
    "    return torch.norm( scales[i]*p.get_coordinate(node)@rots[i] + shifts[i] -(scales[i+1]*q.get_coordinate(node)@rots[i+1] +shifts[i+1]))**2\n",
    "        \n",
    "def grad_loss_at_node_patch_p(p, node, rots,shifts, scales, patches):\n",
    "    i=patches.index(p)\n",
    "    W=rots[i].clone().detach().requires_grad_(True)\n",
    "    rots[i]=W\n",
    "    g=loss_at_node_patch_p(p, node, rots,shifts, scales, patches).clone().detach().requires_grad_(True)\n",
    "    W.retain_grad()\n",
    "    g.backward()\n",
    "    return W.grad\n",
    "        \n",
    "\n",
    "def min_loss(Rotations, scales, translations , patches, nodes):\n",
    "    l=0\n",
    "    fij=dict()\n",
    "    for i, p in enumerate(patches[:len(patches)-1]):\n",
    "        q=patches[i+1]\n",
    "        for n in nodes[i,i+1]:\n",
    "            theta1=scales[i]*p.get_coordinate(n)@Rotations[i]+ translations[i]\n",
    "            theta2= scales[i+1]*q.get_coordinate(n)@Rotations[i+1]+translations[i+1]\n",
    "            l+=np.linalg.norm(theta1-theta2)**2\n",
    "            #fij[(i, j+1+i, n)]=[theta1, theta2]\n",
    "    return l#, fij\n",
    "\n",
    "def numbering_tangent_coordinates(dim):\n",
    "    return {i: (j, l) for i, (j, l) in enumerate((j, l) for j in range(dim) for l in range(j + 1, dim))}\n",
    "\n",
    "def get_coordinates_from_index(i, dim):\n",
    "    return numbering_tangent_coordinates(dim)[i]\n",
    "\n",
    "\n",
    "def select_coordinate(X):\n",
    "    return torch.argmax(torch.norm(X, dim=0)).item()\n",
    "\n",
    "def basis(j, l, dim):\n",
    "    ej=torch.zeros((dim,1))\n",
    "    ej[j]=1\n",
    "    el=torch.zeros((dim,1))\n",
    "    el[l]=1\n",
    "    return 1/np.sqrt(2)* (ej@el.t() - el@ej.t()).numpy() #.type(torch.float64)\n",
    "    \n",
    "\n",
    "\n",
    "#def grad_s_X_R_norm(scale, emb, rot):\n",
    "    #rot=torch.tensor(rot, requires_grad=True)\n",
    "    #pr=torch.norm(scale*emb@rot)**2\n",
    "    #pr.backward()\n",
    "    #return rot.grad\n",
    "def grad_s_X_R_norm(scale, emb, rot):\n",
    "    return 2*scale**2*emb*rot\n",
    "\n",
    "\n",
    "def grad_s_X_R_scalar_y(scale, emb ,rot, y):\n",
    "    #rot=torch.tensor(rot, requires_grad=True)\n",
    "    \n",
    "    #pr=torch.dot((scale*emb@rot).to(torch.float32), shift.to(torch.float32))\n",
    "    #pr.backward()\n",
    "    return 2*scale*emb@rot@y\n",
    "\n",
    "def EU_grad_R(p, dict_nodes, rots, shifts, scales, patches):\n",
    "    \n",
    "    i=patches.index(p)\n",
    "    W=rots[i]\n",
    "    s=scales[i]\n",
    "    dim=np.shape(W)[0]\n",
    "    #W=torch.tensor(rots[i])\n",
    "    if i==len(patches)-1:\n",
    "        p_=patches[i-1]\n",
    "        s_=scales[i-1]\n",
    "        t_=shifts[i-1]\n",
    "        R_=rots[i-1]\n",
    "        grad=np.zeros((dim, dim)) #0\n",
    "        for node in dict_nodes[i-1, i]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            grad+=grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, shifts[i]) - grad_s_X_R_scalar_y(s, X, W, theta)\n",
    "        return grad\n",
    "    else:\n",
    "        p_=patches[i+1]\n",
    "        s_=scales[i+1]\n",
    "        t_=shifts[i+1]\n",
    "        R_=rots[i+1]\n",
    "        grad=np.zeros((dim, dim)) #0\n",
    "        for node in dict_nodes[i, i+1]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            grad+=grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, shifts[i]) - grad_s_X_R_scalar_y(s, X, W, theta)\n",
    "        if i==0:\n",
    "            return grad\n",
    "        else:\n",
    "            p_=patches[i-1]\n",
    "            s_=scales[i-1]\n",
    "            t_=shifts[i-1]\n",
    "            R_=rots[i-1]\n",
    "            for node in dict_nodes[i-1, i]:\n",
    "                X=p.get_coordinate(node)\n",
    "                Y=p_.get_coordinate(node)\n",
    "                theta=s_*Y@R_ + t_\n",
    "                grad+=grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, shifts[i]) - grad_s_X_R_scalar_y(s, X, W, theta)\n",
    "            return grad\n",
    "        \n",
    "        \n",
    "def grad_s(p, dict_nodes, rots, shifts, scales, patches):\n",
    "    i=patches.index(p)\n",
    "    s=scales[i]\n",
    "    #W=torch.tensor(rots[i])\n",
    "    if i==len(patches)-1:\n",
    "        p_=patches[i-1]\n",
    "        s_=scales[i-1]\n",
    "        t_=shifts[i-1]\n",
    "        R_=rots[i-1]\n",
    "        grad=0\n",
    "        for node in dict_nodes[i-1, i]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            X_R=X@rots[i]\n",
    "            grad+=2*s*np.linalg.norm(X_R) + 2*X_R@shifts[i] - 2*X_R@theta\n",
    "        return grad\n",
    "    else:\n",
    "        p_=patches[i+1]\n",
    "        s_=scales[i+1]\n",
    "        t_=shifts[i+1]\n",
    "        R_=rots[i+1]\n",
    "        grad=0\n",
    "        for node in dict_nodes[i, i+1]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            X_R=X@rots[i]\n",
    "            grad+=2*s*np.linalg.norm(X_R) + 2*X_R@shifts[i] - 2*X_R@theta\n",
    "        if i==0:\n",
    "            return grad\n",
    "        else:\n",
    "            p_=patches[i-1]\n",
    "            s_=scales[i-1]\n",
    "            t_=shifts[i-1]\n",
    "            R_=rots[i-1]\n",
    "            for node in dict_nodes[i-1, i]:\n",
    "                X=p.get_coordinate(node)\n",
    "                Y=p_.get_coordinate(node)\n",
    "                theta=s_*Y@R_ + t_\n",
    "                X_R=X@rots[i]\n",
    "                grad+=2*s*np.linalg.norm(X_R) + 2*X_R@shifts[i] - 2*X_R@theta\n",
    "            return grad\n",
    "            \n",
    "def grad_t(p, dict_nodes, rots, shifts, scales, patches):\n",
    "    i=patches.index(p)\n",
    "    s=scales[i]\n",
    "    #W=torch.tensor(rots[i])\n",
    "    if i==len(patches)-1:\n",
    "        p_=patches[i-1]\n",
    "        s_=scales[i-1]\n",
    "        t_=shifts[i-1]\n",
    "        R_=rots[i-1]\n",
    "        grad=np.zeros(np.shape(shifts[0]))\n",
    "        for node in dict_nodes[i-1, i]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            s_X_R=s*X@rots[i]\n",
    "            grad+=2*(s_X_R + shifts[i] - theta)\n",
    "        return grad\n",
    "    else:\n",
    "        p_=patches[i+1]\n",
    "        s_=scales[i+1]\n",
    "        t_=shifts[i+1]\n",
    "        R_=rots[i+1]\n",
    "        grad=np.zeros(np.shape(shifts[0]))\n",
    "        for node in dict_nodes[i, i+1]:\n",
    "            X=p.get_coordinate(node)\n",
    "            Y=p_.get_coordinate(node)\n",
    "            theta=s_*Y@R_ + t_\n",
    "            s_X_R=s*X@rots[i]\n",
    "            grad+=2*(s_X_R + shifts[i] - theta)\n",
    "        if i==0:\n",
    "            return grad\n",
    "        else:\n",
    "            p_=patches[i-1]\n",
    "            s_=scales[i-1]\n",
    "            t_=shifts[i-1]\n",
    "            R_=rots[i-1]\n",
    "            for node in dict_nodes[i-1, i]:\n",
    "                X=p.get_coordinate(node)\n",
    "                Y=p_.get_coordinate(node)\n",
    "                theta=s_*Y@R_ + t_\n",
    "                s_X_R=s*X@rots[i]\n",
    "                grad+=2*(s_X_R + shifts[i] - theta)\n",
    "            return grad\n",
    "\n",
    "\n",
    "def euclidean_grad_loss(W, p ,node, rots, shifts, scales, patches):\n",
    "    i=patches.index(p)\n",
    "    if i==len(patches):\n",
    "        q=patches[i-1]\n",
    "        theta=scales[i-1]*q.get_coordinate(node)@rots[i-1]\n",
    "    else:\n",
    "        q=patches[i+1]\n",
    "        theta=scales[i+1]*q.get_coordinate(node)@rots[i+1]\n",
    "    s=scales[i]\n",
    "    W=torch.tensor(rots[i])#, requires_grad=True)\n",
    "    X=torch.tensor(p.get_coordinate(node))\n",
    "    Y=torch.tensor(q.get_coordinate(node))\n",
    "        \n",
    "    if i==0 or i==len(patches):\n",
    "        return grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, torch.tensor(shifts[i])) - grad_s_X_R_scalar_y(s, X, W, torch.tensor(theta))\n",
    "    else:\n",
    "        if node in set(patches[i-1].nodes):\n",
    "            theta2=scales[i-1]*patches[i-1].get_coordinate(node)@rots[i-1]\n",
    "            return 2*(grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, torch.tensor(shifts[i])))- grad_s_X_R_scalar_y(s, X, W, torch.tensor(theta +theta2))\n",
    "        else:\n",
    "            return grad_s_X_R_norm(s, X, W) + grad_s_X_R_scalar_y(s, X, W, torch.tensor(shifts[i])) - grad_s_X_R_scalar_y(s, X, W, torch.tensor(theta))\n",
    "\n",
    "    \n",
    "def riemannian_gradient(eucl_grad_h, W):\n",
    "    #dim=np.shape(W)[0]\n",
    "   # D=int(0.5*dim*(dim-1))\n",
    "    #r_grad=[]\n",
    "    #for index in range(D):\n",
    "        #j=get_coordinates_from_index(index, dim)[0]\n",
    "        #l=get_coordinates_from_index(index, dim)[1]\n",
    "        #Hjl=basis(j, l, dim)\n",
    "        #r_grad.append(np.trace(Hjl.t()@np.transpose(W)@eucl_grad_h))\n",
    "    #W=torch.tensor(W)\n",
    "    return 0.5*(eucl_grad_h- W@np.transpose(eucl_grad_h)@W  )                  #torch.tensor(r_grad)\n",
    "\n",
    "\n",
    "def givens_matrix(j, l, alpha, dim):\n",
    "\n",
    "    \n",
    "    row_col = torch.tensor([list(range(dim))+ [j, l], \n",
    "                 list(range(dim))+  [l, j ]])\n",
    "\n",
    "    values=torch.tensor([1] * j +\n",
    "                 [np.cos(alpha)] + \n",
    "                 [1] * (l-j-1) + \n",
    "                 [np.cos(alpha)] + \n",
    "                 [1]*(dim-l-1) +\n",
    "                 [np.sin(alpha) , -np.sin(alpha)],\n",
    "                dtype=torch.float64)\n",
    "    row=row_col[0,:].numpy()\n",
    "    col=row_col[1,:].numpy()\n",
    "\n",
    "    S=torch.sparse_coo_tensor(row_col, values, [dim, dim])\n",
    "    T= coo_matrix((values, (row, col)), shape=(dim, dim))\n",
    "\n",
    "\n",
    "    return  T\n",
    "#def grad_s():\n",
    "    \n",
    "    \n",
    "    \n",
    "def update(W, grad, alpha, index):\n",
    "    dim=np.shape(W)[0]\n",
    "    jl=get_coordinates_from_index(index, dim)\n",
    "    j=jl[0]\n",
    "    l=jl[1]\n",
    "    Hjl=basis(j,l,dim)\n",
    "    W=torch.tensor(W)\n",
    "    eta=W@Hjl\n",
    "    grad_eta=torch.trace(grad.t()@eta)\n",
    "    grad_direction_index=grad_eta*eta\n",
    "    givens_m=givens_matrix(j, l, -alpha*grad_eta, dim)\n",
    "    W_new=torch.sparse.mm(W, givens_m)\n",
    "    return W_new\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_step_rie_grad_descent( p,patches, dict_nodes, rots, shifts, scales, alpha, tang_basis):\n",
    "    W=rots[patches.index(p)]\n",
    "    s=scales[patches.index(p)]\n",
    "    t=shifts[patches.index(p)]\n",
    "    dim=np.shape(W)[0]\n",
    "    D=int(0.5*dim*(dim-1))\n",
    "    e_grad=EU_grad_R(p, dict_nodes, rots, shifts, scales, patches)\n",
    "    t_grad=grad_t(p, dict_nodes, rots, shifts, scales, patches)\n",
    "    s_grad=grad_s(p, dict_nodes, rots, shifts, scales, patches)\n",
    "    s_new=s-alpha*s_grad\n",
    "    t_new=t-alpha*t_grad\n",
    "    #for n in nodes:\n",
    "        #e_grad.append(euclidean_grad_loss(W, p ,n, rots, shifts, scales, patches))\n",
    "\n",
    "    #e_grad=sum(e_grad)\n",
    "    #W=torch.tensor(W)\n",
    "\n",
    "    tang_basis=[W@e for e in tang_basis]\n",
    "    \n",
    "    R_grad=0.5*(e_grad- W@np.transpose(e_grad)@W  ) #riemannian_gradient(e_grad, W)\n",
    "    g_W_i=np.stack([ np.trace(np.transpose(R_grad)@ e)*e for e in tang_basis])\n",
    "\n",
    "\n",
    "    index=np.argmax([np.trace(np.transpose(g_W_i[i,:,:])@g_W_i[i,:,:]) for i in range(D)])\n",
    "    grad_eta_direction_index=g_W_i[index]\n",
    "    phi=np.trace(np.transpose(R_grad)@ tang_basis[index])\n",
    "    jl=get_coordinates_from_index(index, dim)\n",
    "    j=jl[0]\n",
    "    l=jl[1]\n",
    "    givens_m=givens_matrix(j, l, -alpha*phi, dim)\n",
    "    W_new=W@givens_m #torch.sparse.mm(W, givens_m) \n",
    "    return W_new, t_new, s_new\n",
    "        \n",
    "        \n",
    "        \n",
    "def optimize_rotations(list_of_patches, dict_nodes, list_rots, list_shifts, list_scales, learning_rate, tangent_space_basis):\n",
    "    #rots=[ortho_group.rvs(dim=5) for i in range(n_patches)]\n",
    "    n_patches=len(list_of_patches)\n",
    "    for i, p in enumerate(list_of_patches[:n_patches]):\n",
    "        W_new, t_new, s_new=one_step_rie_grad_descent(p, list_of_patches, dict_nodes, list_rots, list_shifts, list_scales, learning_rate, tangent_space_basis)\n",
    "        list_rots[i]=W_new #.numpy()\n",
    "        list_shifts[i]=t_new\n",
    "        list_scales[i]=s_new\n",
    "    return list_rots, list_shifts, list_scales\n",
    "        \n",
    "def optimize_rotations_two_patches(list_of_patches, dict_nodes, list_rots, list_shifts, list_scales, learning_rate, tangent_space_basis):\n",
    "    #rots=[ortho_group.rvs(dim=5) for i in range(n_patches)]\n",
    "    nodes=list(dict_nodes.values())[0]\n",
    "    P=list_of_patches[0]\n",
    "    Q=list_of_patches[1]\n",
    "    if len(nodes)>0:\n",
    "        W1_new=one_step_rie_grad_descent(P, list_of_patches, nodes, list_rots, list_shifts, list_scales, learning_rate, tangent_space_basis)\n",
    "        list_rots[0]=W1_new.numpy()\n",
    "        W2_new=one_step_rie_grad_descent(Q, list_of_patches[::-1], nodes[::-1], list_rots[::-1], list_shifts[::-1], list_scales[::-1], learning_rate, tangent_space_basis)\n",
    "        list_rots[1]=W2_new.numpy()\n",
    "    return list_rots    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088a230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=254\n",
    "patches=patch_emb\n",
    "n_patches=len(patches)\n",
    "rots=[ortho_group.rvs(dim) for i in range(n_patches)]\n",
    "scales=np.random.randn(len(patches)) #ones(len(patches))\n",
    "shifts=np.random.rand(len(patches), dim)\n",
    "alpha=0.00005\n",
    "D=int(0.5*dim*(dim-1))\n",
    "\n",
    "coordinates=numbering_tangent_coordinates(dim)\n",
    "list_coordinates=list(coordinates.values())\n",
    "tang_basis=[basis(j[0],j[1],dim) for j in tqdm(list_coordinates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections=nodes\n",
    "l=min_loss(rots, scales, shifts , patches, intersections)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist=[]\n",
    "for k in tqdm(range(50)):\n",
    "    rots, shifts, scales= optimize_rotations(patches, intersections, rots, shifts, scales, alpha, tang_basis)\n",
    "    loss=min_loss(rots, scales, shifts , patches, nodes)\n",
    "    loss_hist.append(loss)\n",
    "    #if k%5==0:\n",
    "    print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62864d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

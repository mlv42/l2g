{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334ef1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f84710-fa07-4ae7-a37f-f933992a5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea5d599-883a-41b7-9084-a2c54697cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from dask import delayed\n",
    "from runpy import run_path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import umap.umap_ as umap\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import local2global as l2g # ADDED\n",
    "\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.transforms import LargestConnectedComponents\n",
    "from torch_geometric.utils import to_networkx, from_networkx, one_hot\n",
    "#from torch_geometric.nn import Node2Vec, GCNConv, VGAE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "import random\n",
    "#import manopt_optimization as moptim\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric as tg\n",
    "from local2global_embedding.network import tgraph\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering, metis_clustering\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "\n",
    "\n",
    "import torch_geometric as tg\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import torch_scatter as ts\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.sparse import diags\n",
    "\n",
    "\n",
    "import scipy.sparse as ss\n",
    "import scipy.sparse.linalg as sl\n",
    "\n",
    "from local2global import Patch\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "\n",
    "import autograd.numpy as anp\n",
    "import pymanopt\n",
    "import pymanopt.manifolds\n",
    "import pymanopt.optimizers\n",
    "import random\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import numpy as np\n",
    "from pymanopt.manifolds import Stiefel, Euclidean,SpecialOrthogonalGroup,  Product\n",
    "from pymanopt.optimizers import SteepestDescent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import embedding as emb\n",
    "import torch_geometric as tg\n",
    "import local2global as l2g\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "#ADDED\n",
    "#import patches as pt\n",
    "#import network as ntw \n",
    "import autograd.numpy as anp\n",
    "import pymanopt\n",
    "import pymanopt.manifolds\n",
    "import pymanopt.optimizers\n",
    "import random\n",
    "import local2global as l2g\n",
    "import local2global.example as ex\n",
    "import numpy as np\n",
    "\n",
    "def double_intersections_nodes(patches):\n",
    "    double_intersections=dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "            double_intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return double_intersections\n",
    "    \n",
    "import itertools\n",
    "\n",
    "\n",
    "#import patches as pt\n",
    "#import network as ntw \n",
    "import numpy as np\n",
    "import geotorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import local2global as l2g\n",
    "\n",
    "\n",
    "import local2global.example as ex\n",
    "import local2global_embedding\n",
    "from scipy.stats import ortho_group \n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "import random\n",
    "#import manopt_optimization as moptim\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric as tg\n",
    "from local2global_embedding.network import tgraph\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.sparse as ss\n",
    "import scipy.sparse.linalg as sl\n",
    "\n",
    "import raphtory as rp\n",
    "from raphtory import Graph as rgraph\n",
    "\n",
    "\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering, fennel_clustering, distributed_clustering\n",
    "import local2global as l2g\n",
    "\n",
    "import local2global_embedding\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "\n",
    "#from Local2Global_embedding.local2global_embedding import  clustering\n",
    "import community\n",
    "#from Local2Global_embedding.local2global_embedding.network import graph\n",
    "from local2global_embedding.network import TGraph\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "from local2global import Patch\n",
    "#import Local2Global_embedding.local2global_embedding.embedding.svd as svd\n",
    "#import Local2Global_embedding.local2global_embedding.embedding.gae as gae\n",
    "#import Local2Global_embedding.local2global_embedding.patches as patches\n",
    "\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torch_scatter as ts\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import optuna\n",
    "#from optuna.trial import TrialState\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from google.colab import drive, files\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.transforms import LargestConnectedComponents\n",
    "\n",
    "def connected_components(data: tg.data.Data):\n",
    "    \"\"\"Find the (weakly)-connected components of graph data. Components are sorted by size, such that id=0 corresponds\n",
    "     to the largest connected component\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "    is_undir = tg.utils.is_undirected(edge_index)\n",
    "    last_components = torch.full((data.num_nodes,), data.num_nodes, dtype=torch.long)\n",
    "    components = torch.arange(data.num_nodes, dtype=torch.long)\n",
    "    while not torch.equal(last_components, components):\n",
    "        last_components[:] = components\n",
    "        components = ts.scatter(last_components[edge_index[0]], edge_index[1], out=components, reduce='min')\n",
    "        if not is_undir:\n",
    "            components = ts.scatter(last_components[edge_index[1]], edge_index[0], out=components, reduce='min')\n",
    "    component_id, inverse, component_size = torch.unique(components, return_counts=True, return_inverse=True)\n",
    "    new_id = torch.argsort(component_size, descending=True)\n",
    "    return new_id[inverse]\n",
    "\n",
    "\n",
    "def largest_connected_component(data: tg.data.Data):\n",
    "    \"\"\"find largest connected component of data\"\"\"\n",
    "    components = connected_components(data)\n",
    "    nodes = torch.nonzero(components == 0).flatten()\n",
    "    return induced_subgraph(data, nodes)\n",
    "\n",
    "\n",
    "def induced_subgraph(data: tg.data.Data, nodes, extend_hops=0):\n",
    "    nodes = torch.as_tensor(nodes, dtype=torch.long)\n",
    "    if extend_hops > 0:\n",
    "        nodes, edge_index, node_map, edge_mask = tg.utils.k_hop_subgraph(nodes, num_hops=extend_hops,\n",
    "                                                                         edge_index=data.edge_index,\n",
    "                                                                         relabel_nodes=True)\n",
    "        edge_attr = data.edge_attr[edge_mask, :] if data.edge_attr is not None else None\n",
    "    else:\n",
    "        edge_index, edge_attr = tg.utils.subgraph(nodes, data.edge_index, data.edge_attr, relabel_nodes=True)\n",
    "\n",
    "    subgraph = tg.data.Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "    for key, value in data.__dict__.items():\n",
    "        if not key.startswith('edge'):\n",
    "            if hasattr(value, 'shape') and value.shape[0] == data.num_nodes:\n",
    "                setattr(subgraph, key, value[nodes])\n",
    "            else:\n",
    "                setattr(subgraph, key, value)\n",
    "    subgraph.nodes = nodes\n",
    "    subgraph.num_nodes = len(nodes)\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def conductance(graph: TGraph, source, target=None):\n",
    "    if target is None:\n",
    "        target_mask = torch.ones(graph.num_nodes, dtype=torch.bool, device=graph.device)\n",
    "        target_mask[source] = False\n",
    "    else:\n",
    "        target_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        target_mask[target] = True\n",
    "    out = torch.cat([graph.adj(node) for node in source])\n",
    "    cond = torch.sum(target_mask[out]).float()\n",
    "    s_deg = graph.degree[source].sum()\n",
    "    t_deg = graph.num_edges-s_deg if target is None else graph.degree[target].sum()\n",
    "    cond /= torch.minimum(s_deg, t_deg)\n",
    "    return cond\n",
    "\n",
    "\n",
    "def speye(n, dtype=torch.float):\n",
    "    \"\"\"identity matrix of dimension n as sparse_coo_tensor.\"\"\"\n",
    "    return torch.sparse_coo_tensor(torch.tile(torch.arange(n, dtype=torch.long), (2, 1)),\n",
    "                                   torch.ones(n, dtype=dtype),\n",
    "                                   (n, n))\n",
    "\n",
    "\n",
    "class DistanceDecoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistanceDecoder, self).__init__()\n",
    "        self.dist = torch.nn.PairwiseDistance()\n",
    "\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        value = -self.dist(z[edge_index[0]], z[edge_index[1]])\n",
    "        return torch.sigmoid(value) if sigmoid else value\n",
    "\n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        adj = torch.cdist(z, z)\n",
    "        return torch.sigmoid(adj) if sigmoid else adj\n",
    "\n",
    "\n",
    "class GAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_index\n",
    "        x = F.relu(self.conv1(data.x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class VGAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.mean_conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                        normalize=normalize)\n",
    "        self.var_conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                       normalize=normalize)\n",
    "\n",
    "    def forward(self, data: tg.data.Data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        mu = self.mean_conv2(x, edge_index)\n",
    "        sigma = self.var_conv2(x, edge_index)\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "def VGAE_loss(model, data):\n",
    "    return model.recon_loss(model.encode(data), data.edge_index) + model.kl_loss() / data.num_nodes\n",
    "\n",
    "\n",
    "def VGAE_model(dim, hidden_dim, num_features, dist=False):\n",
    "    if dist:\n",
    "        return tg.nn.VGAE(encoder=VGAEconv(dim, num_node_features=num_features, hidden_dim=hidden_dim),\n",
    "                          decoder=DistanceDecoder())\n",
    "    else:\n",
    "        return tg.nn.VGAE(encoder=VGAEconv(dim, num_node_features=num_features, hidden_dim=hidden_dim))\n",
    "\n",
    "\n",
    "def lr_grid_search(data, model, loss_fun, validation_loss_fun, lr_grid=(0.1, 0.01, 0.005, 0.001),\n",
    "                   num_epochs=10, runs=1, verbose=True):\n",
    "    val_loss = torch.zeros((len(lr_grid), runs))\n",
    "    val_start = torch.zeros((len(lr_grid), runs))\n",
    "    for i, lr in enumerate(lr_grid):\n",
    "        for r in range(runs):\n",
    "            model.reset_parameters()\n",
    "            val_start[i, r] = validation_loss_fun(model, data)\n",
    "            model = train(data, model, loss_fun, num_epochs=num_epochs, lr=lr, verbose=verbose)\n",
    "            val_loss[i, r] = validation_loss_fun(model, data)\n",
    "    model.reset_parameters()\n",
    "    return lr_grid[torch.argmax(torch.mean(val_loss, 1))], val_loss, val_start\n",
    "\n",
    "\n",
    "def train(data, model, loss_fun, num_epochs=100, verbose=True, lr=0.01, logger=lambda loss: None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    for e in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger(float(loss))\n",
    "        if verbose:\n",
    "            print(f'epoch {e}: loss={loss.item()}')\n",
    "        # schedule.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGAE_patch_embeddings(patch_data, dim=2, hidden_dim=32, num_epochs=100, decoder=None, device='cpu', lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        #for i in range(len(patch)):#added this for loop and replace the commented part with this\n",
    "            #if patch[i].x is None:\n",
    "                #patch[i].x=speye(patch[i].num_nodes)\n",
    "        \n",
    "        \n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        print(f\"training patch with {patch.edge_index.shape[1]} edges\")   #added [i] to every patch\n",
    "        model = tg.nn.VGAE(encoder=VGAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "\n",
    "        def loss_fun(model, data):\n",
    "            return model.recon_loss(model.encode(data), data.edge_index) + model.kl_loss() / data.num_nodes\n",
    "\n",
    "        model = train(patch, model, loss_fun, num_epochs=num_epochs, lr=lr)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            coordinates = model.encode(patch).to('cpu').numpy()\n",
    "            models.append(model)\n",
    "            patch_list.append(l2g.Patch(patch.nodes.to('cpu').numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "\n",
    "def GAE_patch_embeddings(patch_data, dim=2, hidden_dim=32, num_epochs=100, device='cpu', decoder=None, lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        print(f\"training patch with {patch.edge_index.shape[1]} edges\")\n",
    "        model = tg.nn.GAE(encoder=GAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "\n",
    "        def loss_fun(model, data):\n",
    "            return model.recon_loss(model.encode(data), data.edge_index)\n",
    "        model.train()\n",
    "        model = train(patch, model, loss_fun, num_epochs=num_epochs, lr=lr)\n",
    "        model.eval()\n",
    "        coordinates = model.encode(patch).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "        models.append(model)\n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "def double_intersections_nodes(patches):\n",
    "    double_intersections = dict()\n",
    "    for i in range(len(patches)-1):\n",
    "        double_intersections[(i,i+1)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[i+1].nodes.tolist())))\n",
    "    return double_intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list = []\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list = list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "def get_embedding(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    #prob=l2g.AlignmentProblem(patches)\n",
    "    #old_embedding=prob.get_aligned_embedding()\n",
    "    #embedding=embedding[nodes]\n",
    "    #old_embedding=old_embedding[nodes]\n",
    "    #error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import geotorch\n",
    "\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches, num_epochs=100, learning_rate=0.05):\n",
    "    #device = get_device()\n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    \n",
    "    model = Model(dim, n_patches, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model, loss_hist\n",
    "\n",
    "\n",
    "def connected_components(data: tg.data.Data):\n",
    "    \"\"\"Find the (weakly)-connected components of graph data. Components are sorted by size, such that id=0 corresponds\n",
    "     to the largest connected component\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "    is_undir = tg.utils.is_undirected(edge_index)\n",
    "    last_components = torch.full((data.num_nodes,), data.num_nodes, dtype=torch.long)\n",
    "    components = torch.arange(data.num_nodes, dtype=torch.long)\n",
    "    while not torch.equal(last_components, components):\n",
    "        last_components[:] = components\n",
    "        components = ts.scatter(last_components[edge_index[0]], edge_index[1], out=components, reduce='min')\n",
    "        if not is_undir:\n",
    "            components = ts.scatter(last_components[edge_index[1]], edge_index[0], out=components, reduce='min')\n",
    "    component_id, inverse, component_size = torch.unique(components, return_counts=True, return_inverse=True)\n",
    "    new_id = torch.argsort(component_size, descending=True)\n",
    "    return new_id[inverse]\n",
    "\n",
    "\n",
    "def largest_connected_component(data: tg.data.Data):\n",
    "    \"\"\"find largest connected component of data\"\"\"\n",
    "    components = connected_components(data)\n",
    "    nodes = torch.nonzero(components == 0).flatten()\n",
    "    return induced_subgraph(data, nodes)\n",
    "\n",
    "\n",
    "def induced_subgraph(data: tg.data.Data, nodes, extend_hops=0):\n",
    "    nodes = torch.as_tensor(nodes, dtype=torch.long)\n",
    "    if extend_hops > 0:\n",
    "        nodes, edge_index, node_map, edge_mask = tg.utils.k_hop_subgraph(nodes, num_hops=extend_hops,\n",
    "                                                                         edge_index=data.edge_index,\n",
    "                                                                         relabel_nodes=True)\n",
    "        edge_attr = data.edge_attr[edge_mask, :] if data.edge_attr is not None else None\n",
    "    else:\n",
    "        edge_index, edge_attr = tg.utils.subgraph(nodes, data.edge_index, data.edge_attr, relabel_nodes=True)\n",
    "\n",
    "    subgraph = tg.data.Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "    for key, value in data.__dict__.items():\n",
    "        if not key.startswith('edge'):\n",
    "            if hasattr(value, 'shape') and value.shape[0] == data.num_nodes:\n",
    "                setattr(subgraph, key, value[nodes])\n",
    "            else:\n",
    "                setattr(subgraph, key, value)\n",
    "    subgraph.nodes = nodes\n",
    "    subgraph.num_nodes = len(nodes)\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def conductance(graph: TGraph, source, target=None):\n",
    "    if target is None:\n",
    "        target_mask = torch.ones(graph.num_nodes, dtype=torch.bool, device=graph.device)\n",
    "        target_mask[source] = False\n",
    "    else:\n",
    "        target_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        target_mask[target] = True\n",
    "    out = torch.cat([graph.adj(node) for node in source])\n",
    "    cond = torch.sum(target_mask[out]).float()\n",
    "    s_deg = graph.degree[source].sum()\n",
    "    t_deg = graph.num_edges-s_deg if target is None else graph.degree[target].sum()\n",
    "    cond /= torch.minimum(s_deg, t_deg)\n",
    "    return cond\n",
    "\n",
    "\n",
    "def speye(n, dtype=torch.float):\n",
    "    \"\"\"identity matrix of dimension n as sparse_coo_tensor.\"\"\"\n",
    "    return torch.sparse_coo_tensor(torch.tile(torch.arange(n, dtype=torch.long), (2, 1)),\n",
    "                                   torch.ones(n, dtype=dtype),\n",
    "                                   (n, n))\n",
    "\n",
    "\n",
    "class DistanceDecoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistanceDecoder, self).__init__()\n",
    "        self.dist = torch.nn.PairwiseDistance()\n",
    "\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        value = -self.dist(z[edge_index[0]], z[edge_index[1]])\n",
    "        return torch.sigmoid(value) if sigmoid else value\n",
    "\n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        adj = torch.cdist(z, z)\n",
    "        return torch.sigmoid(adj) if sigmoid else adj\n",
    "\n",
    "\n",
    "class GAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_index\n",
    "        x = F.relu(self.conv1(data.x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_patches(data, model, loss_fun, num_epochs=100, verbose=True, lr=0.01, logger=lambda loss: None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    for e in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(model, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logger(float(loss))\n",
    "        if verbose:\n",
    "            print(f'epoch {e}: loss={loss.item()}')\n",
    "        # schedule.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GAE_patch_embeddings(patch_data, dim=2, hidden_dim=32, num_epochs=100, device='cpu', decoder=None, lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        print(f\"training patch with {patch.edge_index.shape[1]} edges\")\n",
    "        model = tg.nn.GAE(encoder=GAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "\n",
    "        def loss_fun(model, data):\n",
    "            return model.recon_loss(model.encode(data), data.edge_index)\n",
    "        model.train()\n",
    "        model = train_patches(patch, model, loss_fun, num_epochs=num_epochs, lr=lr)\n",
    "        model.eval()\n",
    "        coordinates = model.encode(patch).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "        models.append(model)\n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "\n",
    "def intersections_nodes(patches):\n",
    "    intersections = dict()\n",
    "    for i in range(len(patches)):\n",
    "        for j in range(i+1, len(patches)):\n",
    "           intersections[(i,j)]=list(set(patches[i].nodes.tolist()).intersection(set(patches[j].nodes.tolist())))\n",
    "    return intersections\n",
    "\n",
    "def preprocess_graphs(list_of_patches, nodes_dict):\n",
    "    emb_list = []\n",
    "    for i in range(len(list_of_patches)-1):\n",
    "        emb_list.append([torch.tensor(list_of_patches[i].get_coordinates(list(nodes_dict[i,i+1]))),\n",
    "                         torch.tensor(list_of_patches[i+1].get_coordinates(list(nodes_dict[i,i+1])))])\n",
    "    emb_list = list(itertools.chain.from_iterable(emb_list))\n",
    "    return emb_list    \n",
    "\n",
    "def get_embedding(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    #prob=l2g.AlignmentProblem(patches)\n",
    "    #old_embedding=prob.get_aligned_embedding()\n",
    "    #embedding=embedding[nodes]\n",
    "    #old_embedding=old_embedding[nodes]\n",
    "    #error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def train_model(patch_emb, dim, n_patches,device='cpu',  num_epochs=100, learning_rate=0.05, verbose=True):\n",
    "    \n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    \n",
    "    model = Model(dim, n_patches, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        if verbose:\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model, loss_hist\n",
    "\n",
    "\n",
    "from local2global_embedding.patches import create_patch_data\n",
    "from local2global_embedding.clustering import louvain_clustering\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "\n",
    "#TO ADD HYPERPARAMETERS TUNING ( LEARNING RATE & NUM_EPOCHS) WITH GRID SEARCH\n",
    "def prepare_test(test_data, min_overlap=100, target_overlap=200):\n",
    "\n",
    "    TG=TGraph(edge_index=test_data.edge_index, edge_attr=test_data.edge_attr,  num_nodes=test_data.num_nodes, ensure_sorted=True, undir=False)\n",
    "    pt, pgraph= create_patch_data(TG, partition_tensor= louvain_clustering(TG),\n",
    "                                           min_overlap=min_overlap, target_overlap=target_overlap, verbose=True)\n",
    "    patch_data = [induced_subgraph(test_data, p) for p in pt]\n",
    "    \n",
    "    neg_edges = tg.utils.negative_sampling(test_data.edge_index, num_nodes=test_data.num_nodes)\n",
    "    return patch_data, neg_edges\n",
    "\n",
    "\n",
    "def test_new_emb(test_data, dimensions, patches, neg_edges, device='cpu'):\n",
    "    \n",
    "    \n",
    "    nodes = intersections_nodes(patches)\n",
    "    n_patches=len(patches)\n",
    "    \n",
    "    AUC=[]\n",
    "    AP=[]\n",
    "    Loss=[]\n",
    "    PATCHES=[]\n",
    "    new_emb=[]\n",
    "    for dim in tqdm(dimensions):\n",
    "        patches_emb, _ =VGAE_patch_embeddings(patches, dim=dim, num_epochs=50, device=device)\n",
    "        PATCHES.append(patches_emb)\n",
    "        emb_patches = preprocess_graphs(patches_emb, nodes)\n",
    "        print('New Alignment')\n",
    "        res, loss_hist= train_model(emb_patches, dim, n_patches ,device=device, num_epochs=50, learning_rate=0.5, verbose=False)\n",
    "        print('Get new embedding & Statistics')\n",
    "        emb=get_embedding(patches_emb, res)\n",
    "        new_emb.append(emb)\n",
    "        full_model_ip = tg.nn.VGAE(encoder=VGAEconv(dim, test_data.num_node_features))\n",
    "        auc, ap = full_model_ip.test(torch.tensor(emb), test_data.edge_index, neg_edges)\n",
    "        AUC.append(auc)\n",
    "        AP.append(ap)\n",
    "        Loss.append(loss_hist)\n",
    "\n",
    "    test_results={'AUC': AUC, 'AP': AP, 'Losses_hist': Loss}\n",
    "    \n",
    "\n",
    "    return test_results, PATCHES, new_emb, neg_edges\n",
    "\n",
    "\n",
    "def test_old_emb(list_patches_emb, test_data, neg_edges, dimensions):\n",
    "\n",
    "    \n",
    "    OLD_AUC=[]\n",
    "    OLD_AP=[]\n",
    "    old_emb=[]\n",
    "    for i in tqdm(range(len(list_patches_emb))):\n",
    "        prob=l2g.AlignmentProblem(list_patches_emb[i])\n",
    "        e=prob.get_aligned_embedding()\n",
    "        old_emb.append(e)\n",
    "        full_model_ip = tg.nn.VGAE(encoder=VGAEconv(dimensions[i], test_data.num_node_features))\n",
    "        auc, ap= full_model_ip.test(torch.tensor(e), test_data.edge_index, neg_edges)\n",
    "        OLD_AUC.append(auc)\n",
    "        OLD_AP.append(ap)\n",
    "    test_results={'AUC': OLD_AUC, 'AP': OLD_AP}\n",
    "\n",
    "    return test_results, old_emb\n",
    "\n",
    "def test_new_vs_old(test_data, dimensions, patches, neg_edges, device='cpu'):\n",
    "    print('Testing new algorithm')\n",
    "    test_results_new, list_patch_emb, new_emb, neg_edges= test_new_emb(test_data, dimensions, patches, neg_edges, device=device)\n",
    "    print('Testing old algorithm')\n",
    "    test_results_old, old_emb=test_old_emb(list_patch_emb, test_data, neg_edges, dimensions)\n",
    "    \n",
    "    \n",
    "    print('Computing the error')\n",
    "    ERR=[]\n",
    "    for i in range(len(dimensions)):\n",
    "        _, _, error = procrustes(new_emb[i], old_emb[i])\n",
    "        ERR.append(error)\n",
    "\n",
    "    \n",
    "\n",
    "    return test_results_new, test_results_old, ERR, new_emb, old_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87be9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_day=data.edge_time.max()\n",
    "#last_day\n",
    "#days=[]\n",
    "#for i in tqdm(range(1, last_day)):\n",
    "   # day_mask=data.edge_time==i\n",
    "    #edge_index=data.edge_index[:, day_mask]\n",
    "   # day_nodes=torch.concat((edge_index[0,:], edge_index[1,:])).unique()\n",
    "    ##x=data.x[day_nodes]\n",
    "    #day=Data(edge_index=data.edge_index[:, day_mask], nodes=day_nodes, x=x)\n",
    "    #days.append(day)\n",
    "   \n",
    " #nodes_dict=[]\n",
    "#new_patches=[]\n",
    "#for p in tqdm(patches):\n",
    "    #old_new_nodes={x.item():i for i, x in enumerate(p.nodes)}\n",
    "    #nodes_dict.append(old_new_nodes)\n",
    "    #new_nodes=torch.tensor(list(old_new_nodes.values()))\n",
    "    #new_edge_index = torch.tensor([[old_new_nodes[val.item()] for val in row] for row in p.edge_index])\n",
    "    #new_patches.append(Data(edge_index=new_edge_index, nodes=p.nodes, x=p.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadec485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import DGraphFin\n",
    "\n",
    "dataset = DGraphFin(root='./dataset')\n",
    "data = dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e136dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7e21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=torch.concat((data.edge_index[0,:],data.edge_index[1,:])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6127fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nodes=nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf132eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b85646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85147d7c22974cf69985ca67b5012c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3700550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, not converged: 3700550\n",
      "number of patches: 10\n",
      "average patch degree: 3.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9671d71d71f48dead2fcbf4b789bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "enlarging patch overlaps:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TG=TGraph(edge_index=data.edge_index,  num_nodes=data.num_nodes, x=data.x, ensure_sorted=True, undir=False)\n",
    "partition_tensor=fennel_clustering(TG,400  )\n",
    "pt, pgraph= create_patch_data(TG, partition_tensor= partition_tensor,\n",
    "                                           min_overlap=200000, target_overlap=500000, verbose=True)\n",
    "\n",
    "patch_data = [induced_subgraph(data, p) for p in pt]\n",
    "#neg_edges = tg.utils.negative_sampling(data.edge_index, num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b92794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b779b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c4cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for p in patch_data:\n",
    "    p.y=data.y[p.nodes]\n",
    "    p.x=data.x[p.nodes]\n",
    "    #p.nodes=torch.concat((p.edge_index[0,:], p.edge_index[1,:])).unique().sort()[0]\n",
    "    labels=p.y\n",
    "    p.num_nodes=p.nodes.size()[0]\n",
    "    \n",
    "    p.train_mask=data.train_mask[p.nodes]\n",
    "    p.valid_mask=data.val_mask[p.nodes]\n",
    "    p.test_mask=data.test_mask[p.nodes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ea02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a2a2a26",
   "metadata": {},
   "source": [
    "! python gnn.py --model gcn --dataset DGraphFin --epochs 20 --runs 1 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b36a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnn\n",
    "from models import gcn\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "\n",
    "data.adj_t = SparseTensor(row=data.edge_index[0], col=data.edge_index[1], sparse_sizes=(data.num_nodes, data.num_nodes))\n",
    "data.adj_t = data.adj_t.to_symmetric()  # Make it symmetric efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04450441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50a93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in patch_data:\n",
    "    p.adj_t = SparseTensor(row=p.edge_index[0], col=p.edge_index[1], sparse_sizes=(p.num_nodes, p.num_nodes))\n",
    "    p.adj_t = p.adj_t.to_symmetric()  # Make it symmetric efficiently\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2098e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DGraphFin\n",
    "from utils.utils import prepare_folder\n",
    "from utils.evaluator import Evaluator\n",
    "from models import MLP, MLPLinear, GCN, SAGE, GAT, GATv2\n",
    "from logger import Logger\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_sparse import SparseTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.typing import SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaeab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0b723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146ac36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2045b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535ee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9de8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d627b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ce98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=intersections_nodes(patch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db1087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAE_patch_embeddings_no_train(patch_data, dim=2, hidden_dim=32, num_epochs=100, device='cpu', decoder=None, lr=0.01):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        if patch.x is None:\n",
    "            patch.x = speye(patch.num_nodes)\n",
    "        #print(f\"training patch with {patch.edge_index.shape[1]} edges\")\n",
    "        model = tg.nn.GAE(encoder=GAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        patch.to(device)\n",
    "        model.eval()\n",
    "        coordinates = model.encode(patch).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "        models.append(model)\n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "\n",
    "class Alignment_Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def alignment_loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def alignment_train_model(patch_emb, dim, n_patches,device='cpu',  num_epochs=10, learning_rate=0.05, verbose=True):\n",
    "    \n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    \n",
    "    model = Alignment_Model(dim, n_patches, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = loss_function(transformed_patch_emb)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "        if verbose:\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model, loss_hist\n",
    "\n",
    "def get_embedding(patches, result):\n",
    "    n=len(patches)\n",
    "    rot=[result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift=[result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node)@rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "\n",
    "    #prob=l2g.AlignmentProblem(patches)\n",
    "    #old_embedding=prob.get_aligned_embedding()\n",
    "    #embedding=embedding[nodes]\n",
    "    #old_embedding=old_embedding[nodes]\n",
    "    #error= l2g.utils.procrustes_error(embedding,old_embedding)\n",
    "\n",
    "    return embedding\n",
    "def get_aligned_emb_from_patches(patches, nodes, dim):\n",
    "    emb_patches=preprocess_graphs(patches, nodes)\n",
    "    n_patches=len(patches)\n",
    "    res, _ =alignment_train_model(emb_patches, dim, n_patches,device='cpu', \n",
    "                                  num_epochs=10, learning_rate=0.05, verbose=False)\n",
    "    \n",
    "    \n",
    "    emb=torch.tensor(get_embedding(patches, res))\n",
    "    return emb\n",
    "                     \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "\n",
    "\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_sparse import SparseTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.typing import SparseTensor\n",
    "\n",
    "\n",
    "class GAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "        self.conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops,\n",
    "                                   normalize=normalize)\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_index\n",
    "        x = F.log_softmax(self.conv1(data.x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_patch_embeddings(patch_data,model, dim, device):\n",
    "    patch_list = []\n",
    "   \n",
    "    for i, patch in enumerate(patch_data):\n",
    "        print(i)\n",
    "\n",
    "        patch.to(device)\n",
    "        coordinates = model(patch.x, patch.edge_index).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "       \n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_tot_model(patches, data, nodes, model, reducer, dim, device, optimizer,  epochs=50):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    loss_hist=[]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute patch embeddings patch_data, dim=2, hidden_dim=32\n",
    "        patch_embeddings, _ =GAE_patch_embeddings_no_train(patch_data, dim=128, hidden_dim=32, num_epochs=1, device=device, decoder=None, lr=0.01)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get full graph embedding\n",
    "        print('Alignment')\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim).to(device).float()\n",
    "\n",
    "        # Reduce embedding dimension\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "\n",
    "        # Compute loss using node labels\n",
    "        loss = F.nll_loss(reduced_embedding, data.y.to(device))  # Assuming `data.y` has node labels\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "    return loss_hist #full_graph_embedding, reduced_embedding, patch_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device( \"cuda\")\n",
    "\n",
    "# Initialize models\n",
    "gcn_model = GCN(in_channels = 17, out_channels = 128, hidden_channels=128, num_layers=1, dropout=0.0).to(device)\n",
    "dim_reducer = GCN(in_channels=128, out_channels=2, hidden_channels=128, num_layers=1, dropout=0.0).to(device)  # Reduce to 2D\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = torch.optim.Adam(list(gcn_model.parameters()) + list(dim_reducer.parameters()), lr=0.01)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Example data (Replace with actual patches & graph data)\n",
    "\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c892e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=train_tot_model(patch_data, data, nodes, gcn_model, dim_reducer, dim, device, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c9d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941661f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0616c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches=patch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e9668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches=patch_data\n",
    "nodes=intersections_nodes(patch_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7215463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae995af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ====== Graph Autoencoder (GAE) Model ======\n",
    "\n",
    "class GAEconv(torch.nn.Module):\n",
    "    def __init__(self, dim, num_node_features, hidden_dim=32, cached=True, bias=True, add_self_loops=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, hidden_dim, cached=cached, bias=bias, add_self_loops=add_self_loops, normalize=normalize)\n",
    "        self.conv2 = tg.nn.GCNConv(hidden_dim, dim, cached=cached, bias=bias, add_self_loops=add_self_loops, normalize=normalize)\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index = data.edge_index\n",
    "        x = F.log_softmax(self.conv1(data.x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "def GAE_patch_embeddings_no_train(patch_data, dim=2, hidden_dim=32, device='cpu', decoder=None):\n",
    "    patch_list = []\n",
    "    models = []\n",
    "    for patch in patch_data:\n",
    "        if patch.x is None:\n",
    "            patch.x = torch.eye(patch.num_nodes)\n",
    "        model = tg.nn.GAE(encoder=GAEconv(dim, patch.x.shape[1], hidden_dim=hidden_dim), decoder=decoder).to(device)\n",
    "        model.eval()\n",
    "        patch.to(device)\n",
    "        coordinates = model.encode(patch).to('cpu').data.numpy()\n",
    "        patch.to('cpu')\n",
    "        models.append(model)\n",
    "        patch_list.append(l2g.Patch(patch.nodes.numpy(), coordinates))\n",
    "    return patch_list, models\n",
    "\n",
    "# ====== Alignment Model ======\n",
    "\n",
    "class Alignment_Model(nn.Module):\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformation = nn.ParameterList([nn.Linear(dim, dim).to(device) for _ in range(n_patches)])\n",
    "        [geotorch.orthogonal(self.transformation[i], 'weight') for i in range(n_patches)]\n",
    "        \n",
    "    \n",
    "    def forward(self, patch_emb):\n",
    "        m = len(patch_emb)\n",
    "        transformations = [self.transformation[0]] + [item for i in range(1, len(self.transformation)-1) for item in (self.transformation[i], self.transformation[i])] + [self.transformation[-1]]\n",
    "        transformed_emb = [transformations[i](patch_emb[i]) for i in range(m)]\n",
    "        return transformed_emb\n",
    "\n",
    "def alignment_loss_function(transformed_emb):\n",
    "    m = len(transformed_emb)\n",
    "    diff = [transformed_emb[i] - transformed_emb[i+1] for i in range(0, m-1, 2)]\n",
    "    loss = sum([torch.norm(d) ** 2 for d in diff])\n",
    "    return loss\n",
    "\n",
    "def alignment_train_model(patch_emb, dim, n_patches, device='cpu', num_epochs=10, learning_rate=0.05):\n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    model = Alignment_Model(dim, n_patches, device).to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = alignment_loss_function(transformed_patch_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "    \n",
    "    return model, loss_hist\n",
    "\n",
    "def alignment_eval_model(patch_emb, dim, n_patches, device=device):\n",
    "    patch_emb = [p.to(device) for p in patch_emb]\n",
    "    model = Alignment_Model(dim, n_patches, device).to(device)\n",
    "    model.eval()  # Keep evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Prevents gradients from being computed\n",
    "        transformed_patch_emb = model(patch_emb)\n",
    "        loss = alignment_loss_function(transformed_patch_emb)\n",
    "\n",
    "    return model, loss.item()  # No training, so return final loss\n",
    "\n",
    "\n",
    "#def alignment_eval_model(patch_emb, dim, n_patches, device=device, num_epochs=10, learning_rate=0.05):\n",
    "    #patch_emb = [p.to(device) for p in patch_emb]\n",
    "    #model = Alignment_Model(dim, n_patches, device).to(device)\n",
    "    #model.eval()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #loss_hist = []\n",
    "    \n",
    "    #for epoch in tqdm(range(num_epochs)):\n",
    "      #  optimizer.zero_grad()\n",
    "       # transformed_patch_emb = model(patch_emb)\n",
    "       # loss = alignment_loss_function(transformed_patch_emb)\n",
    "       # loss.backward()\n",
    "       # optimizer.step()\n",
    "       # loss_hist.append(loss.item())\n",
    "    \n",
    "    #return model, loss_hist\n",
    "\n",
    "def get_embedding(patches, result):\n",
    "    n = len(patches)\n",
    "    rot = [result.transformation[i].weight.to('cpu').detach().numpy() for i in range(n)]\n",
    "    shift = [result.transformation[i].bias.to('cpu').detach().numpy() for i in range(n)]\n",
    "    emb_problem = l2g.AlignmentProblem(patches)\n",
    "    embedding = np.empty((emb_problem.n_nodes, emb_problem.dim))\n",
    "    \n",
    "    for node, patch_list in enumerate(emb_problem.patch_index):\n",
    "        embedding[node] = np.mean([emb_problem.patches[p].get_coordinate(node) @ rot[i] + shift[i] for i, p in enumerate(patch_list)], axis=0)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def get_aligned_emb_from_patches(patches, nodes, dim, train, eval_):\n",
    "    emb_patches = preprocess_graphs(patches, nodes)\n",
    "    n_patches = len(patches)\n",
    "    if train:\n",
    "        res, _ = alignment_train_model(emb_patches, dim, n_patches, device=device, num_epochs=50, learning_rate=0.05)\n",
    "        emb = torch.tensor(get_embedding(patches, res))\n",
    "    if eval_:\n",
    "        res, _ = alignment_eval_model(emb_patches, dim, n_patches, device=device)#, num_epochs=10, learning_rate=0.05)\n",
    "        emb = torch.tensor(get_embedding(patches, res))\n",
    "    return emb\n",
    "\n",
    "# ====== Training, Validation, and Testing Functions ======\n",
    "\n",
    "def train_tot_model(patches, data, nodes, model, reducer, dim, device, optimizer, epochs=50):\n",
    "    model.train()\n",
    "    reducer.train()\n",
    "    loss_hist = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute patch embeddings\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, \n",
    "                                                            device=device, decoder=None)\n",
    "        \n",
    "        # Get full graph embedding\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim,\n",
    "                                                            train=True, eval_=False).to(device).float()\n",
    "\n",
    "        # Reduce embedding dimension\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "\n",
    "        # Compute loss using node labels\n",
    "        loss = F.nll_loss(reduced_embedding[data.train_mask], data.y[data.train_mask].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return loss_hist\n",
    "\n",
    "def validate_tot_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, \n",
    "                                                            device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim,\n",
    "                                                           train=False,\n",
    "                                                           eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        loss = F.nll_loss(reduced_embedding[data.val_mask], data.y[data.val_mask].to(device))\n",
    "        val_mask = data.val_mask.to(device)\n",
    "        y_val = data.y.to(device)\n",
    "        accuracy = (reduced_embedding[val_mask].argmax(dim=1) ==y_val[val_mask]).float().mean().item()\n",
    "    \n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def test_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim,\n",
    "                                                           train=False,\n",
    "                                                           eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        loss = F.nll_loss(reduced_embedding[data.test_mask], data.y[data.test_mask].to(device))\n",
    "        test_mask = data.test_mask.to(device)\n",
    "        y_val = data.y.to(device)\n",
    "        accuracy = (reduced_embedding[test_mask].argmax(dim=1) == y_val[test_mask]).float().mean().item()\n",
    "    \n",
    "    return loss.item(), accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00d05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb918c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efebb9db46ee4548af9e2e816ec255a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_201208/3148853247.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.conv1(data.x, edge_index))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877f9d6725b140ca90bada24f84e5d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacity of 23.46 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 21.58 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 5.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(gcn_model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(dim_reducer\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_reducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Validate model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_tot_model(patches, data, nodes, gcn_model, dim_reducer, dim\u001b[38;5;241m=\u001b[39mdim, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn [16], line 139\u001b[0m, in \u001b[0;36mtrain_tot_model\u001b[0;34m(patches, data, nodes, model, reducer, dim, device, optimizer, epochs)\u001b[0m\n\u001b[1;32m    135\u001b[0m full_graph_embedding \u001b[38;5;241m=\u001b[39m get_aligned_emb_from_patches(patch_embeddings, nodes, dim,\n\u001b[1;32m    136\u001b[0m                                                     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eval_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Reduce embedding dimension\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m reduced_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mreducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_graph_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Compute loss using node labels\u001b[39;00m\n\u001b[1;32m    142\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(reduced_embedding[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask]\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/pytorch_env/Local2Global_private-master/doc/git/l2glite/DGraphFin_baseline/models/gcn.py:47\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index: Union[Tensor, SparseTensor]):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 47\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm: \n\u001b[1;32m     49\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbns[i](x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_3jq5czyx.py:209\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    200\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    201\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m                 edge_weight\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_weight\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    206\u001b[0m             )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:271\u001b[0m, in \u001b[0;36mGCNConv.message\u001b[0;34m(self, x_j, edge_weight)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, edge_weight: OptTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_j \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacity of 23.46 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 21.58 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 5.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"lr\": [0.01],\n",
    "    \"hidden_dim\": [256],\n",
    "    \"epochs\": [ 60],\n",
    "    \"dropout\": [0.2]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for params in product(*param_grid.values()):\n",
    "    # Unpack parameters\n",
    "    lr, hidden_dim, epochs, dropout = params\n",
    "    \n",
    "    # Initialize models\n",
    "    gcn_model = GCN(in_channels=17, out_channels=dim, hidden_channels=hidden_dim, num_layers=2, dropout=dropout).to(device)\n",
    "    dim_reducer = GCN(in_channels=dim, out_channels=2, hidden_channels=hidden_dim, num_layers=2, dropout=dropout).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(list(gcn_model.parameters()) + list(dim_reducer.parameters()), lr=lr)\n",
    "    \n",
    "    # Train model\n",
    "    loss_history = train_tot_model(patches, data, nodes, gcn_model, dim_reducer, dim=dim, \n",
    "                                   device=device, optimizer=optimizer, epochs=epochs)\n",
    "    \n",
    "    # Validate model\n",
    "    val_loss, val_acc = validate_tot_model(patches, data, nodes, gcn_model, dim_reducer, dim=dim, device=device)\n",
    "    \n",
    "    print(f\"Params: {params} -> Val Accuracy: {val_acc}\")\n",
    "    \n",
    "    # Store best model\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        best_model = (gcn_model, dim_reducer)\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Parameters: {best_params}, Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def test_tot_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim, train=False, eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.nll_loss(reduced_embedding[data.test_mask], data.y[data.test_mask].to(device))\n",
    "        \n",
    "        # Get predictions\n",
    "        test_mask = data.test_mask#.to(device)\n",
    "        y_true = data.y[test_mask].cpu().numpy()\n",
    "        y_pred = reduced_embedding[test_mask].argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) == 2 else \"N/A\"  # Only for binary classification\n",
    "        \n",
    "        print(f\"Test Loss: {loss.item()}\")\n",
    "        print(f\"Test Accuracy: {acc}\")\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "        print(f\"ROC-AUC: {auc}\")\n",
    "\n",
    "    return loss.item(), acc, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ac30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def test_tot_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim, train=False, eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.nll_loss(reduced_embedding[data.test_mask], data.y[data.test_mask].to(device))\n",
    "        \n",
    "        # Get predictions\n",
    "        test_mask = data.test_mask#.to(device)\n",
    "        y_true = data.y[test_mask].cpu().numpy()\n",
    "        y_pred = reduced_embedding[test_mask].argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) == 2 else \"N/A\"  # Only for binary classification\n",
    "        \n",
    "        print(f\"Test Loss: {loss.item()}\")\n",
    "        print(f\"Test Accuracy: {acc}\")\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "        print(f\"ROC-AUC: {auc}\")\n",
    "\n",
    "    return loss.item(), acc, precision, recall, f1, auc\n",
    "loss, acc, precision, recall, f1, auc= test_tot_model(patches, data, nodes, \n",
    "                                                      model=best_model[0], reducer=best_model[1], dim=dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss, acc, precision, recall, f1, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c507aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a6527",
   "metadata": {},
   "source": [
    "Params: (0.001, 64, 100, 0.2) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 100, 0.0) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 50, 0.5) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 50, 0.2) -> Val Accuracy: 0.9871153235435486\n",
    "\n",
    "Params: (0.001, 64, 50, 0.0) -> Val Accuracy: 0.9869195222854614\n",
    "\n",
    "Params: (0.001, 128, 150, 0.5) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 150, 0.2) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 150, 0.0) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 100, 0.5) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 100, 0.2) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 100, 0.0) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 50, 0.5) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 50, 0.2) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 128, 50, 0.0) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 150, 0.5) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 150, 0.2) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 150, 0.0) -> Val Accuracy: 0.9873491525650024\n",
    "\n",
    "Params: (0.001, 64, 100, 0.5) -> Val Accuracy: 0.9873491525650024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=128\n",
    "num_epochs=50\n",
    "\n",
    "def test_tot_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim, train=False, eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.nll_loss(reduced_embedding[data.test_mask], data.y[data.test_mask].to(device))\n",
    "        #test_mask = data.test_mask.to(device)\n",
    "        #y_val = data.y.to(device)\n",
    "        # Get predictions\n",
    "        test_mask = data.test_mask.to(device)\n",
    "        y_true = data.y[test_mask].cpu().numpy()\n",
    "        y_pred = reduced_embedding[test_mask].argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) == 2 else \"N/A\"  # Only for binary classification\n",
    "        \n",
    "        print(f\"Test Loss: {loss.item()}\")\n",
    "        print(f\"Test Accuracy: {acc}\")\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "        print(f\"ROC-AUC: {auc}\")\n",
    "\n",
    "    return loss.item(), acc, precision, recall, f1, auc\n",
    "\n",
    "# ====== Running the Training, Validation, and Testing ======\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "gcn_model = GCN(in_channels=17, out_channels=dim, hidden_channels=dim, num_layers=2, dropout=0.0).to(device)\n",
    "dim_reducer = GCN(in_channels=dim, out_channels=2, hidden_channels=dim, num_layers=2, dropout=0.0).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(list(gcn_model.parameters()) + list(dim_reducer.parameters()), lr=0.01)\n",
    "\n",
    "# Train\n",
    "loss_history = train_tot_model(patches, data, nodes, gcn_model, dim_reducer, dim=dim, \n",
    "                               device=device, optimizer=optimizer, epochs=num_epochs)\n",
    "\n",
    "# Validate\n",
    "val_loss, val_acc = validate_tot_model(patches, data, nodes, gcn_model, dim_reducer, dim=dim, device=device)\n",
    "print(f\"Validation Loss: {val_loss}, Accuracy: {val_acc}\")\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = test_model(patches, data, nodes, gcn_model, dim_reducer, dim=dim, device=device)\n",
    "print(f\"Test Loss: {test_loss}, Accuracy: {test_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84973fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def test_tot_model(patches, data, nodes, model, reducer, dim, device):\n",
    "    model.eval()\n",
    "    reducer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        patch_embeddings, _ = GAE_patch_embeddings_no_train(patches, dim=dim, hidden_dim=32, device=device, decoder=None)\n",
    "        full_graph_embedding = get_aligned_emb_from_patches(patch_embeddings, nodes, dim, train=False, eval_=True).to(device).float()\n",
    "        reduced_embedding = reducer(full_graph_embedding, data.edge_index.to(device))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.nll_loss(reduced_embedding[data.test_mask], data.y[data.test_mask].to(device))\n",
    "        test_mask = data.test_mask #.to(device)\n",
    "        y_val = data.y.to(device)\n",
    "        # Get predictions\n",
    "        #test_mask = data.test_mask.to(device)\n",
    "        y_true = data.y[test_mask].cpu().numpy()\n",
    "        y_pred = reduced_embedding[test_mask].argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "        auc = roc_auc_score(y_true, y_pred) if len(set(y_true)) == 2 else \"N/A\"  # Only for binary classification\n",
    "        \n",
    "        print(f\"Test Loss: {loss.item()}\")\n",
    "        print(f\"Test Accuracy: {acc}\")\n",
    "        print(f\"Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "        print(f\"ROC-AUC: {auc}\")\n",
    "\n",
    "    return loss.item(), acc, precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=128\n",
    "num_epochs=50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gcn_model = GCN(in_channels=17, out_channels=dim, hidden_channels=dim, num_layers=2, dropout=0.5).to(device)\n",
    "dim_reducer = GCN(in_channels=dim, out_channels=2, hidden_channels=dim, num_layers=2, dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run best model on test set\n",
    "test_loss, test_acc, precision, recall, f1, auc = test_tot_model(patches, data, nodes, \n",
    "                                                                 gcn_model, dim_reducer, dim=dim, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d08dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579bbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
